{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRWAxlXmZ3qG"
      },
      "source": [
        "# IST664 Lab 3 #\n",
        "\n",
        "**Credit**: **Jeff Stanton and Preeti Jagadev**\n",
        "\n",
        "In the realm of natural language processing, syntax is one level up from morphology. Whereas morphology pertains to the components of words, syntax examines how words are sequenced together.  \n",
        "\n",
        "Although contemporary deep learning methods tend to hide a lot of these details behind the veil of the neural network, syntactical analysis remains a key part of effective NLP solutions, which is why it is such a core process in spaCy. Your ability to create, debug, and successfully modify a natural language system will be enhanced by deepening your understanding of how we use code to assign meaning to various parts of speech as well as the ways that sentences fit together.\n",
        "\n",
        "This lab begins by reading a complete text from the Project Gutenberg website. We are downloading Dostoevsky's Crime and Punishment, as plain text, in a translation by Constance Garnett."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3ZLeTxJVBjB"
      },
      "source": [
        "# Section 3.1 #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a9wxZxwNb3dN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\Black\n",
            "[nltk_data]     Knight\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to C:\\Users\\Black\n",
            "[nltk_data]     Knight\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(str, 1135213)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# text from online gutenberg\n",
        "from urllib import request\n",
        "\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "type(raw), len(raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Iw0XuitLcSiR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n\\nBy Fyodor Dostoevsky\\n\\n\\n\\nTranslated By Constance Garnett\\n\\n\\n\\n\\nTRANSLATOR’S PREFACE\\n\\nA few words about Do'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show the first 178 characters of the string stored in raw.\n",
        "# It means: “start at index 0, and go up to (but not including) index 178.”\n",
        "raw[:178]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qVfL-VuX9rNH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['the',\n",
              " 'Petersburg',\n",
              " 'school',\n",
              " 'of',\n",
              " 'Engineering',\n",
              " '.',\n",
              " 'There',\n",
              " 'he',\n",
              " 'had',\n",
              " 'already']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We'll begin our processing with tokenization\n",
        "crimetokens = nltk.word_tokenize(raw)\n",
        "crimetokens[112:122]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vVVkw7kFfIYz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11103"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's keep track of how many unique tokens we're starting with.\n",
        "len(set(crimetokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8Sb09Wv-ekV_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['the',\n",
              " 'petersburg',\n",
              " 'school',\n",
              " 'of',\n",
              " 'engineering',\n",
              " '.',\n",
              " 'there',\n",
              " 'he',\n",
              " 'had',\n",
              " 'already']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's normalize to lower case to reduce the number of unique tokens\n",
        "crimetokens = [w.lower() for w in crimetokens]\n",
        "crimetokens[112:122]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptGA8TN4erc4"
      },
      "source": [
        "Let's compare three stemmers provided by NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eSK7UQRn9rNQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(nltk.stem.porter.PorterStemmer,\n",
              " nltk.stem.lancaster.LancasterStemmer,\n",
              " nltk.stem.snowball.SnowballStemmer)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "porter = nltk.PorterStemmer()\n",
        "lancaster = nltk.LancasterStemmer()\n",
        "snowball = nltk.stem.SnowballStemmer('english')\n",
        "type(porter), type(lancaster), type(snowball)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TMgJIRW-eMVL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7173, 6245, 6988)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# From a data reduction standpoint, which stemmer results in the greatest reduction in the number of unique tokens?\n",
        "# lancaster stemmer results in the greatest reduction in the number of unique tokens\n",
        "crimePstem = [porter.stem(t) for t in crimetokens]\n",
        "crimeLstem = [lancaster.stem(t) for t in crimetokens]\n",
        "crimeSstem = [snowball.stem(t) for t in crimetokens]\n",
        "\n",
        "len(set(crimePstem)), len(set(crimeLstem)), len(set(crimeSstem))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9mgq8DbXpPE8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6907077515647568"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# What proportion of reduction have we achieved with the Porter stemmer?\n",
        "len(set(crimePstem))/len(set(crimetokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pv2hXaupp-Ia"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6013480982185845"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 3.1\n",
        "# Calculate and show the percent reduction in the number of tokens for the other two stemmers.\n",
        "len(set(crimeLstem))/len(set(crimetokens))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6728935965334617"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(set(crimeSstem))/len(set(crimetokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tabulate in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.9.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kRdy05ubPszU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Porter          Lancaster       Snowball\n",
            "--------------  --------------  --------------\n",
            "(',', 16041)    (',', 16041)    (',', 16041)\n",
            "('.', 8790)     ('.', 8790)     ('.', 8790)\n",
            "('the', 7821)   ('the', 7853)   ('the', 7821)\n",
            "('and', 6960)   ('and', 6960)   ('and', 6960)\n",
            "('to', 5267)    ('to', 5267)    ('to', 5267)\n",
            "('he', 4767)    ('he', 4767)    ('he', 4767)\n",
            "('a', 4593)     ('a', 4593)     ('a', 4593)\n",
            "('i', 4397)     ('i', 4397)     ('i', 4397)\n",
            "('’', 4039)     ('’', 4039)     ('’', 4039)\n",
            "('you', 4011)   ('you', 4019)   ('you', 4011)\n",
            "('“', 3980)     ('“', 3980)     ('“', 3980)\n",
            "('”', 3931)     ('”', 3931)     ('”', 3931)\n",
            "('of', 3808)    ('of', 3808)    ('of', 3808)\n",
            "('it', 3456)    ('it', 3456)    ('it', 3456)\n",
            "('that', 3267)  ('that', 3267)  ('that', 3267)\n",
            "('in', 3188)    ('in', 3201)    ('in', 3188)\n",
            "('wa', 2824)    ('was', 2824)   ('was', 2824)\n",
            "('!', 2363)     ('on', 2591)    ('!', 2363)\n",
            "('?', 2277)     ('!', 2363)     ('?', 2277)\n",
            "('hi', 2114)    ('?', 2277)     ('his', 2113)\n"
          ]
        }
      ],
      "source": [
        "# Let's compare the highest frequency tokens from the three stemmers\n",
        "from tabulate import tabulate\n",
        "from nltk import FreqDist\n",
        "pdist = FreqDist(crimePstem)\n",
        "ldist = FreqDist(crimeLstem)\n",
        "sdist = FreqDist(crimeSstem)\n",
        "\n",
        "compare = zip(pdist.most_common(20),\n",
        "              ldist.most_common(20),\n",
        "              sdist.most_common(20))\n",
        "\n",
        "print(tabulate(compare, headers=[\"Porter\", \"Lancaster\", \"Snowball\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YaP4dyYsMnsw"
      },
      "outputs": [],
      "source": [
        "# Question 3.2:\n",
        "# Reference: https://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg\n",
        "\n",
        "# Can you think of some reasons why the word \"the\" has a different count for the Lancaster stemmer?\n",
        "# Lancaster stemmer is said to be very aggressive so could over stemm words like their or there to the increasing the word count for the\n",
        "\n",
        "# What's going on near the end of the list where we have the following output:(('wa', 2825), ('was', 2825), ('was', 2825))\n",
        "# Porter algorithm has removed \"s\" from the word was while the toher 2 know that this is not a plural but a word.\n",
        "\n",
        "# The counts match, but what has the Porter stemmer done differently?\n",
        "# Porter is a rules based algorithm and does not use a dictionary so removes \"s\" from words\n",
        "\n",
        "# What conclusions can you draw about the advantages and disadvantages of various stemmers?\n",
        "# Some stemmers are better when there are irregular words like verbs with odd conjugations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKo1qgxFRtlk"
      },
      "source": [
        "# Section 3.2 #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7hFZwVJRbYr"
      },
      "source": [
        "Because stemming is quite variable in the results it produces, some NLP processing methods use lemmatization instead. A lemma is the root form on a word. Let's try this with the Wordnet Lemmatizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yjBF4aidUt0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to C:\\Users\\Black\n",
            "[nltk_data]     Knight\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Black\n",
            "[nltk_data]     Knight\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "nltk.stem.wordnet.WordNetLemmatizer"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "wnl = nltk.WordNetLemmatizer()\n",
        "type(wnl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3kV6ONqbUz1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'be'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wnl.lemmatize(\"am\", pos =\"v\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "t_Ab7tmyWKZm"
      },
      "outputs": [],
      "source": [
        "# Question 3.3\n",
        "# a: Lemmatize \"is\" and \"are\" using wnl.lemmatize(), where pos=\"v\"\n",
        "# b: Test what happens if you leave out the pos argument?\n",
        "# c: Write a comment describing what the pos argument does.\n",
        "\n",
        "# Solution:\n",
        "# pos argument means part of speech and it specifys what the part of speech is for the word passed. If it is left out the default is noun and so often does not change the word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'be'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wnl.lemmatize(\"is\", pos =\"v\")\n",
        "wnl.lemmatize(\"are\", pos =\"v\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'is'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wnl.lemmatize(\"is\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'are'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wnl.lemmatize(\"are\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFxkTiX50X9Z"
      },
      "source": [
        "# Section 3.3\n",
        "WordNet really seems to require that the user specify the part of speech. Without that specification, there are likely to be errors.\n",
        "\n",
        "Switching gears for a moment, one way of capturing more contextual information in our token lists is to analyze tokens in sets of two or more. Two tokens together is called a bigram, three is called a trigram, and more generally any number \"n\" is called an ngram.\n",
        "\n",
        "NLTK and other language packages contain numerous tools for working with bigrams. Let's look at the output of the NLTK ngrams() function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "r1LV66cE0Xla"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Rather than a whole book, let's begin by working with one sentence:\n",
        "sentence = \"thomas jefferson began building monticello at the age of twenty-six.\"\n",
        "len(sentence)  #len() counts all characters in a string, including:Letters, Spaces, and Punctuation marks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LPhN_iydrW1M"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('thomas', 'jefferson'),\n",
              " ('jefferson', 'began'),\n",
              " ('began', 'building'),\n",
              " ('building', 'monticello'),\n",
              " ('monticello', 'at'),\n",
              " ('at', 'the'),\n",
              " ('the', 'age'),\n",
              " ('age', 'of'),\n",
              " ('of', 'twenty'),\n",
              " ('twenty', 'six')]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.util import ngrams\n",
        "import re # Regular expressions library\n",
        "pattern = re.compile('[a-z]+')\n",
        "tokens = pattern.findall(sentence)\n",
        "list(ngrams(tokens, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MxxAEH6e0j7C"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('thomas', 'jefferson', 'began'),\n",
              " ('jefferson', 'began', 'building'),\n",
              " ('began', 'building', 'monticello'),\n",
              " ('building', 'monticello', 'at'),\n",
              " ('monticello', 'at', 'the'),\n",
              " ('at', 'the', 'age'),\n",
              " ('the', 'age', 'of'),\n",
              " ('age', 'of', 'twenty'),\n",
              " ('of', 'twenty', 'six')]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can easily repeat the process for trigrams:\n",
        "list(ngrams(tokens, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-BiCdAM0xBX"
      },
      "source": [
        "Research has shown that there is value in understanding the context around words - i.e., the other words that occur nearby. This was an idea called \"**The Distributional Hypothesis**\" imagined by linguist Zellig Harris, that words with similar meanings tend to occur in similar contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xhCwSXoG0nz6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['thomas jefferson', 'jefferson began', 'began building', 'building monticello', 'monticello at', 'at the', 'the age', 'age of', 'of twenty', 'twenty six']\n"
          ]
        }
      ],
      "source": [
        "bigrams = [\" \".join(w) for w in ngrams(tokens, 2)]\n",
        "print(bigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "TGG43sOl3sDp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['thomas jefferson began', 'jefferson began building', 'began building monticello', 'building monticello at', 'monticello at the', 'at the age', 'the age of', 'age of twenty', 'of twenty six']\n"
          ]
        }
      ],
      "source": [
        "# Question 3.4:\n",
        "# Build trigram tokens from the Thomas Jefferson tokens.\n",
        "# Make sure the trigram tokens have spaces between the component words as shown in the bigram example in the code block just above.\n",
        "\n",
        "# Solution\n",
        "trigrams = [\" \".join(w) for w in ngrams(tokens, 3)]\n",
        "print(trigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Rm1YYKtF0-QV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\Black\n",
            "[nltk_data]     Knight\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "74155"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Given only one sentence, that result is not very exciting, but what if we did a whole book?\n",
        "nltk.download('stopwords')\n",
        "nltk_stops = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "crimenopunct = [w for w in crimetokens if w.isalnum()]\n",
        "crimenostops = [w for w in crimenopunct if w not in nltk_stops]\n",
        "crimebigrams = [\" \".join(w) for w in ngrams(crimenostops, 2)]\n",
        "\n",
        "fdist = FreqDist(crimebigrams) # This creates a list of frequencies for bigrams\n",
        "len(fdist) # This is the total number of unique bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UWX0Lle-1Fg8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('katerina ivanovna', 215),\n",
              " ('pyotr petrovitch', 172),\n",
              " ('pulcheria alexandrovna', 123),\n",
              " ('avdotya romanovna', 112),\n",
              " ('old woman', 91),\n",
              " ('rodion romanovitch', 82),\n",
              " ('porfiry petrovitch', 81),\n",
              " ('marfa petrovna', 76),\n",
              " ('sofya semyonovna', 71),\n",
              " ('amalia ivanovna', 54)]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fdist.most_common(10) # What do you notice about the most frequent bigrams\n",
        "# The most frequent bigrams are all full names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dayl1Ab3kYSv"
      },
      "source": [
        "**Part Two**\n",
        "\n",
        "The WordNet lemmatizer does not work very well unless we already know the part of speech of the word we are trying to lemmatize. This is a significant limitation.\n",
        "\n",
        "Given the limitations of stemmers and simple lemmatizers, it is time to take a more serious look at part of speech tagging. For this, we are going to graduate from NLTK to our first effort with spaCy. Whereas NLTK was designed for teaching and research, spaCy was architected so that it can serve as the basis of a production-grade NLP pipeline. Unlike other NLP toolkits (e.g., Stanford core NLP) spaCy was written in Python and Cython, so it is convenient for use directly from the Jupyter notebook environment. For now, we will just try out a few basic techniques.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeYUwDxTfFu8"
      },
      "source": [
        "# Section 3.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (25.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
            "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
            "     - ------------------------------------- 0.5/12.8 MB 707.5 kB/s eta 0:00:18\n",
            "     ---- ----------------------------------- 1.3/12.8 MB 1.4 MB/s eta 0:00:09\n",
            "     ---- ----------------------------------- 1.3/12.8 MB 1.4 MB/s eta 0:00:09\n",
            "     ------ --------------------------------- 2.1/12.8 MB 1.7 MB/s eta 0:00:07\n",
            "     ------ --------------------------------- 2.1/12.8 MB 1.7 MB/s eta 0:00:07\n",
            "     ---------- ----------------------------- 3.4/12.8 MB 2.0 MB/s eta 0:00:05\n",
            "     ---------- ----------------------------- 3.4/12.8 MB 2.0 MB/s eta 0:00:05\n",
            "     ------------ --------------------------- 3.9/12.8 MB 1.9 MB/s eta 0:00:05\n",
            "     ---------------- ----------------------- 5.2/12.8 MB 2.3 MB/s eta 0:00:04\n",
            "     ----------------- ---------------------- 5.5/12.8 MB 2.2 MB/s eta 0:00:04\n",
            "     --------------------- ------------------ 6.8/12.8 MB 2.5 MB/s eta 0:00:03\n",
            "     --------------------- ------------------ 6.8/12.8 MB 2.5 MB/s eta 0:00:03\n",
            "     ---------------------- ----------------- 7.1/12.8 MB 2.3 MB/s eta 0:00:03\n",
            "     ---------------------- ----------------- 7.3/12.8 MB 2.2 MB/s eta 0:00:03\n",
            "     ------------------------------- -------- 10.2/12.8 MB 2.8 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 11.3/12.8 MB 3.0 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 12.3/12.8 MB 3.1 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.8/12.8 MB 3.1 MB/s  0:00:05\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fwIUu68mwY4Q"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "spacy.lang.en.English"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm') # sm means small - some pipeline capabilities not loaded\n",
        "type(nlp) # This is our pipeline: an instantiated class that we can use to process any string\n",
        "# You can ignore this warning if you see it: \"UserWarning: Can't initialize NVML\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "hV4_95-Vy7Qg"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(spacy.tokens.doc.Doc, 15)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's process a small example from NLPIA first\n",
        "sentence = \"The faster Harry got to the store, the faster Harry would get home.\"\n",
        "spsent = nlp(sentence)\n",
        "type(spsent), len(spsent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "1rVaDDfizZqn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cats',\n",
              " 'char_span',\n",
              " 'copy',\n",
              " 'count_by',\n",
              " 'doc',\n",
              " 'ents',\n",
              " 'extend_tensor',\n",
              " 'from_array',\n",
              " 'from_bytes',\n",
              " 'from_dict',\n",
              " 'from_disk',\n",
              " 'from_docs',\n",
              " 'from_json',\n",
              " 'get_extension',\n",
              " 'get_lca_matrix',\n",
              " 'has_annotation',\n",
              " 'has_extension',\n",
              " 'has_unknown_spaces',\n",
              " 'has_vector',\n",
              " 'is_nered',\n",
              " 'is_parsed',\n",
              " 'is_sentenced',\n",
              " 'is_tagged',\n",
              " 'lang',\n",
              " 'lang_',\n",
              " 'mem',\n",
              " 'noun_chunks',\n",
              " 'noun_chunks_iterator',\n",
              " 'remove_extension',\n",
              " 'retokenize',\n",
              " 'sentiment',\n",
              " 'sents',\n",
              " 'set_ents',\n",
              " 'set_extension',\n",
              " 'similarity',\n",
              " 'spans',\n",
              " 'tensor',\n",
              " 'text',\n",
              " 'text_with_ws',\n",
              " 'to_array',\n",
              " 'to_bytes',\n",
              " 'to_dict',\n",
              " 'to_disk',\n",
              " 'to_json',\n",
              " 'to_utf8_array',\n",
              " 'user_data',\n",
              " 'user_hooks',\n",
              " 'user_span_hooks',\n",
              " 'user_token_hooks',\n",
              " 'vector',\n",
              " 'vector_norm',\n",
              " 'vocab']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[m for m in dir(spsent) if m[0] != '_']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Wg6tH-vrzvAI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spsent.has_annotation(\"TAG\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "HprBAeam5R80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token    POS Tag\n",
            "-------  ---------\n",
            "The      DET\n",
            "faster   ADJ\n",
            "Harry    PROPN\n",
            "got      VERB\n",
            "to       ADP\n",
            "the      DET\n",
            "store    NOUN\n",
            ",        PUNCT\n",
            "the      DET\n",
            "faster   ADJ\n",
            "Harry    PROPN\n",
            "would    AUX\n",
            "get      VERB\n",
            "home     ADV\n",
            ".        PUNCT\n"
          ]
        }
      ],
      "source": [
        "tags = [(i, i.pos_) for i in spsent]\n",
        "print(tabulate(tags, headers=[\"Token\", \"POS Tag\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "7zLxoqQH6HJf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token    Lemma    Tag\n",
            "-------  -------  -----\n",
            "The      the      DET\n",
            "faster   fast     ADJ\n",
            "Harry    Harry    PROPN\n",
            "got      get      VERB\n",
            "to       to       ADP\n",
            "the      the      DET\n",
            "store    store    NOUN\n",
            ",        ,        PUNCT\n",
            "the      the      DET\n",
            "faster   fast     ADJ\n",
            "Harry    Harry    PROPN\n",
            "would    would    AUX\n",
            "get      get      VERB\n",
            "home     home     ADV\n",
            ".        .        PUNCT\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Make a little dataset for tabulate() to work on.\n",
        "poslist = [ (i, i.lemma_, i.pos_) for i in spsent]\n",
        "\n",
        "print(tabulate(poslist,  headers=[\"Token\", \"Lemma\", \"Tag\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ipfZfKyp8zAZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['is_alpha',\n",
              " 'is_ancestor',\n",
              " 'is_ascii',\n",
              " 'is_bracket',\n",
              " 'is_currency',\n",
              " 'is_digit',\n",
              " 'is_left_punct',\n",
              " 'is_lower',\n",
              " 'is_oov',\n",
              " 'is_punct',\n",
              " 'is_quote',\n",
              " 'is_right_punct',\n",
              " 'is_sent_end',\n",
              " 'is_sent_start',\n",
              " 'is_space',\n",
              " 'is_stop',\n",
              " 'is_title',\n",
              " 'is_upper']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[m for m in dir(spsent[0]) if m[0:2] == 'is']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "L5DfLI85mOUH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token    Head    Lemma    Tag    Details    Alpha?\n",
            "-------  ------  -------  -----  ---------  --------\n",
            "The      Harry   the      DET    DT         True\n",
            "faster   Harry   fast     ADJ    JJR        True\n",
            "Harry    got     Harry    PROPN  NNP        True\n",
            "got      get     get      VERB   VBD        True\n",
            "to       got     to       ADP    IN         True\n",
            "the      store   the      DET    DT         True\n",
            "store    to      store    NOUN   NN         True\n",
            ",        get     ,        PUNCT  ,          False\n",
            "the      Harry   the      DET    DT         True\n",
            "faster   Harry   fast     ADJ    JJR        True\n",
            "Harry    get     Harry    PROPN  NNP        True\n",
            "would    get     would    AUX    MD         True\n",
            "get      get     get      VERB   VB         True\n",
            "home     get     home     ADV    RB         True\n",
            ".        get     .        PUNCT  .          False\n"
          ]
        }
      ],
      "source": [
        "# Let's make a more detailed table with a few of these fields.\n",
        "poslist = [ (i, i.head, i.lemma_, i.pos_, i.tag_, i.is_alpha) for i in spsent]\n",
        "\n",
        "print(tabulate(poslist,  headers=[\"Token\", \"Head\", \"Lemma\", \"Tag\", \"Details\",\"Alpha?\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZo9P_EJoUsr"
      },
      "source": [
        "The table above just scratches the surface, but there's still a lot of interesting stuff happening there. In the first column we have the token itself, which can be a word, a number, or punctuation. The second column starts to unpack the idea of dependency grammar - that each word in a sentence represents a portion of a tree, with \"ancestors\" that it depends on and \"children\" that depend on it. \"Head\" refers to the immediate ancestor of a word. So for instance, the proper noun \"Harry\" depends on the corresponding verb \"got.\" Next we have the lemmas and the simple part of speech tag as before. By the way, you can find an explanation of these tags here:\n",
        "\n",
        "https://universaldependencies.org/docs/u/pos/\n",
        "\n",
        "Finally, there is a fine-grained part of speech - a more complicated tag provided by spaCy. These are unique to each language model, but there is a function call that will provide information about any of the tags:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "PJA12TMdoI1C"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'adjective, comparative'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spacy.explain(\"JJR\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UIPOTKM1u2A"
      },
      "source": [
        "#Checkpoint! Use spacy.explain(\"RB\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "tCAlMrxH1o3p"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'adverb'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 3.5\n",
        "# Add and run spacy.explain(\"RB\")\n",
        "\n",
        "spacy.explain(\"RB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "DjmOSyCrr5WE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(spacy.tokens.doc.Doc, 130)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's practice by tagging another sentence. Here's some text extracted from\n",
        "# Wikipedia's article on kites.\n",
        "kites = \"\"\"A kite is a tethered heavier-than-air or lighter-than-air craft with wing surfaces that react against the air to create lift and drag forces.\n",
        "A kite consists of wings, tethers and anchors. Kites often have a bridle and tail to guide the face of the kite so the wind can lift it.\n",
        "Some kite designs don’t need a bridle; box kites can have a single attachment point.\n",
        "A kite may have fixed or moving anchors that can balance the kite.\n",
        "One technical definition is that a kite is “a collection of tether-coupled wing sets“.\n",
        "The name derives from its resemblance to a hovering bird.\"\"\"\n",
        "\n",
        "spkites = nlp(kites)\n",
        "type(spkites), len(spkites)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "WTE4tWNIsoFd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token        Lemma        Tag\n",
            "-----------  -----------  -----\n",
            "A            a            DET\n",
            "kite         kite         NOUN\n",
            "is           be           AUX\n",
            "a            a            DET\n",
            "tethered     tethered     ADJ\n",
            "heavier      heavy        ADJ\n",
            "-            -            PUNCT\n",
            "than         than         ADP\n",
            "-            -            PUNCT\n",
            "air          air          NOUN\n",
            "or           or           CCONJ\n",
            "lighter      light        ADJ\n",
            "-            -            PUNCT\n",
            "than         than         ADP\n",
            "-            -            PUNCT\n",
            "air          air          NOUN\n",
            "craft        craft        NOUN\n",
            "with         with         ADP\n",
            "wing         wing         NOUN\n",
            "surfaces     surface      NOUN\n",
            "that         that         PRON\n",
            "react        react        VERB\n",
            "against      against      ADP\n",
            "the          the          DET\n",
            "air          air          NOUN\n",
            "to           to           PART\n",
            "create       create       VERB\n",
            "lift         lift         NOUN\n",
            "and          and          CCONJ\n",
            "drag         drag         NOUN\n",
            "forces       force        NOUN\n",
            ".            .            PUNCT\n",
            "                          SPACE\n",
            "A            a            DET\n",
            "kite         kite         NOUN\n",
            "consists     consist      VERB\n",
            "of           of           ADP\n",
            "wings        wing         NOUN\n",
            ",            ,            PUNCT\n",
            "tethers      tether       NOUN\n",
            "and          and          CCONJ\n",
            "anchors      anchor       NOUN\n",
            ".            .            PUNCT\n",
            "Kites        Kites        PROPN\n",
            "often        often        ADV\n",
            "have         have         VERB\n",
            "a            a            DET\n",
            "bridle       bridle       NOUN\n",
            "and          and          CCONJ\n",
            "tail         tail         NOUN\n",
            "to           to           PART\n",
            "guide        guide        VERB\n",
            "the          the          DET\n",
            "face         face         NOUN\n",
            "of           of           ADP\n",
            "the          the          DET\n",
            "kite         kite         NOUN\n",
            "so           so           SCONJ\n",
            "the          the          DET\n",
            "wind         wind         NOUN\n",
            "can          can          AUX\n",
            "lift         lift         VERB\n",
            "it           it           PRON\n",
            ".            .            PUNCT\n",
            "                          SPACE\n",
            "Some         some         DET\n",
            "kite         kite         NOUN\n",
            "designs      design       NOUN\n",
            "do           do           AUX\n",
            "n’t          not          PART\n",
            "need         need         VERB\n",
            "a            a            DET\n",
            "bridle       bridle       NOUN\n",
            ";            ;            PUNCT\n",
            "box          box          PROPN\n",
            "kites        kite         NOUN\n",
            "can          can          AUX\n",
            "have         have         VERB\n",
            "a            a            DET\n",
            "single       single       ADJ\n",
            "attachment   attachment   NOUN\n",
            "point        point        NOUN\n",
            ".            .            PUNCT\n",
            "                          SPACE\n",
            "A            a            DET\n",
            "kite         kite         NOUN\n",
            "may          may          AUX\n",
            "have         have         AUX\n",
            "fixed        fix          VERB\n",
            "or           or           CCONJ\n",
            "moving       move         VERB\n",
            "anchors      anchor       NOUN\n",
            "that         that         PRON\n",
            "can          can          AUX\n",
            "balance      balance      VERB\n",
            "the          the          DET\n",
            "kite         kite         NOUN\n",
            ".            .            PUNCT\n",
            "                          SPACE\n",
            "One          one          NUM\n",
            "technical    technical    ADJ\n",
            "definition   definition   NOUN\n",
            "is           be           AUX\n",
            "that         that         SCONJ\n",
            "a            a            DET\n",
            "kite         kite         NOUN\n",
            "is           be           AUX\n",
            "“            \"            PUNCT\n",
            "a            a            DET\n",
            "collection   collection   NOUN\n",
            "of           of           ADP\n",
            "tether       tether       NOUN\n",
            "-            -            PUNCT\n",
            "coupled      couple       VERB\n",
            "wing         wing         NOUN\n",
            "sets         set          NOUN\n",
            "“            \"            PUNCT\n",
            ".            .            PUNCT\n",
            "                          SPACE\n",
            "The          the          DET\n",
            "name         name         NOUN\n",
            "derives      derive       VERB\n",
            "from         from         ADP\n",
            "its          its          PRON\n",
            "resemblance  resemblance  NOUN\n",
            "to           to           ADP\n",
            "a            a            DET\n",
            "hovering     hover        VERB\n",
            "bird         bird         NOUN\n",
            ".            .            PUNCT\n"
          ]
        }
      ],
      "source": [
        "# Question 3.6:\n",
        "# Display tokens, lemmas, and parts of speech for spkites.\n",
        "# Try using a nice, neat tabular format for the output.\n",
        "\n",
        "# Solution\n",
        "poslist = [ (i, i.lemma_, i.pos_) for i in spkites]\n",
        "\n",
        "print(tabulate(poslist,  headers=[\"Token\", \"Lemma\", \"Tag\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "QSdCtkO-uvX7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "A kite is a tethered heavier-than-air or lighter-than-air craft with wing surfaces that react against the air to create lift and drag forces."
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# It might be more convenient to work with individual sentences:\n",
        "kitespans = list(spkites.sents)\n",
        "kitespans[0] # Let's view just the first sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(kitespans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "# And import any other functions here as well: from IPython.display import HTML, Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Wm-eThSIvV6g"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"4667d120c1b24b7497e6de66b864fa7c-0\" class=\"displacy\" width=\"4950\" height=\"749.5\" direction=\"ltr\" style=\"max-width: none; height: 749.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">A</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">kite</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">tethered</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">heavier-</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">than-</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">air</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">or</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">lighter-</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">than-</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">air</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">craft</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">with</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">wing</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">surfaces</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">that</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">react</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">against</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">air</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">to</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3900\">create</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3900\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4075\">lift</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4075\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4250\">and</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4250\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4425\">drag</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4425\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4600\">forces.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4600\">PUNCT</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4775\">\n",
              "</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4775\">SPACE</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-0\" stroke-width=\"2px\" d=\"M70,614.5 C70,527.0 195.0,527.0 195.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,616.5 L62,604.5 78,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-1\" stroke-width=\"2px\" d=\"M245,614.5 C245,527.0 370.0,527.0 370.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,616.5 L237,604.5 253,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-2\" stroke-width=\"2px\" d=\"M595,614.5 C595,177.0 2140.0,177.0 2140.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M595,616.5 L587,604.5 603,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-3\" stroke-width=\"2px\" d=\"M770,614.5 C770,264.5 2135.0,264.5 2135.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M770,616.5 L762,604.5 778,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-4\" stroke-width=\"2px\" d=\"M945,614.5 C945,352.0 2130.0,352.0 2130.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M945,616.5 L937,604.5 953,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-5\" stroke-width=\"2px\" d=\"M945,614.5 C945,527.0 1070.0,527.0 1070.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1070.0,616.5 L1078.0,604.5 1062.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-6\" stroke-width=\"2px\" d=\"M1120,614.5 C1120,527.0 1245.0,527.0 1245.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1245.0,616.5 L1253.0,604.5 1237.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-7\" stroke-width=\"2px\" d=\"M1295,614.5 C1295,527.0 1420.0,527.0 1420.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1420.0,616.5 L1428.0,604.5 1412.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-8\" stroke-width=\"2px\" d=\"M1295,614.5 C1295,439.5 1600.0,439.5 1600.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1600.0,616.5 L1608.0,604.5 1592.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-9\" stroke-width=\"2px\" d=\"M1645,614.5 C1645,527.0 1770.0,527.0 1770.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1770.0,616.5 L1778.0,604.5 1762.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-10\" stroke-width=\"2px\" d=\"M1820,614.5 C1820,527.0 1945.0,527.0 1945.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1945.0,616.5 L1953.0,604.5 1937.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-11\" stroke-width=\"2px\" d=\"M420,614.5 C420,89.5 2145.0,89.5 2145.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2145.0,616.5 L2153.0,604.5 2137.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-12\" stroke-width=\"2px\" d=\"M2170,614.5 C2170,527.0 2295.0,527.0 2295.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2295.0,616.5 L2303.0,604.5 2287.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-13\" stroke-width=\"2px\" d=\"M2520,614.5 C2520,527.0 2645.0,527.0 2645.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2520,616.5 L2512,604.5 2528,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-14\" stroke-width=\"2px\" d=\"M2345,614.5 C2345,439.5 2650.0,439.5 2650.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2650.0,616.5 L2658.0,604.5 2642.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-15\" stroke-width=\"2px\" d=\"M2870,614.5 C2870,527.0 2995.0,527.0 2995.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2870,616.5 L2862,604.5 2878,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-16\" stroke-width=\"2px\" d=\"M2695,614.5 C2695,439.5 3000.0,439.5 3000.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3000.0,616.5 L3008.0,604.5 2992.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-17\" stroke-width=\"2px\" d=\"M3045,614.5 C3045,527.0 3170.0,527.0 3170.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3170.0,616.5 L3178.0,604.5 3162.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-18\" stroke-width=\"2px\" d=\"M3395,614.5 C3395,527.0 3520.0,527.0 3520.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3395,616.5 L3387,604.5 3403,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-19\" stroke-width=\"2px\" d=\"M3220,614.5 C3220,439.5 3525.0,439.5 3525.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3525.0,616.5 L3533.0,604.5 3517.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-20\" stroke-width=\"2px\" d=\"M3745,614.5 C3745,527.0 3870.0,527.0 3870.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3745,616.5 L3737,604.5 3753,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-21\" stroke-width=\"2px\" d=\"M3045,614.5 C3045,352.0 3880.0,352.0 3880.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-21\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3880.0,616.5 L3888.0,604.5 3872.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-22\" stroke-width=\"2px\" d=\"M4095,614.5 C4095,352.0 4580.0,352.0 4580.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-22\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M4095,616.5 L4087,604.5 4103,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-23\" stroke-width=\"2px\" d=\"M4095,614.5 C4095,527.0 4220.0,527.0 4220.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-23\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M4220.0,616.5 L4228.0,604.5 4212.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-24\" stroke-width=\"2px\" d=\"M4095,614.5 C4095,439.5 4400.0,439.5 4400.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-24\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M4400.0,616.5 L4408.0,604.5 4392.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-25\" stroke-width=\"2px\" d=\"M420,614.5 C420,2.0 4600.0,2.0 4600.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-25\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M4600.0,616.5 L4608.0,604.5 4592.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4667d120c1b24b7497e6de66b864fa7c-0-26\" stroke-width=\"2px\" d=\"M4620,614.5 C4620,527.0 4745.0,527.0 4745.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4667d120c1b24b7497e6de66b864fa7c-0-26\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M4745.0,616.5 L4753.0,604.5 4737.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# One other neat trick: We can use spaCy to display a graphical\n",
        "# version of the dependence tree for any sentence or document.\n",
        "from spacy import displacy\n",
        "from IPython.display import display, HTML\n",
        "html = displacy.render(kitespans[0], style=\"dep\", jupyter=False)\n",
        "\n",
        "display(HTML(html))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "eN2GoLJYvrT5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"e85af44603b84853b61cf6ed83195db9-0\" class=\"displacy\" width=\"1450\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">A</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">kite</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">consists</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">of</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">wings,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">tethers</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">and</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">anchors.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-e85af44603b84853b61cf6ed83195db9-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-e85af44603b84853b61cf6ed83195db9-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-e85af44603b84853b61cf6ed83195db9-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-e85af44603b84853b61cf6ed83195db9-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-e85af44603b84853b61cf6ed83195db9-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-e85af44603b84853b61cf6ed83195db9-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-e85af44603b84853b61cf6ed83195db9-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-e85af44603b84853b61cf6ed83195db9-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-e85af44603b84853b61cf6ed83195db9-0-4\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-e85af44603b84853b61cf6ed83195db9-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M920.0,179.0 L928.0,167.0 912.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-e85af44603b84853b61cf6ed83195db9-0-5\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-e85af44603b84853b61cf6ed83195db9-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1095.0,179.0 L1103.0,167.0 1087.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-e85af44603b84853b61cf6ed83195db9-0-6\" stroke-width=\"2px\" d=\"M945,177.0 C945,2.0 1275.0,2.0 1275.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-e85af44603b84853b61cf6ed83195db9-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1275.0,179.0 L1283.0,167.0 1267.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Question 3.7\n",
        "# Add a dependency structure graph for the second sentence in kites.\n",
        "\n",
        "# Solution\n",
        "html1 = displacy.render(kitespans[1], style=\"dep\", jupyter=False)\n",
        "\n",
        "display(HTML(html1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbgHBwm66EM_"
      },
      "source": [
        "Let's close the loop on the idea of data reduction by seeing how many unique lemmas spaCy creates for Crime and Punishment.\n",
        "\n",
        "Recall that we were unsatisfied with the lemmatizer from NLTK because, for it to work efficiently we needed to know the POS for each token before calling the lemmatizer.\n",
        "\n",
        "The spaCy nlp() call ingests our whole text, applies tags, and determines lemmas, all based on a swappable language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "0t6ne0lS6A1N"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(spacy.tokens.doc.Doc, 270889)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Process Crime and Punishment with spaCy: takes a minute!\n",
        "nlp.max_length = 1200000\n",
        "crimespacy = nlp(raw) # We're going back to the original raw text data!\n",
        "type(crimespacy), len(crimespacy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Y-BTBbW-7_nZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7570"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's count unique lemmas\n",
        "newcrimelemma = [l.lemma_ for l in crimespacy]\n",
        "len(set(newcrimelemma))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "IRC0_cIT8mnW"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7289359653346172"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(set(newcrimelemma))/len(set(crimetokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-5oEhkb8141"
      },
      "source": [
        "**Part Three**\n",
        "\n",
        "One essential way of representing a corpus is to transform the token counts into a \"**Document Term Matrix\" (DTM)** or a transposed version of the same thing, a \"**Term Document Matrix.**\"\n",
        "\n",
        "The most basic DTM contains word frequencies in each cell. A more advanced DTM contains adjusted values known as **TF-IDF** (term frequency, inverse document frequency).\n",
        "\n",
        "Creating a DTM, either with counts or with TF-IDF values begins with a process called vectorization. Let's vectorize Crime and Punishment, treating each sentence as a document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "7-8PqwJhAOrH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[His\n",
              " garret was under the roof of a high, five-storied house and was more\n",
              " like a cupboard than a room.,\n",
              " The landlady who provided him with garret,\n",
              " dinners, and attendance, lived on the floor below, and every time\n",
              " he went out he was obliged to pass her kitchen, the door of which\n",
              " invariably stood open.,\n",
              " And each time he passed, the young man had a\n",
              " sick, frightened feeling, which made him scowl and feel ashamed.]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "crimespans = list(crimespacy.sents)\n",
        "crimespans[42:45] # Let's view three sample sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "iAHlVE8AA_eF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "spacy.tokens.span.Span"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(crimespans[42]) # Check on the type of a single sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "f9ilXNA1BYUJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "scipy.sparse._csr.csr_matrix"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a vectorizer using the powerful Sci-Kit Learn\" library.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Instantiate a vectorizer, removing stopwords, setting min doc frequency\n",
        "vectorizer = CountVectorizer(min_df=1, stop_words='english', lowercase=True)\n",
        "\n",
        "crimesparse = vectorizer.fit_transform([ t.text for t in crimespans])\n",
        "type(crimesparse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "DwO1fkeWGCpv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(14177, 9295)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# A sparse matrix DTM is excellent for efficient storage, but to do useful\n",
        "# manipulations, we will need to blow it up into a data frame.\n",
        "import pandas as pd\n",
        "dtmDF = pd.DataFrame(crimesparse.toarray(),\n",
        "                      columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "dtmDF.shape # Make sure you know what these numbers are: Confirm with your partner!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Lv7BDlLX6DJh"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>14</th>\n",
              "      <th>1849</th>\n",
              "      <th>1859</th>\n",
              "      <th>1861</th>\n",
              "      <th>1864</th>\n",
              "      <th>1880</th>\n",
              "      <th>2554</th>\n",
              "      <th>47</th>\n",
              "      <th>_a</th>\n",
              "      <th>_a_</th>\n",
              "      <th>...</th>\n",
              "      <th>zest</th>\n",
              "      <th>zeus</th>\n",
              "      <th>zigzags</th>\n",
              "      <th>zimmerman</th>\n",
              "      <th>zossimov</th>\n",
              "      <th>æsthetic</th>\n",
              "      <th>æsthetically</th>\n",
              "      <th>æsthetics</th>\n",
              "      <th>éternelle_</th>\n",
              "      <th>êtes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14172</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14173</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14174</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14175</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14176</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14177 rows × 9295 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       14  1849  1859  1861  1864  1880  2554  47  _a  _a_  ...  zest  zeus  \\\n",
              "0       0     0     0     0     0     0     1   0   0    0  ...     0     0   \n",
              "1       0     0     0     0     0     0     0   0   0    0  ...     0     0   \n",
              "2       0     0     0     0     0     0     0   0   0    0  ...     0     0   \n",
              "3       0     0     0     0     0     0     0   0   0    0  ...     0     0   \n",
              "4       0     0     0     0     0     0     0   0   0    0  ...     0     0   \n",
              "...    ..   ...   ...   ...   ...   ...   ...  ..  ..  ...  ...   ...   ...   \n",
              "14172   0     0     0     0     0     0     0   0   0    0  ...     0     0   \n",
              "14173   0     0     0     0     0     0     0   0   0    0  ...     0     0   \n",
              "14174   0     0     0     0     0     0     0   0   0    0  ...     0     0   \n",
              "14175   0     0     0     0     0     0     0   0   0    0  ...     0     0   \n",
              "14176   0     0     0     0     0     0     1   0   0    0  ...     0     0   \n",
              "\n",
              "       zigzags  zimmerman  zossimov  æsthetic  æsthetically  æsthetics  \\\n",
              "0            0          0         0         0             0          0   \n",
              "1            0          0         0         0             0          0   \n",
              "2            0          0         0         0             0          0   \n",
              "3            0          0         0         0             0          0   \n",
              "4            0          0         0         0             0          0   \n",
              "...        ...        ...       ...       ...           ...        ...   \n",
              "14172        0          0         0         0             0          0   \n",
              "14173        0          0         0         0             0          0   \n",
              "14174        0          0         0         0             0          0   \n",
              "14175        0          0         0         0             0          0   \n",
              "14176        0          0         0         0             0          0   \n",
              "\n",
              "       éternelle_  êtes  \n",
              "0               0     0  \n",
              "1               0     0  \n",
              "2               0     0  \n",
              "3               0     0  \n",
              "4               0     0  \n",
              "...           ...   ...  \n",
              "14172           0     0  \n",
              "14173           0     0  \n",
              "14174           0     0  \n",
              "14175           0     0  \n",
              "14176           0     0  \n",
              "\n",
              "[14177 rows x 9295 columns]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dtmDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "s5doBSS5MKoW"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.int64(19)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can look up any word in the DTM by name and find out how frequently it occurs.\n",
        "dtmDF['priest'].sum() # We're computing the column sum of word counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "rh910qxKMmNE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.int64(166)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3.8: Get a total frequency count for a different word.\n",
        "\n",
        "# Solution\n",
        "dtmDF['money'].sum() # We're computing the column sum of word counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Wofzzc3ENDKb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('raskolnikov', np.int64(785)),\n",
              " ('know', np.int64(529)),\n",
              " ('said', np.int64(519)),\n",
              " ('did', np.int64(497)),\n",
              " ('come', np.int64(479)),\n",
              " ('man', np.int64(479)),\n",
              " ('don', np.int64(464)),\n",
              " ('like', np.int64(453)),\n",
              " ('sonia', np.int64(402)),\n",
              " ('time', np.int64(385)),\n",
              " ('went', np.int64(356)),\n",
              " ('razumihin', np.int64(347)),\n",
              " ('dounia', np.int64(325)),\n",
              " ('thought', np.int64(306)),\n",
              " ('ivanovna', np.int64(304)),\n",
              " ('say', np.int64(296)),\n",
              " ('looked', np.int64(293)),\n",
              " ('suddenly', np.int64(293)),\n",
              " ('little', np.int64(288)),\n",
              " ('petrovitch', np.int64(287))]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's make a complete frequency list of all words (columns)\n",
        "wordfreqs = [ (word, dtmDF[word].sum()) for word in vectorizer.get_feature_names_out()]\n",
        "wordfreqs.sort(key=lambda w: w[1], reverse=True)\n",
        "# Show the top 20 items\n",
        "wordfreqs[0:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "WNuwmdAzRTth"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " ...]"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Rodio Raskolnikov and Dmitri Prokofych Razumikhin are focal characters in the book,\n",
        "# so it is pretty cool that their names are among the most frequently appearing terms in our DTM.\n",
        "\n",
        "# Question 3.9:\n",
        "# Make a list of *row sums* from our dtm using dtmDF.sum(axis=1).\n",
        "# Examine this list to see if there are any documents that have a row sum of zero.\n",
        "# What would this imply, if you found it?\n",
        "# these are spaces in the book between the paragraphs\n",
        "\n",
        "# Solution\n",
        "sorted(dtmDF.sum(axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "xSI1Fxe8tm-O"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "318"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's find out all of the words that are included in the default list of stop words used by CountVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "stop_words = text.ENGLISH_STOP_WORDS\n",
        "len(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "bSHKdjhot-k3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "frozenset({'himself', 'due', 'towards', 'beyond', 'noone', 'therefore', 'our', 'every', 'and', 'only', 'others', 'seeming', 'through', 'per', 'three', 'last', 'are', 'since', 'co', 'why', 'whereafter', 'someone', 'give', 'never', 'anyway', 'becoming', 'neither', 'herself', 'toward', 'side', 'with', 'in', 'that', 'system', 'else', 'becomes', 'via', 'although', 'yourselves', 'whose', 'get', 'cannot', 'should', 'un', 'over', 'up', 'together', 'until', 'upon', 'among', 'eg', 'anywhere', 'put', 'nor', 'once', 'you', 'became', 'without', 'de', 'against', 'within', 'bill', 'were', 'around', 'ever', 'first', 'hundred', 'namely', 'name', 'anyhow', 'below', 'former', 'one', 'hereby', 'ie', 'elsewhere', 'along', 'yourself', 'to', 'two', 'or', 'between', 'has', 'before', 'most', 'they', 'as', 'out', 'herein', 're', 'further', 'any', 'whom', 'perhaps', 'empty', 'detail', 'everything', 'next', 'wherein', 'under', 'her', 'a', 'been', 'mill', 'such', 'many', 'eight', 'seem', 'about', 'often', 'anything', 'yet', 'both', 'throughout', 'become', 'least', 'sincere', 'very', 'my', 'from', 'call', 'is', 'was', 'might', 'fifteen', 'your', 'couldnt', 'seemed', 'sometime', 'somehow', 'except', 'cry', 'than', 'however', 'again', 'third', 'must', 'twelve', 'may', 'find', 'almost', 'thereupon', 'serious', 'because', 'his', 'across', 'always', 'whenever', 'it', 'whither', 'even', 'too', 'thereafter', 'nothing', 'then', 'something', 'hereafter', 'onto', 'keep', 'he', 'where', 'amoungst', 'off', 'nevertheless', 'anyone', 'nobody', 'rather', 'how', 'the', 'yours', 'sometimes', 'hereupon', 'be', 'either', 'him', 'whoever', 'thick', 'other', 'ten', 'behind', 'whatever', 'whereas', 'con', 'here', 'i', 'into', 'whence', 'full', 'me', 'mine', 'not', 'move', 'wherever', 'own', 'hence', 'eleven', 'those', 'sixty', 'interest', 'latterly', 'also', 'hers', 'their', 'what', 'somewhere', 'whole', 'beforehand', 'being', 'forty', 'this', 'myself', 'thereby', 'which', 'done', 'do', 'everyone', 'hasnt', 'beside', 'formerly', 'nine', 'thin', 'on', 'during', 'six', 'etc', 'thus', 'will', 'had', 'much', 'ltd', 'though', 'besides', 'latter', 'already', 'no', 'have', 'take', 'show', 'these', 'when', 'fire', 'all', 'meanwhile', 'an', 'therein', 'cant', 'everywhere', 'us', 'enough', 'above', 'we', 'thence', 'would', 'who', 'each', 'fifty', 'several', 'by', 'but', 'part', 'mostly', 'describe', 'alone', 'themselves', 'found', 'go', 'whether', 'back', 'itself', 'some', 'am', 'there', 'less', 'nowhere', 'down', 'five', 'four', 'afterwards', 'front', 'ours', 'whereupon', 'could', 'otherwise', 'inc', 'well', 'so', 'now', 'top', 'made', 'while', 'please', 'them', 'she', 'whereby', 'more', 'few', 'twenty', 'of', 'fill', 'ourselves', 'its', 'amongst', 'for', 'indeed', 'another', 'after', 'can', 'still', 'amount', 'thru', 'none', 'at', 'bottom', 'if', 'seems', 'see', 'same', 'moreover'})\n"
          ]
        }
      ],
      "source": [
        "# Question 3.10:\n",
        "# Print out the contents of stop_words.\n",
        "# Review it carefully. Are there any surprises?\n",
        "# the numbers are suprising to me since they give meaning by stating a quantity\n",
        "\n",
        "# Solution\n",
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "T1q4Vc_uUYfR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((775, 9295), (343, 9295))"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's conclude with a primitive analysis of the dtm.\n",
        "# First we'll make two subsets of our data, based on mentions of characters:\n",
        "raskolDF = dtmDF[dtmDF.raskolnikov > 0]\n",
        "razumihinDF = dtmDF[dtmDF.razumihin > 0]\n",
        "raskolDF.shape, razumihinDF.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "QOisk1ZUVU9H"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(3.1666666666666665)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This creates a ratio of the number of times the word good is mentioned in each of the two data subsets.\n",
        "raskolDF['good'].sum()/razumihinDF['good'].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "_EWnAHHTglVt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(1.9090909090909092)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 3.11: Obtain a ratio of total word frequency for a word other than good\n",
        "\n",
        "# Solution\n",
        "raskolDF['time'].sum()/razumihinDF['time'].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "dwa0NAn0ZDLH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "scipy.sparse._csr.csr_matrix"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 3.12: Revectorize Crime and Punishment sentences with TF-IDF\n",
        "\n",
        "# Solution\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfvectorizer = TfidfVectorizer(min_df=1, stop_words='english', lowercase=True)\n",
        "\n",
        "crimesparsetf = tfvectorizer.fit_transform([ t.text for t in crimespans])\n",
        "type(crimesparsetf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Sq7aU1_Vg6R_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(14177, 9295)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 3.13: Convert vectorization results (the TF-IDF DTM) to pandas data frame\n",
        "\n",
        "# Solution\n",
        "dtmDFtf = pd.DataFrame(crimesparsetf.toarray(),\n",
        "                      columns=tfvectorizer.get_feature_names_out())\n",
        "\n",
        "dtmDFtf.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "2pDtQ3REg6Du"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((775, 9295), (343, 9295))"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 3.14:\n",
        "# Repeat one or more of diagnostic tests demonstrated for the count vectorization.\n",
        "\n",
        "# Solution\n",
        "raskolDFtf = dtmDFtf[dtmDFtf.raskolnikov > 0]\n",
        "razumihinDFtf = dtmDFtf[dtmDFtf.razumihin > 0]\n",
        "raskolDFtf.shape, razumihinDFtf.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "jLuQET7hg5g_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(2.5104687596893855)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 3.15: Repeat ratio tests, comparing the contents of the DTMs for the two characters.\n",
        "\n",
        "# Solution\n",
        "raskolDFtf['time'].sum()/razumihinDFtf['time'].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh5P4HwR5Aml"
      },
      "source": [
        "##Part 4\n",
        "## Pointwise Mutual Information (PMI)\n",
        "\n",
        "PMI calculates the probability of the co-occurence of two words using the probability of each word independently as a baseline.\n",
        "\n",
        "Here's an example: Let's say that \"fish\" occurs five times in 100 words, while \"cake\" appears eight times. The combination \"fish cake\" appears 3 times. Now run the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "h4IpmsGU5ECY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.9068905956085187\n"
          ]
        }
      ],
      "source": [
        "pfish = 5/100\n",
        "pcake = 8/100\n",
        "pfishcake = 3/100\n",
        "\n",
        "import math # We will need the log2() function\n",
        "pmi = math.log2( pfishcake / (pfish * pcake))\n",
        "print(pmi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ci9nHHtiQzd"
      },
      "source": [
        "So based on this result, Fish and Cake are occuring together somewhat more frequently than would be expected based on how often they appear independently. You can fiddle around with the probability values to see how it affects the PMI calculation.\n",
        "\n",
        "To do this kind of analysis at scale, we'll pull in some code from the nltk.collocations module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "LTw6dkhA5Gd3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(nltk.metrics.association.BigramAssocMeasures,\n",
              " nltk.collocations.BigramCollocationFinder)"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.collocations import BigramAssocMeasures\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "finder = BigramCollocationFinder.from_words(crimetokens)\n",
        "type(bigram_measures), type(finder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "GxTr-WcA5mKG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(('_die', 'wäsche_'), 16.952690065907774),\n",
              " (('_sein', 'rock_'), 16.952690065907774),\n",
              " (('_special', 'case_'), 16.952690065907774),\n",
              " (('_tout', 'court_'), 16.952690065907774),\n",
              " (('alexandr', 'grigorievitch'), 16.952690065907774),\n",
              " (('du', 'mehr'), 16.952690065907774),\n",
              " (('ebook', '2554'), 16.952690065907774),\n",
              " (('en', 'va-t-en'), 16.952690065907774),\n",
              " (('gutenberg', 'ebook'), 16.952690065907774),\n",
              " (('unfailing', 'regularity'), 16.952690065907774),\n",
              " (('willst', 'du'), 16.952690065907774)]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The NLTK Pointwise Mutual Information scoring function, PMI,\n",
        "# scores the bigrams by taking into account the frequency of the two component words.\n",
        "# When infrequent words make a bigram they get a boost in PMI.\n",
        "# A higher score thus means a more interesting bigram.\n",
        "finder.apply_freq_filter(2)\n",
        "scored = finder.score_ngrams(bigram_measures.pmi)\n",
        "# Examine the pairs with PMI greater than 16.5 (an arbitrary number chosen simply to keep the list short).\n",
        "[bg for bg in scored if bg[1] > 16.5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "hhdAo8Yn7GBt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(('_die', 'wäsche_'), 16.952690065907774),\n",
              " (('_sein', 'rock_'), 16.952690065907774),\n",
              " (('_special', 'case_'), 16.952690065907774),\n",
              " (('_tout', 'court_'), 16.952690065907774),\n",
              " (('alexandr', 'grigorievitch'), 16.952690065907774),\n",
              " (('du', 'mehr'), 16.952690065907774),\n",
              " (('ebook', '2554'), 16.952690065907774),\n",
              " (('en', 'va-t-en'), 16.952690065907774),\n",
              " (('gutenberg', 'ebook'), 16.952690065907774),\n",
              " (('unfailing', 'regularity'), 16.952690065907774),\n",
              " (('willst', 'du'), 16.952690065907774),\n",
              " (('cracking', 'nuts'), 16.36772756518662),\n",
              " (('va-t-en', 'guerre'), 16.36772756518662),\n",
              " (('_vater', 'aus'), 16.367727565186616),\n",
              " (('aus', 'berlin_'), 16.367727565186616),\n",
              " (('cleft', 'palate'), 15.952690065907774),\n",
              " (('cleft', 'palates'), 15.952690065907774),\n",
              " (('krestovsky', 'island'), 15.952690065907774),\n",
              " (('mental', 'diseases'), 15.952690065907774),\n",
              " (('ninety', 'versts'), 15.952690065907774),\n",
              " (('penal', 'servitude'), 15.952690065907774),\n",
              " (('titular', 'counsellor'), 15.952690065907774),\n",
              " (('darya', 'frantsovna'), 15.952690065907772),\n",
              " (('_weekly', 'review_'), 15.782765064465462),\n",
              " (('acute', 'angle'), 15.782765064465462),\n",
              " (('_du', 'hast'), 15.630761971020412),\n",
              " (('ivan', 'afanasyvitch'), 15.630761971020412),\n",
              " (('medium', 'height'), 15.630761971020412),\n",
              " (('notorious', 'behaviour.'), 15.630761971020412),\n",
              " (('political', 'conspirator'), 15.630761971020412),\n",
              " (('wadded', 'quilt'), 15.630761971020412),\n",
              " (('commit', 'breaches'), 15.367727565186618),\n",
              " (('commonplace', 'simpleton'), 15.367727565186618),\n",
              " (('habits', 'associated'), 15.367727565186618),\n",
              " (('inward', 'conflict'), 15.367727565186618),\n",
              " (('mutton-chop', 'whiskers'), 15.367727565186618),\n",
              " (('sentimental', 'effusions'), 15.367727565186618),\n",
              " (('successive', 'evenings'), 15.367727565186618),\n",
              " (('cinq', 'sous'), 15.30883387613305),\n",
              " (('religious', 'maniac'), 15.14533514385017),\n",
              " (('petrovsky', 'park'), 15.045799470299254),\n",
              " (('vassilyevsky', 'ostrov'), 15.045799470299254),\n",
              " (('wandered', 'aimlessly'), 15.045799470299254),\n",
              " (('foreign', 'parts'), 14.952690065907774),\n",
              " (('ostrich', 'feather'), 14.952690065907774),\n",
              " (('project', 'gutenberg'), 14.952690065907774),\n",
              " (('flame-coloured', 'feather'), 14.952690065907772),\n",
              " (('praskovya', 'pavlovna'), 14.782765064465462),\n",
              " (('universal', 'history'), 14.782765064465462),\n",
              " (('hectic', 'flush'), 14.78276506446546),\n",
              " (('american', 'leather'), 14.630761971020412),\n",
              " (('chamois', 'leather'), 14.630761971020412),\n",
              " (('thickly', 'smeared'), 14.630761971020412),\n",
              " (('goatskin', 'shoes'), 14.630761971020409),\n",
              " (('furthest', 'limit'), 14.560372643129014),\n",
              " (('suppressed', 'enthusiasm'), 14.560372643129014)]"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 3.16:\n",
        "# Lower the PMI threshold to 14 or 15 and examine some of the additional bigrams.\n",
        "# What do you see? Are high PMI bigrams useful at telling us something about the corpus?\n",
        "# somewhat from the high PMI bigrams I maybe able tell what type of story its going to be\n",
        "\n",
        "# Solution\n",
        "[bg for bg in scored if bg[1] > 14.5]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UpdpUdlA4F6"
      },
      "source": [
        "## Lab 0 ##\n",
        "\n",
        "Copyright 2022, Jeffrey Stanton, Edited by Preeti Jagadev\n",
        "\n",
        "## Python Demonstration ##\n",
        "\n",
        "## Section 0.1 ##\n",
        "\n",
        "Here are a few demonstrations and exercises to help you get started with Python and learn the key programming constructs needed to do text processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUrSarPHTl1F",
        "outputId": "d6072d6e-0507-418e-bbe3-8bf38a3ae035"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello world!\n"
          ]
        }
      ],
      "source": [
        "# First line of code when learning a new programming language:\n",
        "print(\"Hello world!\") # Print is a built-in function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KJYO6L1CcDD"
      },
      "source": [
        "Note that any text that appears after the # is treated as a comment rather than computer code. Use comments freely to help you remember what a piece of code does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lDvWvdQtW3Ir"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15.333333333333334\n"
          ]
        }
      ],
      "source": [
        "# Question 1: Make up a multiline piece of text and print it.\n",
        "\n",
        "# Solution\n",
        "num_A = 34\n",
        "num_B = 12\n",
        "num_C = 3\n",
        "print((num_A + num_B)/num_C) # What is (34 + 12)/ 3?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "piQcd_9JXexz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.1415926\n"
          ]
        }
      ],
      "source": [
        " print(3.1415926) # It is easy to print numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q5baaFEtbGZH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This value of pi, 3.14159, has 5 digits of precision!\n"
          ]
        }
      ],
      "source": [
        "pi = 3.1415926 #Assigns the value of pi (up to 7 decimal digits).\n",
        "precise = 5 #Specifies that we want to keep 5 digits after the decimal point.\n",
        "pi = round(pi, ndigits=precise) #Rounds pi to 5 decimal places, resulting in 3.14159.\n",
        "print(\"This value of pi, {0}, has {1} digits of precision!\".format(pi, precise)) #This is a format string, where {0} and {1} are placeholders.\n",
        "                                                                                 # These get replaced with the values you pass into .format(...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aIDvuHuHCMDt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avogadro constant is 6.022 * 10^23, rounded to 3 decimal places\n"
          ]
        }
      ],
      "source": [
        "# Question 2\n",
        "# Make up your own number, round it to three digits and print it within an explanatory sentence.\n",
        "\n",
        "# Solution\n",
        "avogaC = 6.02214076 # Avogadro constant * 10^23\n",
        "rnd = 3\n",
        "avogaC = round(avogaC, ndigits= rnd)\n",
        "print(\"Avogadro constant is {0} * 10^23, rounded to {1} decimal places\".format(avogaC, rnd))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3pOSq5iqcmG"
      },
      "source": [
        "## Section 0.2 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wux9sqnQCsEj"
      },
      "source": [
        "Now we will switch gears and examine packages (AKA libraries) of code that can be added into your Pythion programming environment. Packages are what makes Python so useful. There are thousands of them but most projects use the same few packages over and over."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "m9nswLMvXdrw"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<cyfunction default_rng at 0x000001A00EB662C0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# numpy is Numerical Python, a standard and widely used library of numeric functions for science and engineering.\n",
        "import numpy # Import the numpy package\n",
        "from numpy.random import default_rng # Get random number generator functions\n",
        "default_rng # Shows as a function that is from numpy.random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6DiTb_2soFnZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.39361072, -0.37872839,  0.35333259, -1.21058891,  0.48746962,\n",
              "       -1.23738192,  0.51127416, -0.31555205,  1.5911041 ,  0.1143816 ])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rng = default_rng() # Create an instance of a random number generator\n",
        "normVals = rng.standard_normal(10) # Generate 10 standard normal values\n",
        "normVals # Reported as an \"array\" - a special numpy type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR5-3rRsDHa8"
      },
      "source": [
        "In the code above, we used something called a \"**class.**\"\n",
        "You can think of a class like a blueprint or recipe that tells the computer how to make a certain kind of object.\n",
        "\n",
        "But just having the blueprint isn’t enough, we need to build something from it. That step is called \"instantiation\", a big word that simply means \"make a copy I can actually use.\"\n",
        "\n",
        "Once we make that copy, we can start using it to do things, like creating random numbers, storing data, or running special actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1HfqHeriwxEI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The negative random number was: -0.7233831846835561\n"
          ]
        }
      ],
      "source": [
        "oneNormVal = rng.standard_normal(1) # Generate a standard normal value\n",
        "\n",
        "if oneNormVal > 0:\n",
        "  # Note the indentation\n",
        "  print(f\"The positive random number was: {oneNormVal[0]}\")\n",
        "elif oneNormVal < 0:\n",
        "  print(f\"The negative random number was: {oneNormVal[0]}\")\n",
        "else:\n",
        "  print(\"A zero? We almost never get exactly zero. . .\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhesG_myDsgG"
      },
      "source": [
        "Python is one of the few languages where white space really matters. In the code above the indentation on the print() statements shows that they \"belong to\" either the if, the elif, or the else clauses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Fe7sQC24ULiH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.45316244]\n",
            "[0.54420173]\n",
            "[-0.35139771]\n",
            "[-1.07372507]\n",
            "[-0.25120236]\n",
            "[0.11946048]\n",
            "[1.89581142]\n",
            "[-2.24273531]\n",
            "[-1.48554829]\n",
            "[-1.36055897]\n",
            "[-0.87725549]\n",
            "[-0.91784509]\n",
            "[0.60910047]\n",
            "[-1.42325944]\n",
            "[1.06978946]\n",
            "[-1.34639368]\n",
            "[1.02821576]\n",
            "[0.11963741]\n",
            "[-0.09596573]\n",
            "[0.38653889]\n",
            "[0.28425324]\n",
            "[0.29631374]\n",
            "[0.43762214]\n",
            "[-0.66501112]\n",
            "[0.36676528]\n",
            "[1.06763593]\n",
            "[-0.62082687]\n",
            "[0.43245727]\n",
            "[-1.18382472]\n",
            "[-1.28537161]\n",
            "[1.50519218]\n",
            "[-0.33473083]\n",
            "[0.05532771]\n",
            "[-0.21879335]\n",
            "[0.50017797]\n",
            "[1.1395039]\n",
            "[-0.67867245]\n",
            "[0.6685387]\n",
            "[-0.28118166]\n",
            "[-0.55419118]\n",
            "[0.44361302]\n",
            "[-0.74821738]\n",
            "[1.06142453]\n",
            "[-0.06802532]\n",
            "[-0.34115346]\n",
            "[0.09535291]\n",
            "[1.13710134]\n",
            "[1.42612747]\n",
            "[1.72618077]\n",
            "[-0.9014151]\n",
            "[-0.87743902]\n",
            "[-1.28465905]\n",
            "[1.15860044]\n",
            "[-0.96605669]\n",
            "[0.33341735]\n",
            "[0.45386659]\n",
            "[1.46574768]\n",
            "[-0.60381326]\n",
            "This value, [2.52976317], caused the loop to exit!\n"
          ]
        }
      ],
      "source": [
        "# Now show how a while statement works by drawing and printing random numbers.\n",
        "# Only stop when you get one that is pretty large. In a normal distribution\n",
        "# about 95% of the values are between -2 and +2.\n",
        "\n",
        "oneNormVal = rng.standard_normal(1) # Generate a standard normal value\n",
        "\n",
        "while oneNormVal < 2:\n",
        "  print(oneNormVal) # Show this one\n",
        "  oneNormVal = rng.standard_normal(1) # Draw another one\n",
        "\n",
        "# After exiting the loop, print the final value\n",
        "print(f\"This value, {oneNormVal}, caused the loop to exit!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5fbj8iNrOSD2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "The for loop is done.\n",
            "[1, 2, 3, 4]\n"
          ]
        }
      ],
      "source": [
        "# Now show how a for loop works.\n",
        "\n",
        "valueList = range(1, 5)\n",
        "\n",
        "for i in valueList:\n",
        "  print(i)\n",
        "else:\n",
        "  print('The for loop is done.')\n",
        "\n",
        "print(list(valueList))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fYH-ebhBQabg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "with\n",
            "so\n",
            "many\n",
            "years\n",
            "in\n",
            "the\n",
            "world\n"
          ]
        }
      ],
      "source": [
        "# We can also do a for loop with other kinds of data\n",
        "wordList = ['with', 'so', 'many', 'years', 'in', 'the', 'world']\n",
        "\n",
        "for word in wordList:\n",
        "  print(word) # Note how word takes on all the different values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LzoxgDZBRB0g"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "with\n",
            "so\n",
            "many\n",
            "years\n",
            "All finished!\n"
          ]
        }
      ],
      "source": [
        "# It is possible to jump out of a loop before it is\n",
        "# finished by using break:\n",
        "\n",
        "wordList = ['with', 'so', 'many', 'years', 'in', 'the', 'world']\n",
        "\n",
        "for word in wordList:\n",
        "  print(word) # The indentation shows that the if is inside the loop\n",
        "  if word == 'years':\n",
        "    break # This causes the loop to exit when the if is true\n",
        "\n",
        "print(\"All finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zKYjlCwlKjfS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "with\n",
            "so\n",
            "years\n",
            "in\n",
            "the\n",
            "world\n",
            "That's all folks!\n"
          ]
        }
      ],
      "source": [
        "# This code prints each word in a list except the word \"many\" by using the continue command.\n",
        "wordList = ['with', 'so', 'many', 'years', 'in', 'the', 'world']\n",
        "\n",
        "for word in wordList:\n",
        "  if word == 'many':\n",
        "    continue # If the word is \"many\", Skip the rest of the loop for this word and go to the next one. So \"many\" is not printed.\n",
        "\n",
        "  # This line prints the word, but only if it's not \"many\".\n",
        "  print(word)\n",
        "\n",
        "print(\"That's all folks!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoAPHDd3f-Qb"
      },
      "source": [
        "## Section 0.3 ##\n",
        "\n",
        "A **list comprehension** is a shortcut way to write a **for loop**.\n",
        "\n",
        "Instead of writing a whole **for loop** in several lines, you can write it all in one line, and it still works the same!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6Exsy2Q9UmAs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['With', 'so', 'many', 'YEARS', 'in', 'the', 'world']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wordList = ['With', 'so', 'many', 'YEARS', 'in', 'the', 'world']\n",
        "\n",
        "[w for w in wordList] #It means: Take each word w in wordList, and make a new list with them. So basically… it just copies the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WvoRxKiLVK79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['with', 'so', 'many', 'years', 'in', 'the', 'world']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The real fun with a list comprehension begins when you use the iteration to do some processing, in this case converting to lowercase:\n",
        "[w.lower() for w in wordList]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rO8FN1lFVZkG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['with', 'so', 'many', 'in', 'the', 'world']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can make the list comprehension selective about what it does and does not process.\n",
        "[w.lower() for w in wordList if w != \"YEARS\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gUS6RivaVu78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The type() command is built-in to Python and it reveals what a data object is.\n",
        "type(wordList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "odBNfJjMWJWK"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# A list is a structure that contains \"pointers\" to the individual items in the list.\n",
        "# Each item in the list has its own type:\n",
        "type(wordList[0]) # The first element in Python data structures is always indexed with 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Z1AmDdVNWXfu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['With', 'YEARS', 'in', 'many', 'so', 'the', 'world']\n"
          ]
        }
      ],
      "source": [
        "# Because a list is just a set of pointers, it is easily rearranged.\n",
        "# In Python, this is called \"mutability.\"\n",
        "wordList.sort() #It rearranges the items in the list in alphabetical order (or numerical order, if it's numbers).\n",
        "print(wordList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Kx7t-VoyWsYG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['in', 'many', 'so', 'the', 'with', 'world', 'years']\n"
          ]
        }
      ],
      "source": [
        "# The pointers have been reordered, such that the result is now alphabetical.\n",
        "# Notice the interesting result that words with capital letters are earlier in the sorted list.\n",
        "# If we wanted true alpha order we would need consistent casing:\n",
        "lowerWordList = [w.lower() for w in wordList]\n",
        "lowerWordList.sort()\n",
        "print(lowerWordList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "s0wgO5qcXTdk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('To', 'be', 'or', 'not', 'to', 'be', '.')\n"
          ]
        }
      ],
      "source": [
        "# Another data class, called a tuple, is immutable.\n",
        "# Use parentheses to create a tuple.\n",
        "\n",
        "wordTuple = ('To', 'be', 'or', 'not', 'to', 'be', '.')\n",
        "print(wordTuple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZQrI5k3lXw_R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Long', 'YEARS', 'in', 'many', 'so', 'the', 'world']\n"
          ]
        }
      ],
      "source": [
        "# We CAN change an entry in a list:\n",
        "wordList[0] = \"Long\"\n",
        "print(wordList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "NwK17hh0X-On"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'tuple' object does not support item assignment",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# But we cannot change an entry in a tuple.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Uncomment the following line of code to test this:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mwordTuple\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m = \u001b[33m\"\u001b[39m\u001b[33mFish\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# Throws an error\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Good notebook programming practice suggests that you should not\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# leave code in the notebook that throws errors. For convenenience\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# the next time the code is run, we might want to use \"Run All\" and\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# we want the notebook to work without errors all the way to the end.\u001b[39;00m\n",
            "\u001b[31mTypeError\u001b[39m: 'tuple' object does not support item assignment"
          ]
        }
      ],
      "source": [
        "# But we cannot change an entry in a tuple.\n",
        "# Uncomment the following line of code to test this:\n",
        "\n",
        "wordTuple[0] = \"Fish\" # Throws an error\n",
        "\n",
        "# Good notebook programming practice suggests that you should not\n",
        "# leave code in the notebook that throws errors. For convenenience\n",
        "# the next time the code is run, we might want to use \"Run All\" and\n",
        "# we want the notebook to work without errors all the way to the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fQH9TUOkYOJQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tuple, str)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Even so, we can find out the type of wordTuple and use slicing to find out the type of one entry:\n",
        "type(wordTuple), type(wordTuple[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Up5X4TQ5Yhk-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['__add__',\n",
              " '__class__',\n",
              " '__class_getitem__',\n",
              " '__contains__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__getnewargs__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__iter__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__mul__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__rmul__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " 'count',\n",
              " 'index']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dir(wordTuple) #tells you: \"Here are all the things you can do with this tuple.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rGos1AKsY9BH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Once you know the name of a bound method, it is easy to look up the help online.\n",
        "# Google colab also has pop-up help that appears as you type.\n",
        "# Try retyping this line, keeping your eyes open for when the pop-up help appears:\n",
        "wordTuple.count(\"be\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "GmHC8OFPZm_S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'be': 'VERB', 'question': 'NOUN', 'or': 'CONJ', 'nobler': 'ADJ'}\n"
          ]
        }
      ],
      "source": [
        "# One other data structure that is valuable to know about is the dictionary.\n",
        "# Dictionaries are specialized for doing fast lookups. Use the curly brace to define a dictionary:\n",
        "wordTypes = {'be' : 'VERB',\n",
        "             'question' : 'NOUN',\n",
        "             'or' : 'CONJ',\n",
        "             'nobler' : \"ADJ\"}\n",
        "             # Every entry is a \"key-value\" pair\n",
        "\n",
        "print(wordTypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "IWcc3JGbar33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'CONJ'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now we can look up the associated value for any key:\n",
        "wordTypes['or']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "LdX39bKkbK5G"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'be': 'VERB', 'question': 'NOUN', 'or': 'CONJ', 'nobler': 'ADJ', 'are': 'VERB'}\n"
          ]
        }
      ],
      "source": [
        "# The keys in a dictionary must be immutable, but we can add new entries:\n",
        "wordTypes.update({'are' : 'VERB'})\n",
        "print(wordTypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ZDCr8ElVb_V0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# You can probe a dictionary to see if it contains a key:\n",
        "result = 'be' in wordTypes #checks:\"Does the dictionary wordTypes have 'be' as a key?\"\n",
        "                          # It returns True if 'be' is a key in the dictionary. Otherwise, it returns False.\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92EfYZLGgn-g"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'not', 'be', 'to', 'or', '.'}\n"
          ]
        }
      ],
      "source": [
        "# A set is like a dictionary that is all keys and no values.\n",
        "# We can make any list or tuple into a set with the set() function.\n",
        "# A set is a data structure in Python that can only contain unique elements. \n",
        "# This automatically discards all duplicate words, leaving only one instance of each word.\n",
        "\n",
        "wordTuple = ('to', 'be', 'or', 'not', 'to', 'be', '.')\n",
        "wordSet = set(wordTuple)\n",
        "print(wordSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "7YmBEDyuhgQC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "set"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Notice how the curly braces surround the output.\n",
        "# We can create our ownset from scratch using the curly braces notation:\n",
        "wordSet2 = {'be', '.', 'to', 'or', 'not', 'that'}\n",
        "type(wordSet2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "F4ZoIAzdjSGw"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['To',\n",
              " 'be,',\n",
              " 'or',\n",
              " 'not',\n",
              " 'to',\n",
              " 'be,',\n",
              " 'that',\n",
              " 'is',\n",
              " 'the',\n",
              " 'question:',\n",
              " 'Whether',\n",
              " \"'tis\",\n",
              " 'nobler',\n",
              " 'in',\n",
              " 'the',\n",
              " 'mind',\n",
              " 'to',\n",
              " 'suffer',\n",
              " 'The',\n",
              " 'slings',\n",
              " 'and',\n",
              " 'arrows',\n",
              " 'of',\n",
              " 'outrageous',\n",
              " 'fortune,',\n",
              " 'Or',\n",
              " 'to',\n",
              " 'take',\n",
              " 'Arms',\n",
              " 'against',\n",
              " 'a',\n",
              " 'Sea',\n",
              " 'of',\n",
              " 'troubles,',\n",
              " 'And',\n",
              " 'by',\n",
              " 'opposing',\n",
              " 'end',\n",
              " 'them:',\n",
              " 'to',\n",
              " 'die,',\n",
              " 'to',\n",
              " 'sleep;',\n",
              " 'No',\n",
              " 'more;',\n",
              " 'and',\n",
              " 'by',\n",
              " 'a',\n",
              " 'sleep,',\n",
              " 'to',\n",
              " 'say',\n",
              " 'we',\n",
              " 'end',\n",
              " 'The',\n",
              " 'heart-ache,',\n",
              " 'and',\n",
              " 'the',\n",
              " 'thousand',\n",
              " 'natural',\n",
              " 'shocks',\n",
              " 'That',\n",
              " 'Flesh',\n",
              " 'is',\n",
              " 'heir',\n",
              " 'to?',\n",
              " \"'Tis\",\n",
              " 'a',\n",
              " 'consummation',\n",
              " 'Devoutly',\n",
              " 'to',\n",
              " 'be',\n",
              " 'wished.',\n",
              " 'To',\n",
              " 'die,',\n",
              " 'to',\n",
              " 'sleep,',\n",
              " 'To',\n",
              " 'sleep,',\n",
              " 'perchance',\n",
              " 'to',\n",
              " 'Dream;',\n",
              " 'aye,',\n",
              " \"there's\",\n",
              " 'the',\n",
              " 'rub,',\n",
              " 'For',\n",
              " 'in',\n",
              " 'that',\n",
              " 'sleep',\n",
              " 'of',\n",
              " 'death,',\n",
              " 'what',\n",
              " 'dreams',\n",
              " 'may',\n",
              " 'come,',\n",
              " 'When',\n",
              " 'we',\n",
              " 'have',\n",
              " 'shuffled',\n",
              " 'off',\n",
              " 'this',\n",
              " 'mortal',\n",
              " 'coil,',\n",
              " 'Must',\n",
              " 'give',\n",
              " 'us',\n",
              " 'pause.']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now that we know about several different data types, not to mention\n",
        "# branching and looping methods let's create our own custom function that\n",
        "# counts the frequency of words in a string.\n",
        "\n",
        "shakes = \"\"\"\n",
        "To be, or not to be, that is the question:\n",
        "Whether 'tis nobler in the mind to suffer\n",
        "The slings and arrows of outrageous fortune,\n",
        "Or to take Arms against a Sea of troubles,\n",
        "And by opposing end them: to die, to sleep;\n",
        "No more; and by a sleep, to say we end\n",
        "The heart-ache, and the thousand natural shocks\n",
        "That Flesh is heir to? 'Tis a consummation\n",
        "Devoutly to be wished. To die, to sleep,\n",
        "To sleep, perchance to Dream; aye, there's the rub,\n",
        "For in that sleep of death, what dreams may come,\n",
        "When we have shuffled off this mortal coil,\n",
        "Must give us pause.\n",
        "\"\"\"\n",
        "\n",
        "shakes.split() #It breaks the text into smaller parts (words) using spaces as the separator.\n",
        "              # So, instead of one big string, you get a list of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "NB7lI3Yzkhul"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hey', 'there', 'Delilah.']\n"
          ]
        }
      ],
      "source": [
        "# Programmers who write a lot of functions will sometimes do so in small steps,\n",
        "# making incremental changes to the capability of the function and testing after each change:\n",
        "def wordCounter(inputString): #You are creating a function called wordCounter.It takes one input, which is a string (here, \"Hey there Delilah.\")\n",
        "  wordList = inputString.split() #This splits the sentence into a list of words wherever there's a space.\n",
        "  print(wordList)\n",
        "wordCounter(\"Hey there Delilah.\") #This runs the function and shows the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "cAao-6pGlAZu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Hey': 1, 'there': 1, 'Delilah.': 2, 'Yes,': 1, 'you': 1}\n"
          ]
        }
      ],
      "source": [
        "# Now let's create an empty dictionary and add counts to it:\n",
        "\n",
        "def wordCounter(inputString):  #You’re defining a function named wordCounter that takes in a string.\n",
        "  wordList = inputString.split() #This splits the sentence into words by spaces.\n",
        "\n",
        "  wordDict = {} #This will store each word and how many times it appears.\n",
        "\n",
        "  for w in wordList:  #You're going through each word one by one.\n",
        "    if w in wordDict:\n",
        "      wordDict[w] = wordDict[w] + 1 # Increment the count\n",
        "    else:\n",
        "      wordDict[w] = 1 # If it’s the first time seeing the word, you set the count to 1.\n",
        "\n",
        "  print(wordDict) #This shows the final count of each word.\n",
        "\n",
        "wordCounter(\"Hey there Delilah. Yes, you Delilah.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "8V1IZh8WmH8j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'To': 3, 'be,': 2, 'or': 1, 'not': 1, 'to': 9, 'that': 2, 'is': 2, 'the': 4, 'question:': 1, 'Whether': 1, \"'tis\": 1, 'nobler': 1, 'in': 2, 'mind': 1, 'suffer': 1, 'The': 2, 'slings': 1, 'and': 3, 'arrows': 1, 'of': 3, 'outrageous': 1, 'fortune,': 1, 'Or': 1, 'take': 1, 'Arms': 1, 'against': 1, 'a': 3, 'Sea': 1, 'troubles,': 1, 'And': 1, 'by': 2, 'opposing': 1, 'end': 2, 'them:': 1, 'die,': 2, 'sleep;': 1, 'No': 1, 'more;': 1, 'sleep,': 3, 'say': 1, 'we': 2, 'heart-ache,': 1, 'thousand': 1, 'natural': 1, 'shocks': 1, 'That': 1, 'Flesh': 1, 'heir': 1, 'to?': 1, \"'Tis\": 1, 'consummation': 1, 'Devoutly': 1, 'be': 1, 'wished.': 1, 'perchance': 1, 'Dream;': 1, 'aye,': 1, \"there's\": 1, 'rub,': 1, 'For': 1, 'sleep': 1, 'death,': 1, 'what': 1, 'dreams': 1, 'may': 1, 'come,': 1, 'When': 1, 'have': 1, 'shuffled': 1, 'off': 1, 'this': 1, 'mortal': 1, 'coil,': 1, 'Must': 1, 'give': 1, 'us': 1, 'pause.': 1}\n"
          ]
        }
      ],
      "source": [
        "# Let's test on a bigger piece of text:\n",
        "\n",
        "wordCounter(shakes) #shakes is a string from Shakespeare’s Hamlet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn-P0cdM0ago"
      },
      "source": [
        "# Tokenizers #\n",
        "\n",
        "## Section 0.4 ##\n",
        "\n",
        "Tokenization is the first step in most NLP processes. Creating a list of tokens from a raw character string is surprisingly complicated. Here's a demonstration of six different tokenizers starting with the simplest pure Python method of splitting on spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "q5sZlNRC2MSA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ain’t isn’t well-known among my co-workers;\n",
            "2/3rds of middle-aged U.S. citizens don’t use it.\n"
          ]
        }
      ],
      "source": [
        "# Tokenization demonstration\n",
        "my_sentence = \"\"\"Ain’t isn’t well-known among my co-workers;\n",
        "2/3rds of middle-aged U.S. citizens don’t use it.\"\"\"\n",
        "print(my_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xquu2uSBMfe0"
      },
      "source": [
        "## Technique 1 ##\n",
        "*  This splits the sentence wherever there’s a space (\" \").\n",
        "*  It creates a list of words, but keeps punctuation attached to words.\n",
        "*  For example, \"hello,\" and \"world!\" would include the comma and exclamation mark.\n",
        "*   Keeps contractions like “It’s” together (which is good!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "KYwQ-UDt2aZo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ain’t', 'isn’t', 'well-known', 'among', 'my', 'co-workers;', '2/3rds', 'of', 'middle-aged', 'U.S.', 'citizens', 'don’t', 'use', 'it.']\n"
          ]
        }
      ],
      "source": [
        "tokens1 = my_sentence.split()\n",
        "print(tokens1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Nx2JnCkZ3fA3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokens1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjPdI3l1NcT-"
      },
      "source": [
        "## Technique 2 ##\n",
        "*  You're now using regular expressions (regex) to split your sentence into words more cleanly than **split()**\n",
        "*  The punctuation is removed (good for clean tokens!)\n",
        "*  Contractions like \"It's\" become two tokens: \"It\" and \"s\" (might not be desirable sometimes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "6o89a1Gn3srk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ain', 't', 'isn', 't', 'well', 'known', 'among', 'my', 'co', 'workers', '2', '3rds', 'of', 'middle', 'aged', 'U', 'S', 'citizens', 'don', 't', 'use', 'it']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
            "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
            "C:\\Users\\Black Knight\\AppData\\Local\\Temp\\ipykernel_12764\\482224652.py:2: SyntaxWarning: invalid escape sequence '\\w'\n",
            "  tokens2 = re.findall(\"[\\w]+\", my_sentence)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "tokens2 = re.findall(\"[\\w]+\", my_sentence)\n",
        "print(tokens2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "kBvLgCiAO_f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokens2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IQGhZ7rPOzO"
      },
      "source": [
        "# Technique 3 #\n",
        "You're now using NLTK’s **word_tokenize ()**, to split text into words and punctuation using natural language rules. This splits your sentence intelligently into: **Words, Numbers, Punctuation** (kept as separate tokens), and **Contractions** (like \"it's\" stay together as \"It\" and 's').\n",
        "\n",
        "**word_tokenize()** knows where to cut even with punctuation and tricky words.Ideal when working with real-world sentences for NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "pv5VScGEPS5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ain', '’', 't', 'isn', '’', 't', 'well-known', 'among', 'my', 'co-workers', ';', '2/3rds', 'of', 'middle-aged', 'U.S.', 'citizens', 'don', '’', 't', 'use', 'it', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\Black\n",
            "[nltk_data]     Knight\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to C:\\Users\\Black\n",
            "[nltk_data]     Knight\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens3 = word_tokenize(my_sentence)\n",
        "print(tokens3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "TsS5Wi0K6-nS"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokens3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y89LEC839C5Q"
      },
      "source": [
        "## Technique 4 ##\n",
        "You're now using **spaCy**, which is a powerful and fast NLP library in Python! This model knows:\n",
        "* How to split text into words (tokenization)\n",
        "* Part-of-speech (noun, verb, etc.)\n",
        "* Lemmas (base forms of words)\n",
        "* Named entities (like places, people, dates)\n",
        "* Sentence boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.2.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (80.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\black knight\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\black knight\\appdata\\roaming\\python\\python313\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: wrapt in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m nlp = \u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men_core_web_sm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\__init__.py:52\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     29\u001b[39m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m     30\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001b[32m     36\u001b[39m ) -> Language:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\util.py:484\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E050.format(name=name))\n",
            "\u001b[31mOSError\u001b[39m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0LkQ5wm9LuX"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nlp' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokens4 = \u001b[43mnlp\u001b[49m(my_sentence)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokens4)\n",
            "\u001b[31mNameError\u001b[39m: name 'nlp' is not defined"
          ]
        }
      ],
      "source": [
        "tokens4 = nlp(my_sentence)\n",
        "print(tokens4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3K2Z5htLEmWA"
      },
      "outputs": [],
      "source": [
        "# Show them one per line\n",
        "[token.text for token in tokens4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlX-szcPEwXA"
      },
      "outputs": [],
      "source": [
        "len(tokens4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loZlOaWPHf6p"
      },
      "source": [
        "## Technique 5 ##\n",
        "You're now using **Keras** (via **TensorFlow**) to tokenize text using **text_to_word_sequence**, which is a simple but effective method for basic NLP tasks like preparing text for deep learning models.\n",
        "\n",
        "This breaks the sentence into words in a simple, clean way:\n",
        "*  Converts all words to lowercase\n",
        "*  Removes all punctuation\n",
        "*  Splits on spaces\n",
        "\n",
        "This method is fast, clean, and made for neural networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "8dVjuGxGHkJM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ain’t', 'isn’t', 'well', 'known', 'among', 'my', 'co', 'workers', '2', '3rds', 'of', 'middle', 'aged', 'u', 's', 'citizens', 'don’t', 'use', 'it']\n"
          ]
        }
      ],
      "source": [
        "#!pip install keras\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "tokens5 = text_to_word_sequence(my_sentence)\n",
        "print(tokens5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "RNYOaOZvITLO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokens5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVPzQT1yLhiT"
      },
      "source": [
        "## Technique 6 ##\n",
        "You're now using Gensim's tokenize function, another great tool for breaking text into words.\n",
        "It breaks the sentence into clean, lowercase words, and removes: **Punctuation**, **Numbers** (optional), **Accents** and **Symbols**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n",
            "Downloading gensim-4.4.0-cp313-cp313-win_amd64.whl (24.4 MB)\n",
            "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 1.3/24.4 MB 6.8 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 8.1/24.4 MB 20.5 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 18.1/24.4 MB 29.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.4/24.4 MB 31.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 24.4/24.4 MB 29.6 MB/s eta 0:00:00\n",
            "Installing collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install gensim --no-deps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ZBKVCvxqLmdT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ain', 't', 'isn', 't', 'well', 'known', 'among', 'my', 'co', 'workers', 'rds', 'of', 'middle', 'aged', 'U', 'S', 'citizens', 'don', 't', 'use', 'it']\n"
          ]
        }
      ],
      "source": [
        "from gensim.utils import tokenize\n",
        "\n",
        "tokens6 = list(tokenize(my_sentence))\n",
        "print(tokens6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "fHgFir6qM0ty"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokens6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "I2yktOpzPUyo"
      },
      "outputs": [],
      "source": [
        "# Question 3:\n",
        "# Write code that uses the Penn Treebank tokenizer (available in NLTK) to tokenize my_sentence.\n",
        "# You can find documentation for the treebank tokenizer here:\n",
        "# https://www.nltk.org/api/nltk.tokenize.treebank.html\n",
        "\n",
        "# Also write a comment explaining what Penn Treebank is.\n",
        "# The Treebank tokenizer uses regular expressions to tokenize text\n",
        "# Solution\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "tokens7 = TreebankWordTokenizer().tokenize(my_sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ain’t', 'isn’t', 'well-known', 'among', 'my', 'co-workers', ';', '2/3rds', 'of', 'middle-aged', 'U.S.', 'citizens', 'don’t', 'use', 'it', '.']\n"
          ]
        }
      ],
      "source": [
        "print(tokens7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokens7)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

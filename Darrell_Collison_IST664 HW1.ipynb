{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBy_JaSAGpux"
   },
   "source": [
    "##IST664 - Homework 1 \n",
    "\n",
    "Student name: Darrell Collison\n",
    "\n",
    "Originality assertion: By adding my name above, I assert that all of the text and comments in this file are my original work (except for template items written by the instructor). All of the code in this file is my work, except where I give credit to another source.\n",
    "\n",
    "Note: You may freely use code from Labs for this class without the need for attributions. \n",
    "\n",
    "Also note: DO NOT under any circumstances borrow the code of another student. You should feel free to help each other with coaching and suggestions, but do not share code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q15645S3Gqfe",
    "outputId": "485b6867-b0f0-4eef-e7c2-d645b1d2dde9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to C:\\Users\\Black\n",
      "[nltk_data]     Knight\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this code, we import the NLTK, download the Gutenberg texts, extract a \n",
    "# list of file identifiers from the downloaded material.\n",
    "\n",
    "import nltk # Bring in the NLP toolkit\n",
    "nltk.download('gutenberg') # Then import the Gutenberg library, which has books\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "95mzb154GvLC"
   },
   "outputs": [],
   "source": [
    "# Choose a file index using the list above, except for 0 and 1, where the first\n",
    "# letters of the file name are closest to the first letters of your last name.\n",
    "\n",
    "my_file_id = 14 # Change to any value between 2 and 17\n",
    "\n",
    "my_file = nltk.corpus.gutenberg.fileids()[my_file_id] # Extract the file\n",
    "\n",
    "my_text = nltk.corpus.gutenberg.raw(my_file) # Here's the raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shakespeare-caesar.txt\n",
      "112310\n"
     ]
    }
   ],
   "source": [
    "print(my_file)\n",
    "print(len(my_text)) # This tells the length of the string in characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVCzss8S8oel"
   },
   "source": [
    "### Part 1\n",
    "\n",
    "Make sure that you have (at least) one block of code for each of the tasks.\n",
    "\n",
    "1. Tokenize your book and print how many tokens it contains\n",
    "2. Remove stop words and punctuation; lowercase all remaining tokens; make sure to use this result in subsequent operations for Part 1.\n",
    "3. Recalculate and display the total number of remaining tokens (i.e., after removal of stop words and punctuation)\n",
    "4. Find all tokens with a length (in characters) greater than eight; print at least one example from this list and print the length of this list\n",
    "5. Conduct a frequency analysis of the tokens and store it in an appropriate data structure; display the type of that data structure\n",
    "6. Display the 20 most frequently occurring tokens and how often they occur\n",
    "7. Obtain a list of the unique set of tokens and print the length of this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXZdsiIXGn9B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25277\n"
     ]
    }
   ],
   "source": [
    "# Begin tasks 1 - 7 here.\n",
    "# 1\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(my_text)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "caesar_words = [w.lower() for w in tokens]\n",
    "alpha_caesar_words = [w for w in caesar_words if w.isalpha()]\n",
    "\n",
    "nltk_stops = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "caear_nostops = [w for w in alpha_caesar_words if w not in nltk_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10792\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "print(len(caear_nostops)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1189 words with more than 8 charcters in the novel e.g. \"tragedie\".\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "long_words = [word for word in caear_nostops if len(word) >= 8]\n",
    "print(f\"There are {len(long_words)} words with more than 8 charcters in the novel e.g. \\\"{long_words[0]}\\\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5\n",
    "from nltk import FreqDist\n",
    "fdist = FreqDist(caear_nostops)\n",
    "\n",
    "topkeys = fdist.most_common(20)\n",
    "type(topkeys[1])\n",
    "# This is a list of tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('caesar', 190)\n",
      "('brutus', 161)\n",
      "('bru', 153)\n",
      "('haue', 147)\n",
      "('shall', 125)\n",
      "('thou', 115)\n",
      "('cassi', 107)\n",
      "('cassius', 85)\n",
      "('antony', 75)\n",
      "('come', 74)\n",
      "('let', 71)\n",
      "('good', 70)\n",
      "('know', 68)\n",
      "('enter', 64)\n",
      "('men', 64)\n",
      "('heere', 59)\n",
      "('vs', 58)\n",
      "('thy', 56)\n",
      "('man', 55)\n",
      "('thee', 55)\n"
     ]
    }
   ],
   "source": [
    "# 6\n",
    "for pair in topkeys:\n",
    "    print (pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2847\n"
     ]
    }
   ],
   "source": [
    "# 7\n",
    "vocab = set(caear_nostops)\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSXoDJSzGspD"
   },
   "source": [
    "### Part 2\n",
    "\n",
    "A Regular Expression (RegEx), is a sequence of characters that forms a search pattern. RegEx can be used to check if a string contains the specified search pattern. You can use the re package to test whether a regular expression matches a specific string in Python. All of the following exercises should be done using regex expressions.\n",
    "\n",
    "8. Go back to the “raw” text of your book and use a sentence tokenizer to divide the text into a list of sentences. Display the total number of sentences in the list. Perform the following parsing operations on a sample of sentences using RegEx patterns. For readability, I suggest that you do each of these items in a separate code block:\n",
    "9. Divide the sentence into separate tokens based on whitespace. \n",
    "10. Discard all punctuation characters.\n",
    "11. Remove plural endings by discarding a trailing \"s\" on any token.\n",
    "12. Remove \"ed\" endings (i.e., the past tense on a verb such as \"framed\").\n",
    "\n",
    "It doesn't really matter how many sentences you process, just so long as you do enough to demonstrate that your Regex code works. Make sure that the output displayed in the notebook clearly demonstrates the success of your Regex code. Put in a concluding comment that discusses how well your Regex works, compared with a more full-featured stemmer (e.g., the Porter stemmer). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "kBUW4LNEGxC7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1592\n"
     ]
    }
   ],
   "source": [
    "# Begin tasks 8 - 12 here.\n",
    "# 8\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(my_text)\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1586\n"
     ]
    }
   ],
   "source": [
    "# References: https://stackoverflow.com/questions/25735644/python-regex-for-splitting-text-into-sentences-sentence-tokenizing, https://www.geeksforgeeks.org/dsa/write-regular-expressions/\n",
    "\n",
    "import re\n",
    "sentences2 = re.split(r'(?<=[^A-Z].[.?])', my_text)\n",
    "print(len(sentences2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What mean'st thou by that?\n",
      "\n",
      "If thou reade this, O Caesar, thou mayest liue;\n",
      "If not, the Fates with Traitors do contriue.\n",
      " I deny'd you not\n",
      "\n",
      "   Bru.\n"
     ]
    }
   ],
   "source": [
    "print(sentences2[21])\n",
    "print(sentences2[595])\n",
    "print(sentences2[1111])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'If', 'thou', 'reade', 'this,', 'O', 'Caesar,', 'thou', 'mayest', 'liue;', 'If', 'not,', 'the', 'Fates', 'with', 'Traitors', 'do', 'contriue.']\n"
     ]
    }
   ],
   "source": [
    "# 9\n",
    "tokens = re.split(r'\\s+', sentences2[595])\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What meanst thou by that\n"
     ]
    }
   ],
   "source": [
    "# 10\n",
    "clean_text = re.sub(r'[^\\w\\s]', '', sentences2[21])\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'If', 'thou', 'reade', 'this,', 'O', 'Caesar,', 'thou', 'mayest', 'liue;', 'If', 'not,', 'the', 'Fate', 'with', 'Traitor', 'do', 'contriue.']\n"
     ]
    }
   ],
   "source": [
    "# 11\n",
    "no_plural = [re.sub(r'(s)$', '', word) for word in tokens] \n",
    "print(no_plural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'I', \"deny'\", 'you', 'not', 'Bru.']\n"
     ]
    }
   ],
   "source": [
    "# 12\n",
    "\n",
    "tokens1 = re.split(r'\\s+', sentences2[1111])\n",
    "\n",
    "no_past_tense = [re.sub(r'(ed|d)$', '', word) for word in tokens1] \n",
    "print(no_past_tense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKDplCIv4IGe"
   },
   "source": [
    "Change this text to your concluding comment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

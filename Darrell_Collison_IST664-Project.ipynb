{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBy_JaSAGpux"
      },
      "source": [
        "# IST664 - Final Project- Email Spam Classification\n",
        "\n",
        "Originality assertion: All of the text and comments in this file are my original work (except for template items written by the instructor). All of the code in this file is my work, except where I give credit to another source. By adding my name below, I affirm this originality assertion.\n",
        "\n",
        "*** My name: _Darrell_Collison_ ***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBdgCgdcRNrq"
      },
      "source": [
        "**Task 1: Preprocess the data**\n",
        "\n",
        "For your choice of dataset, you will first process the text, tokenize it and choose whether to do further pre-processing or filtering. If you do some pre-processing or filtering, then using the text with and without it can be one of your experiments.\n",
        "\n",
        "For each dataset, there is a program template that reads the data. You should run the program on your data of choice and investigate some of the data to choose pre-processing or filtering. If you choose your own dataset, then you should write a similar program to read the text data and the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wtW_IYsqRMGQ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  This program shell reads email data for the spam classification problem.\n",
        "  The input to the program is the path to the Email directory \"corpus\" and a limit number.\n",
        "  The program reads the first limit number of ham emails and the first limit number of spam.\n",
        "  It creates an \"emaildocs\" variable with a list of emails consisting of a pair\n",
        "    with the list of tokenized words from the email and the label either spam or ham.\n",
        "  It prints a few example emails.\n",
        "  Your task is to generate features sets and train and test a classifier.\n",
        "\n",
        "  Usage:  python classifySPAM.py  <corpus directory path> <limit number>\n",
        "'''\n",
        "# open python and nltk packages needed for processing\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import nltk\n",
        "import collections\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to read spam and ham files, train and test a classifier \n",
        "def processspamham_data_only(dirPath, limitStr):\n",
        "    # convert the limit argument from a string to an int\n",
        "    limit = int(limitStr)\n",
        "    hamtexts = []\n",
        "    spamtexts = []\n",
        "    # os.chdir(dirPath) \n",
        "    spam_dir = os.path.join(dirPath, 'spam')\n",
        "    ham_dir = os.path.join(dirPath, 'ham')\n",
        "\n",
        "  # process all files in directory that end in .txt up to the limit\n",
        "  #    assuming that the emails are sufficiently randomized\n",
        "    for file in os.listdir(spam_dir):\n",
        "        if (file.endswith(\".txt\")) and (len(spamtexts) < limit):\n",
        "            # open file for reading and read entire file into a string\n",
        "            f = open(os.path.join(spam_dir, file), 'r', encoding=\"latin-1\")\n",
        "            spamtexts.append(f.read())\n",
        "            f.close()\n",
        "    for file in os.listdir(ham_dir):\n",
        "        if (file.endswith(\".txt\")) and (len(hamtexts) < limit):\n",
        "            # open file for reading and read entire file into a string\n",
        "            f = open(os.path.join(ham_dir, file), 'r', encoding=\"latin-1\")\n",
        "            hamtexts.append(f.read())\n",
        "            f.close()\n",
        "    # print number emails read\n",
        "    print(f\"Number of spam files read: {len(spamtexts)}\")\n",
        "    print(f\"Number of ham files read: {len(hamtexts)}\")\n",
        "    \n",
        "    emaildocs = []\n",
        "    for spam in spamtexts:\n",
        "        tokens = nltk.word_tokenize(spam)\n",
        "        emaildocs.append((tokens, 'spam'))\n",
        "    for ham in hamtexts:\n",
        "        tokens = nltk.word_tokenize(ham)\n",
        "        emaildocs.append((tokens, 'ham'))\n",
        "    \n",
        "    random.shuffle(emaildocs)\n",
        "    \n",
        "    print(f\"\\nTotal documents prepared: {len(emaildocs)}\")\n",
        "    return emaildocs\n",
        "\n",
        "data_directory = 'c:\\\\Users\\\\Black Knight\\\\Documents\\\\11.Data Science\\\\1. SYR\\\\IST 664\\\\Project' \n",
        "load_limit = '1500' # Load 500 spam and 500 ham files\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of spam files read: 1500\n",
            "Number of ham files read: 1500\n",
            "\n",
            "Total documents prepared: 3000\n"
          ]
        }
      ],
      "source": [
        "email_documents = processspamham_data_only(data_directory, load_limit)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Task 2: Produce Features**\n",
        "\n",
        "The second step is to produce the features in the notation of the NLTK. For this you should write feature functions in Python. You should start with the “bag-of-words” features where you collect all the words in the corpus and select some number of most frequent words to be the word features.\n",
        "\n",
        "Now use the NLTK Naïve Bayes classifier to train and test a classifier on your feature sets. You should use cross-validation to obtain precision, recall and F-measure scores. Or you can choose to produce the features as a csv file and use Weka or Sci-Kit Learn to train and test a classifier, using cross-validation scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Function to Score classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Imports and Helper Functions\n",
        "\n",
        "import os\n",
        "import random\n",
        "import nltk\n",
        "import collections\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import cross_validate, KFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, make_scorer\n",
        "\n",
        "# Helper function to run cross-validation with standardized metrics\n",
        "def run_cross_validation(X, y, model, n_splits=5):\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    scoring = {\n",
        "        'precision_spam': make_scorer(precision_score, pos_label='spam'),\n",
        "        'recall_spam': make_scorer(recall_score, pos_label='spam'),\n",
        "        'f1_spam': make_scorer(f1_score, pos_label='spam'),\n",
        "        'accuracy': 'accuracy'\n",
        "    }\n",
        "    cv_results = cross_validate(model, X, y, cv=kf, scoring=scoring)\n",
        "    mean_results = {metric: round(cv_results[f'test_{metric}'].mean(), 4) \n",
        "                    for metric in scoring.keys()}\n",
        "    return mean_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define a feature definition function here\n",
        "VOCAB_SIZE = 2500\n",
        "model_nb = MultinomialNB()\n",
        "results = {}\n",
        "\n",
        "def run_experiment_on_data(doc_list, name, vocab_size, model, grams):\n",
        "    \"\"\"\n",
        "    Helper function to process a document list, run vectorization, and perform cross-validation.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Convert tokenized lists back into single strings for CountVectorizer\n",
        "    texts = [\" \".join(tokens) for tokens, label in doc_list]\n",
        "    labels = [label for tokens, label in doc_list]\n",
        "\n",
        "    # Generate the feature matrix\n",
        "    vectorizer = CountVectorizer(max_features=vocab_size, ngram_range=grams)\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    \n",
        "    scores = run_cross_validation(X, labels, model, n_splits=5)\n",
        "    \n",
        "    return scores\n",
        "\n",
        "results['Baseline_Raw'] = run_experiment_on_data(\n",
        "    doc_list=email_documents, \n",
        "    name=\"Baseline (Raw text, no filtering)\", \n",
        "    vocab_size=VOCAB_SIZE, \n",
        "    model=model_nb,\n",
        "    grams= (1, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print result scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision_spam  recall_spam  f1_spam  accuracy\n",
            "Baseline_Raw          0.9526       0.9689   0.9606      0.96\n"
          ]
        }
      ],
      "source": [
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMBf3r-qTE_7"
      },
      "source": [
        "**Task 3: Experiments**\n",
        "\n",
        "For a base level completion of experiments, carry out at least several experiments where you use two different sets of features and compare the results. For example, you may take the unigram word features as a baseline and see if the features you designed improve the accuracy of the classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Experiment #1 Same classifer on filtered word set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered documents list size: 3000\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "PUNCTUATION = set(string.punctuation)\n",
        "\n",
        "def filter_tokens(tokens, remove_stopwords=False, remove_punctuation=False):\n",
        "    \"\"\"Filters a single list of tokens based on criteria.\"\"\"\n",
        "    filtered = []\n",
        "    for token in tokens:\n",
        "        token = token.lower()\n",
        "        if remove_punctuation and token in PUNCTUATION:\n",
        "            continue\n",
        "        if remove_stopwords and token in STOP_WORDS:\n",
        "            continue\n",
        "        if len(token) > 1 or token.isalpha(): \n",
        "            filtered.append(token)\n",
        "    return filtered\n",
        "\n",
        "filtered_docs = [(filter_tokens(tokens, remove_stopwords=True, remove_punctuation=True), label) \n",
        "                 for tokens, label in email_documents]\n",
        "\n",
        "random.shuffle(filtered_docs)\n",
        "\n",
        "print(f\"Filtered documents list size: {len(filtered_docs)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Comparison of Results ---\n",
            "                   precision_spam  recall_spam  f1_spam  accuracy\n",
            "Baseline_Raw               0.9526       0.9689   0.9606    0.9600\n",
            "Filtered_SW_Punct          0.9558       0.9661   0.9608    0.9607\n"
          ]
        }
      ],
      "source": [
        "results['Filtered_SW_Punct'] = run_experiment_on_data(\n",
        "    doc_list=filtered_docs, \n",
        "    name=\"Filtered (Stop words/Punctuation removed)\", \n",
        "    vocab_size=VOCAB_SIZE, \n",
        "    model=model_nb,\n",
        "    grams= (1, 1)\n",
        "\n",
        ")\n",
        "\n",
        "print(\"--- Comparison of Results ---\")\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Experiment #2 Unigram classifer on raw and filtered word set comparison, smaller vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Comparison of Results ---\n",
            "                       precision_spam  recall_spam  f1_spam  accuracy\n",
            "Baseline_Raw                   0.9526       0.9689   0.9606    0.9600\n",
            "Filtered_SW_Punct              0.9558       0.9661   0.9608    0.9607\n",
            "Raw_Unigram_1500               0.9385       0.9617   0.9498    0.9490\n",
            "Filtered_Unigram_1500          0.9486       0.9567   0.9525    0.9523\n"
          ]
        }
      ],
      "source": [
        "results['Raw_Unigram_1500'] = run_experiment_on_data(\n",
        "    doc_list=email_documents, \n",
        "    name=\"Filtered (Stop words/Punctuation removed)\", \n",
        "    vocab_size=1500, \n",
        "    model=model_nb,\n",
        "    grams= (1, 1)\n",
        "\n",
        ")\n",
        "results['Filtered_Unigram_1500'] = run_experiment_on_data(\n",
        "    doc_list=filtered_docs, \n",
        "    name=\"Filtered (Stop words/Punctuation removed)\", \n",
        "    vocab_size=1500, \n",
        "    model=model_nb,\n",
        "    grams= (1, 1)\n",
        "\n",
        ")\n",
        "print(\"--- Comparison of Results ---\")\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Experiment #3 Bigram classifer on raw and filtered word set comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Comparison of Results ---\n",
            "                       precision_spam  recall_spam  f1_spam  accuracy\n",
            "Baseline_Raw                   0.9526       0.9689   0.9606    0.9600\n",
            "Filtered_SW_Punct              0.9558       0.9661   0.9608    0.9607\n",
            "Raw_Unigram_1500               0.9385       0.9617   0.9498    0.9490\n",
            "Filtered_Unigram_1500          0.9486       0.9567   0.9525    0.9523\n",
            "Raw_Bigram                     0.9371       0.8971   0.9152    0.9173\n",
            "Filtered_Bigram                0.9598       0.7955   0.8626    0.8783\n"
          ]
        }
      ],
      "source": [
        "results['Raw_Bigram'] = run_experiment_on_data(\n",
        "    doc_list=email_documents, \n",
        "    name=\"Filtered (Stop words/Punctuation removed)\", \n",
        "    vocab_size=VOCAB_SIZE, \n",
        "    model=model_nb,\n",
        "    grams= (2, 2)\n",
        "\n",
        ")\n",
        "results['Filtered_Bigram'] = run_experiment_on_data(\n",
        "    doc_list=filtered_docs, \n",
        "    name=\"Filtered (Stop words/Punctuation removed)\", \n",
        "    vocab_size=VOCAB_SIZE, \n",
        "    model=model_nb,\n",
        "    grams= (2, 2)\n",
        "\n",
        ")\n",
        "print(\"--- Comparison of Results ---\")\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Experiment #4 Ngram classifer on raw and filtered word set comparison, large vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Comparison of Results ---\n",
            "                       precision_spam  recall_spam  f1_spam  accuracy\n",
            "Baseline_Raw                   0.9526       0.9689   0.9606    0.9600\n",
            "Filtered_SW_Punct              0.9558       0.9661   0.9608    0.9607\n",
            "Raw_Unigram_1500               0.9385       0.9617   0.9498    0.9490\n",
            "Filtered_Unigram_1500          0.9486       0.9567   0.9525    0.9523\n",
            "Raw_Bigram                     0.9371       0.8971   0.9152    0.9173\n",
            "Filtered_Bigram                0.9598       0.7955   0.8626    0.8783\n",
            "Raw_Ngram_3500                 0.9360       0.9604   0.9479    0.9470\n",
            "Filtered_Ngram_3500            0.9533       0.9661   0.9596    0.9593\n"
          ]
        }
      ],
      "source": [
        "results['Raw_Ngram_3500'] = run_experiment_on_data(\n",
        "    doc_list=email_documents, \n",
        "    name=\"Filtered (Stop words/Punctuation removed)\", \n",
        "    vocab_size=3500, \n",
        "    model=model_nb,\n",
        "    grams= (1, 3)\n",
        "\n",
        ")\n",
        "results['Filtered_Ngram_3500'] = run_experiment_on_data(\n",
        "    doc_list=filtered_docs, \n",
        "    name=\"Filtered (Stop words/Punctuation removed)\", \n",
        "    vocab_size=3500, \n",
        "    model=model_nb,\n",
        "    grams= (1, 3)\n",
        "\n",
        ")\n",
        "print(\"--- Comparison of Results ---\")\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

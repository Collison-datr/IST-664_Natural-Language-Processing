{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiy-ZEZZlLlh"
      },
      "source": [
        "**IST664 - NLP Lab Week 1**\n",
        "\n",
        "Edited by Jeff Stanton and Preeti Jagadev. Students may freely excerpt this code for use in homework and projects with appropriate attribution.\n",
        "\n",
        "During the lab, you should run the code one block at a time and make sure you know what each line of code accomplished. Add commments to explain the code to your future self.\n",
        "\n",
        "In this first section, we will use the Python “import” statement to load the NLTK package. This contains lots of text examples and unique capabilities for basic syntactic analysis of text. Whenever you see a code block containing only a comment (#) follow the instructions to add your own code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W4vS7tKyICd"
      },
      "source": [
        "## Section 1.1 ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: click in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.5.1)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2025.9.18-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
            "Requirement already satisfied: tqdm in c:\\users\\black knight\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\black knight\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
            "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ------------- -------------------------- 0.5/1.5 MB 5.0 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 1.0/1.5 MB 3.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.5/1.5 MB 3.4 MB/s eta 0:00:00\n",
            "Downloading regex-2025.9.18-cp313-cp313-win_amd64.whl (275 kB)\n",
            "Installing collected packages: regex, nltk\n",
            "\n",
            "   -------------------- ------------------- 1/2 [nltk]\n",
            "   -------------------- ------------------- 1/2 [nltk]\n",
            "   -------------------- ------------------- 1/2 [nltk]\n",
            "   -------------------- ------------------- 1/2 [nltk]\n",
            "   -------------------- ------------------- 1/2 [nltk]\n",
            "   -------------------- ------------------- 1/2 [nltk]\n",
            "   -------------------- ------------------- 1/2 [nltk]\n",
            "   -------------------- ------------------- 1/2 [nltk]\n",
            "   -------------------- ------------------- 1/2 [nltk]\n",
            "   ---------------------------------------- 2/2 [nltk]\n",
            "\n",
            "Successfully installed nltk-3.9.2 regex-2025.9.18\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVAmC7V6Xf9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to C:\\Users\\Black\n",
            "[nltk_data]     Knight\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n",
            "[nltk_data] Downloading package genesis to C:\\Users\\Black\n",
            "[nltk_data]     Knight\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\genesis.zip.\n",
            "[nltk_data] Downloading package inaugural to C:\\Users\\Black\n",
            "[nltk_data]     Knight\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\inaugural.zip.\n",
            "[nltk_data] Downloading package treebank to C:\\Users\\Black\n",
            "[nltk_data]     Knight\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\treebank.zip.\n",
            "[nltk_data] Downloading package nps_chat to C:\\Users\\Black\n",
            "[nltk_data]     Knight\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\nps_chat.zip.\n",
            "[nltk_data] Downloading package webtext to C:\\Users\\Black\n",
            "[nltk_data]     Knight\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\webtext.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ],
      "source": [
        "# IST664 - NLP Lab Week 1\n",
        "# This notebook has small examples that are meant to be run step by step.\n",
        "# nltk popular natural language processing library (toolkit)\n",
        "# download different corpus from the library inlcuding genesis which has text from 1st book of bible\n",
        "\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('genesis')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('treebank')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "from nltk.book import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qQPQXmBJX2cJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Text: Moby Dick by Herman Melville 1851>\n"
          ]
        }
      ],
      "source": [
        "print(text1) # Shows the title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DKIvq6_3m0hu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Text: Sense and Sensibility by Jane Austen 1811>\n"
          ]
        }
      ],
      "source": [
        "# Question 1: Do the same thing for text2, and also give the description for text2:\n",
        "\n",
        "#Solution:\n",
        "print(text2) # Shows the title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alREk3pcyUXh"
      },
      "source": [
        "## Section 1.2 ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "imavAwY23fWZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "nltk.text.Text"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(text1) # The type() command reveals the data type of an object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ao58rDlq42y9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['_CONTEXT_RE',\n",
              " '_COPY_TOKENS',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__firstlineno__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__static_attributes__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_context',\n",
              " '_train_default_ngram_lm',\n",
              " 'collocation_list',\n",
              " 'collocations',\n",
              " 'common_contexts',\n",
              " 'concordance',\n",
              " 'concordance_list',\n",
              " 'count',\n",
              " 'dispersion_plot',\n",
              " 'findall',\n",
              " 'generate',\n",
              " 'index',\n",
              " 'name',\n",
              " 'plot',\n",
              " 'readability',\n",
              " 'similar',\n",
              " 'tokens',\n",
              " 'vocab']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dir(text1) # These are the attributes of the object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "O0zrQaMi5eFj"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method Text.concordance of <Text: Moby Dick by Herman Melville 1851>>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Given the list above, we can find out more things about each attribute\n",
        "getattr(text1, \"concordance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6DyHGRJa0klp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on Text in module nltk.text object:\n",
            "\n",
            "class Text(builtins.object)\n",
            " |  Text(tokens, name=None)\n",
            " |\n",
            " |  A wrapper around a sequence of simple (string) tokens, which is\n",
            " |  intended to support initial exploration of texts (via the\n",
            " |  interactive console).  Its methods perform a variety of analyses\n",
            " |  on the text's contexts (e.g., counting, concordancing, collocation\n",
            " |  discovery), and display the results.  If you wish to write a\n",
            " |  program which makes use of these analyses, then you should bypass\n",
            " |  the ``Text`` class, and use the appropriate analysis function or\n",
            " |  class directly instead.\n",
            " |\n",
            " |  A ``Text`` is typically initialized from a given document or\n",
            " |  corpus.  E.g.:\n",
            " |\n",
            " |  >>> import nltk.corpus\n",
            " |  >>> from nltk.text import Text\n",
            " |  >>> moby = Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))\n",
            " |\n",
            " |  Methods defined here:\n",
            " |\n",
            " |  __getitem__(self, i)\n",
            " |\n",
            " |  __init__(self, tokens, name=None)\n",
            " |      Create a Text object.\n",
            " |\n",
            " |      :param tokens: The source text.\n",
            " |      :type tokens: sequence of str\n",
            " |\n",
            " |  __len__(self)\n",
            " |\n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |\n",
            " |  __str__(self)\n",
            " |      Return str(self).\n",
            " |\n",
            " |  collocation_list(self, num=20, window_size=2)\n",
            " |      Return collocations derived from the text, ignoring stopwords.\n",
            " |\n",
            " |          >>> from nltk.book import text4\n",
            " |          >>> text4.collocation_list()[:2]\n",
            " |          [('United', 'States'), ('fellow', 'citizens')]\n",
            " |\n",
            " |      :param num: The maximum number of collocations to return.\n",
            " |      :type num: int\n",
            " |      :param window_size: The number of tokens spanned by a collocation (default=2)\n",
            " |      :type window_size: int\n",
            " |      :rtype: list(tuple(str, str))\n",
            " |\n",
            " |  collocations(self, num=20, window_size=2)\n",
            " |      Print collocations derived from the text, ignoring stopwords.\n",
            " |\n",
            " |          >>> from nltk.book import text4\n",
            " |          >>> text4.collocations() # doctest: +NORMALIZE_WHITESPACE\n",
            " |          United States; fellow citizens; years ago; four years; Federal\n",
            " |          Government; General Government; Vice President; American people; God\n",
            " |          bless; Chief Justice; one another; fellow Americans; Old World;\n",
            " |          Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
            " |          tribes; public debt; foreign nations\n",
            " |\n",
            " |\n",
            " |      :param num: The maximum number of collocations to print.\n",
            " |      :type num: int\n",
            " |      :param window_size: The number of tokens spanned by a collocation (default=2)\n",
            " |      :type window_size: int\n",
            " |\n",
            " |  common_contexts(self, words, num=20)\n",
            " |      Find contexts where the specified words appear; list\n",
            " |      most frequent common contexts first.\n",
            " |\n",
            " |      :param words: The words used to seed the similarity search\n",
            " |      :type words: str\n",
            " |      :param num: The number of words to generate (default=20)\n",
            " |      :type num: int\n",
            " |      :seealso: ContextIndex.common_contexts()\n",
            " |\n",
            " |  concordance(self, word, width=79, lines=25)\n",
            " |      Prints a concordance for ``word`` with the specified context window.\n",
            " |      Word matching is not case-sensitive.\n",
            " |\n",
            " |      :param word: The target word or phrase (a list of strings)\n",
            " |      :type word: str or list\n",
            " |      :param width: The width of each line, in characters (default=80)\n",
            " |      :type width: int\n",
            " |      :param lines: The number of lines to display (default=25)\n",
            " |      :type lines: int\n",
            " |\n",
            " |      :seealso: ``ConcordanceIndex``\n",
            " |\n",
            " |  concordance_list(self, word, width=79, lines=25)\n",
            " |      Generate a concordance for ``word`` with the specified context window.\n",
            " |      Word matching is not case-sensitive.\n",
            " |\n",
            " |      :param word: The target word or phrase (a list of strings)\n",
            " |      :type word: str or list\n",
            " |      :param width: The width of each line, in characters (default=80)\n",
            " |      :type width: int\n",
            " |      :param lines: The number of lines to display (default=25)\n",
            " |      :type lines: int\n",
            " |\n",
            " |      :seealso: ``ConcordanceIndex``\n",
            " |\n",
            " |  count(self, word)\n",
            " |      Count the number of times this word appears in the text.\n",
            " |\n",
            " |  dispersion_plot(self, words)\n",
            " |      Produce a plot showing the distribution of the words through the text.\n",
            " |      Requires pylab to be installed.\n",
            " |\n",
            " |      :param words: The words to be plotted\n",
            " |      :type words: list(str)\n",
            " |      :seealso: nltk.draw.dispersion_plot()\n",
            " |\n",
            " |  findall(self, regexp)\n",
            " |      Find instances of the regular expression in the text.\n",
            " |      The text is a list of tokens, and a regexp pattern to match\n",
            " |      a single token must be surrounded by angle brackets.  E.g.\n",
            " |\n",
            " |      >>> from nltk.book import text1, text5, text9\n",
            " |      >>> text5.findall(\"<.*><.*><bro>\")\n",
            " |      you rule bro; telling you bro; u twizted bro\n",
            " |      >>> text1.findall(\"<a>(<.*>)<man>\")\n",
            " |      monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
            " |      mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
            " |      pale; furious; better; certain; complete; dismasted; younger; brave;\n",
            " |      brave; brave; brave\n",
            " |      >>> text9.findall(\"<th.*>{3,}\")\n",
            " |      thread through those; the thought that; that the thing; the thing\n",
            " |      that; that that thing; through these than through; them that the;\n",
            " |      through the thick; them that they; thought that the\n",
            " |\n",
            " |      :param regexp: A regular expression\n",
            " |      :type regexp: str\n",
            " |\n",
            " |  generate(self, length=100, text_seed=None, random_seed=42)\n",
            " |      Print random text, generated using a trigram language model.\n",
            " |      See also `help(nltk.lm)`.\n",
            " |\n",
            " |      :param length: The length of text to generate (default=100)\n",
            " |      :type length: int\n",
            " |\n",
            " |      :param text_seed: Generation can be conditioned on preceding context.\n",
            " |      :type text_seed: list(str)\n",
            " |\n",
            " |      :param random_seed: A random seed or an instance of `random.Random`. If provided,\n",
            " |          makes the random sampling part of generation reproducible. (default=42)\n",
            " |      :type random_seed: int\n",
            " |\n",
            " |  index(self, word)\n",
            " |      Find the index of the first occurrence of the word in the text.\n",
            " |\n",
            " |  plot(self, *args)\n",
            " |      See documentation for FreqDist.plot()\n",
            " |      :seealso: nltk.prob.FreqDist.plot()\n",
            " |\n",
            " |  readability(self, method)\n",
            " |\n",
            " |  similar(self, word, num=20)\n",
            " |      Distributional similarity: find other words which appear in the\n",
            " |      same contexts as the specified word; list most similar words first.\n",
            " |\n",
            " |      :param word: The word used to seed the similarity search\n",
            " |      :type word: str\n",
            " |      :param num: The number of words to generate (default=20)\n",
            " |      :type num: int\n",
            " |      :seealso: ContextIndex.similar_words()\n",
            " |\n",
            " |  vocab(self)\n",
            " |      :seealso: nltk.prob.FreqDist\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |\n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |\n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# You can also find lots of details about an object by calling help()\n",
        "help(text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH61PduHxoA9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(sent1) # What type is sent1\n",
        "# sent1 is a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ma-A4S_exrbY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['__add__',\n",
              " '__class__',\n",
              " '__class_getitem__',\n",
              " '__contains__',\n",
              " '__delattr__',\n",
              " '__delitem__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__iadd__',\n",
              " '__imul__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__iter__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__mul__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__reversed__',\n",
              " '__rmul__',\n",
              " '__setattr__',\n",
              " '__setitem__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " 'append',\n",
              " 'clear',\n",
              " 'copy',\n",
              " 'count',\n",
              " 'extend',\n",
              " 'index',\n",
              " 'insert',\n",
              " 'pop',\n",
              " 'remove',\n",
              " 'reverse',\n",
              " 'sort']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 2: Add a line of code that displays the attributes of sent1\n",
        "\n",
        "#Solution\n",
        "dir(sent1) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KBaPQ0myfZc"
      },
      "source": [
        "## Section 1.3 ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QRJ1VIUOX5T2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Call', 'me', 'Ishmael', '.']\n"
          ]
        }
      ],
      "source": [
        "print(sent1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "a0I4yxQM69a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['.', 'Call', 'Ishmael', 'me']\n"
          ]
        }
      ],
      "source": [
        "sent1.sort() # This reorders the list alphabetically\n",
        "print(sent1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WcJW-2DF8Bz0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Call', 'Ishmael', 'me']\n"
          ]
        }
      ],
      "source": [
        "sent1.remove('.') # We can delete items by name\n",
        "print(sent1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YWow54JR8VqF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Call', 'Ishmael', 'me', '.']\n"
          ]
        }
      ],
      "source": [
        "sent1.append('.') # And we can add an item onto the end of the list\n",
        "print(sent1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7tNkiaFYYOXR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Call Ishmael me .\n"
          ]
        }
      ],
      "source": [
        "# The join method can be applied to a list\n",
        "sent1text = ' '.join(sent1) # You can glue elements of the list together\n",
        "print(sent1text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zrVJGccsvIEW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The family of Dashwood had long been settled in Sussex .\n"
          ]
        }
      ],
      "source": [
        "# # Question 3: Similar to what we did above, join sent2 together with spaces in between and then print it (two lines of code).\n",
        "\n",
        "# Solution\n",
        "sent2text = ' '.join(sent2) \n",
        "print(sent2text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj5hUFAkmHYw"
      },
      "source": [
        "##Section 1.4\n",
        "\n",
        "**Searching**\n",
        "\n",
        "The text data structure from nltk has a number of bound methods.\n",
        "\n",
        "A flexible way of searching uses the findall() method. Another function is “similar” which finds all the words that are used in the same context as the one given, where the context is the word before and the word after.\n",
        "\n",
        "We can also examine the contexts that are shared by two or more words, such as “monstrous” and “very” by using common_contexts. We have to enclose these words by square brackets as well as parentheses, and separate them with a comma. The output of common_contexts is a pair of words that surrounds both of the target words provided. For example if the target is very and the function returns a_pretty, the fragment in question would be \"a very pretty\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "WaAdHmOVFWQ4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "morning; morning; morning; morning; morning; morning; morning;\n",
            "morning; morning; morning; morning; morning; morning; morning;\n",
            "morning; morning; morning; morning; morning; morning; morning;\n",
            "morning; morning; morning; morning; morning; morning; morning;\n",
            "morning; morning; morning; morning; morning; morning; morning;\n",
            "morning; morning; morning; morning; morning; morning; morning;\n",
            "morning; morning; morning; morning; morning; morning; morning;\n",
            "morning; morning; morning; morning; morning; morning; morning;\n",
            "morning; morning; morning; morning; morning; morning; morning;\n",
            "morning; morning; morning; morning; morning\n"
          ]
        }
      ],
      "source": [
        "text1.findall('<morning>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-7gbz4FDKOyM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Morning to ye; Morning to ye; Morning to ye; Morning to ye; Morning to\n",
            "ye\n"
          ]
        }
      ],
      "source": [
        "text1.findall('<Morning><to><ye>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "nasu93P_KtZx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Morning to ye , shipmates; Morning to ye , shipmates; Morning to ye ,\n",
            "shipmates; Morning to ye .\" Once; Morning to ye ! morning\n"
          ]
        }
      ],
      "source": [
        "# We can also include two wildcard tokens to find out what comes next\n",
        "text1.findall('<Morning><to><ye><.*><.*>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "29_anrs4RJlJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sea time whale ship man water voyage same moment world while hand word\n",
            "case lord day boat land days other\n"
          ]
        }
      ],
      "source": [
        "text1.similar('morning') # These words are used in the same context as morning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "PMI__hpCRJlK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "day house time room world place matter cottage evening point country\n",
            "door and rest thing moment latter subject better letter\n"
          ]
        }
      ],
      "source": [
        "text2.similar('morning') # And here's the same thing for the second book"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "EJXhAOLHRJlK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the_when the_and that_at\n"
          ]
        }
      ],
      "source": [
        "# Show the common contexts for two different words\n",
        "text1.common_contexts([\"morning\", \"evening\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vsmysnKoifn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the_was this_i the_of the_marianne the_for the_before\n"
          ]
        }
      ],
      "source": [
        "# Question 4:\n",
        "# Now you try it by repeating the common_contexts command, but using the data object text2. Use the terms morning and evening.\n",
        "# Also include your explanation of the output.\n",
        "# common_context gives the words that appears before and after both of the words given\n",
        "#Solution\n",
        "text2.common_contexts([\"morning\", \"evening\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7DHn6RGn6QM"
      },
      "source": [
        "## Section 1.5 ##\n",
        "\n",
        "**Basic Corpus Linguistics: Word Counting**\n",
        "\n",
        "A natural question to ask in the analysis of a text is: how many words are there in the text? While it is straightforward for human beings to answer the question, it is not so easy for computers. As a starting point, we can use Python function len to first count the total number of words and punctuation symbols that appear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GpNt_O7WRJlK"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "260819"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Counting the total number of tokens\n",
        "len(text1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4znga5B2oHJG"
      },
      "source": [
        "How to interpret this number? This number shows how many words and punctuation symbols, or \"tokens\" there are in the complete list.\n",
        "\n",
        "A token is the technical name for a sequence of characters which is treated as a group by the software. Each text from the books was separated into a list of tokens, and this is one of the first NLP processing steps.\n",
        "\n",
        "Now this is the total number of tokens, and we might also want to find out how many distinct words there are, not counting repetitions. The Python “set” function removes the repetitions, and we can apply the “sorted” function to that, returning the resulted sorted list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "RnBCvkekRJlL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['!', '!\"', '!\"--', \"!'\", '!\\'\"', '!)', '!)\"', '!*', '!--', '!--\"']"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sorted(set(text1))[:10] # Review the first 10 of an alphabetized set of tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "UddrbN6HMsr5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Alone',\n",
              " 'Alps',\n",
              " 'Already',\n",
              " 'Also',\n",
              " 'Am',\n",
              " 'Ambergriese',\n",
              " 'Ambergris',\n",
              " 'Amelia',\n",
              " 'America',\n",
              " 'American']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's look at a few more, after we've gotten through the punctuation, numbers\n",
        "# and other stuff that appears early in the sort.\n",
        "sorted(set(text1))[400:410]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "2x5lhq3lRJlL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19317"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The set() function creates an unordered collection\n",
        "# of elements where each element is unique. We can take advantage of this\n",
        "# to count the number of unique tokens in the book.\n",
        "len(sorted(set(text1))) # Total number of tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "NN8U9PBNRJlM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.052607363727335814"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Among tokens that are actual words, \"the\" is often one of the most common in\n",
        "# modern English.\n",
        "text1.count('the')/len(text1) # As a proportion of all tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "895"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text2.count('he')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "8NK-bNsaRJlN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.023096476867099407"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text1.count('and')/len(text1) # \"And\" is also a very common word in English"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84TjUkB81z4i"
      },
      "source": [
        "This next code block contains our first use of a list comprehension in Python. This is like a for loop in any other programming language with the difference that it yields a list object that can be used for other purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "i5uiKi17BZ46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
          ]
        }
      ],
      "source": [
        "my_numbers = [number for number in range(10,20)]\n",
        "print(my_numbers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "cUWH3TrB2YdJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(my_numbers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "AWLssPwCRJlN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "106"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We apply a test to each token to see how many characters it contains. We only\n",
        "# include it in the list if it has 15 or more characters.\n",
        "long_words = [w for w in text1 if len(w) >= 15]\n",
        "len(long_words) # Count how many tokens are 15 letters or more"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "t07jd3sqRJlN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['CIRCUMNAVIGATION',\n",
              " 'perpendicularly',\n",
              " 'unceremoniously',\n",
              " 'uncomfortableness',\n",
              " 'sympathetically',\n",
              " 'Ehrenbreitstein',\n",
              " 'multitudinously',\n",
              " 'phrenologically',\n",
              " 'cannibalistically',\n",
              " 'comfortableness',\n",
              " 'intolerableness',\n",
              " 'perpendicularly',\n",
              " 'unconsciousness',\n",
              " 'circumnavigations',\n",
              " 'untraditionally',\n",
              " 'heterogeneously',\n",
              " 'superfluousness',\n",
              " 'superstitiousness',\n",
              " 'apprehensiveness',\n",
              " 'superstitiously',\n",
              " 'unsurrenderable',\n",
              " 'systematization',\n",
              " 'authoritatively',\n",
              " 'indiscriminately',\n",
              " 'multitudinously',\n",
              " 'notwithstanding',\n",
              " 'indiscriminately',\n",
              " 'Bibliographical',\n",
              " 'Mephistophelean',\n",
              " 'indomitableness',\n",
              " 'individualizing',\n",
              " 'superstitiousness',\n",
              " 'superstitiously',\n",
              " 'supernaturalism',\n",
              " 'unsophisticated',\n",
              " 'perpendicularly',\n",
              " 'comprehensiveness',\n",
              " 'circumnavigating',\n",
              " 'circumnavigated',\n",
              " 'philosophically',\n",
              " 'notwithstanding',\n",
              " 'preternaturalness',\n",
              " 'circumnavigation',\n",
              " 'apprehensiveness',\n",
              " 'unconditionally',\n",
              " 'archiepiscopacy',\n",
              " 'characteristics',\n",
              " 'picturesqueness',\n",
              " 'picturesqueness',\n",
              " 'skrimshandering',\n",
              " 'characteristics',\n",
              " 'amphitheatrical',\n",
              " 'indiscriminately',\n",
              " 'notwithstanding',\n",
              " 'simultaneousness',\n",
              " 'perpendicularly',\n",
              " 'indispensableness',\n",
              " 'perpendicularly',\n",
              " 'dissatisfaction',\n",
              " 'notwithstanding',\n",
              " 'disembowelments',\n",
              " 'perpendicularly',\n",
              " 'conscientiously',\n",
              " 'apprehensiveness',\n",
              " 'miscellaneously',\n",
              " 'undiscriminating',\n",
              " 'thoughtlessness',\n",
              " 'irresistibleness',\n",
              " 'characteristics',\n",
              " 'Physiognomically',\n",
              " 'physiognomically',\n",
              " 'physiognomically',\n",
              " 'phrenologically',\n",
              " 'phrenologically',\n",
              " 'indomitableness',\n",
              " 'inoffensiveness',\n",
              " 'unprecedentedly',\n",
              " 'circumnavigation',\n",
              " 'hermaphroditical',\n",
              " 'circumnavigating',\n",
              " 'philanthropists',\n",
              " 'unsophisticated',\n",
              " 'characteristically',\n",
              " 'comprehensiveness',\n",
              " 'notwithstanding',\n",
              " 'internationally',\n",
              " 'notwithstanding',\n",
              " 'interrogatively',\n",
              " 'perpendicularly',\n",
              " 'instantaneously',\n",
              " 'comprehensiveness',\n",
              " 'uncompromisedness',\n",
              " 'uninterpenetratingly',\n",
              " 'uncatastrophied',\n",
              " 'responsibilities',\n",
              " 'passionlessness',\n",
              " 'phosphorescence',\n",
              " 'supernaturalness',\n",
              " 'subterraneousness',\n",
              " 'apprehensiveness',\n",
              " 'instantaneously',\n",
              " 'interchangeably',\n",
              " 'comprehensively',\n",
              " 'individualities',\n",
              " 'simultaneousness',\n",
              " 'perpendicularly']"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's see them:\n",
        "long_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "collapsed": true,
        "id": "ozgMccJtRJlN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.04826383002768831"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# For text2, calculate the following quantities by copying lines of code from the blocks up above:\n",
        "\n",
        "# Question 5: The size of the token set versus total tokens (expressed as a proportion)\n",
        "len(set(text2)) / len(text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "XkiHFwGbYqbI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.027271571452788607"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 6: The occurrences of the word 'the' as a percentage of all tokens\n",
        "text2.count('the')/len(text2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "kndWzndmYtnU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0006145109340566198"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 7: The occurrences of the word 'morning' as a percentage of all tokens\n",
        "text2.count('morning')/len(text2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "hqU3BJ9ZY0Dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "22605"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 8: How many tokens are 7 letters or more\n",
        "sev_lett_words = [w for w in text2 if len(w) >= 7]\n",
        "len(sev_lett_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJChpd3ey8Q9"
      },
      "source": [
        "## Section 1.6 ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "aR23u1bNO3sY"
      },
      "outputs": [],
      "source": [
        "# Here we are creating a custom function to integrate what we learned.\n",
        "def day_word_analysis(input_text):\n",
        "\n",
        "  m_count = input_text.count('morning')\n",
        "  a_count = input_text.count('afternoon')\n",
        "\n",
        "  print('Percentage of time of day words:',\n",
        "        100*(m_count + a_count)/len(input_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "MV_aDcy5QTlE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage of time of day words: 0.029905796740268154\n"
          ]
        }
      ],
      "source": [
        "day_word_analysis(text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "rCo_613sQpFq"
      },
      "outputs": [],
      "source": [
        "# Question 9:\n",
        "# Now add 'evening' to day_word_analysis() and retest it.\n",
        "# Test with text1 and text2.\n",
        "# What do the results show?\n",
        "\n",
        "# Solution\n",
        "def day_word_analysis1(input_text):\n",
        "\n",
        "  m_count = input_text.count('morning')\n",
        "  a_count = input_text.count('afternoon')\n",
        "  c_count = input_text.count('evening')\n",
        "\n",
        "\n",
        "  print('Percentage of time of day words:',\n",
        "        100*(m_count + a_count + c_count)/len(input_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage of time of day words: 0.036807134449560804\n",
            "Percentage of time of day words: 0.0995931513815901\n"
          ]
        }
      ],
      "source": [
        "day_word_analysis1(text1)\n",
        "day_word_analysis1(text2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8Dj5UYvF9YC"
      },
      "source": [
        "## Section 1.7 ##\n",
        "Here's a piece of code from Natural Language Processing in Action that demonstrates the use of the Counter() function from the collections package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "jnJp80P2GMQ0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({',': 7,\n",
              "         'of': 3,\n",
              "         'a': 2,\n",
              "         'and': 2,\n",
              "         'valuable': 1,\n",
              "         'or': 1,\n",
              "         'entertaining': 1,\n",
              "         'as': 1,\n",
              "         'affording': 1,\n",
              "         'glancing': 1,\n",
              "         'bird': 1,\n",
              "         \"'\": 1,\n",
              "         's': 1,\n",
              "         'eye': 1,\n",
              "         'view': 1,\n",
              "         'what': 1,\n",
              "         'has': 1,\n",
              "         'been': 1,\n",
              "         'promiscuously': 1,\n",
              "         'said': 1,\n",
              "         'thought': 1,\n",
              "         'fancied': 1,\n",
              "         'sung': 1,\n",
              "         'Leviathan': 1,\n",
              "         'by': 1,\n",
              "         'many': 1,\n",
              "         'nations': 1,\n",
              "         'generations': 1,\n",
              "         'including': 1,\n",
              "         'our': 1,\n",
              "         'own': 1,\n",
              "         '.': 1,\n",
              "         'So': 1,\n",
              "         'fare': 1,\n",
              "         'thee': 1,\n",
              "         'well': 1,\n",
              "         'poor': 1,\n",
              "         'devil': 1,\n",
              "         'Sub': 1,\n",
              "         '-': 1})"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "moby_excerpt = text1[400:450] # Just analyze 50 of the words\n",
        "\n",
        "Counter(moby_excerpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "ppp_excerpt = text2[160:210] # Just analyze 50 of the words\n",
        "counts1 = Counter(ppp_excerpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({'.': 5, 'to': 4, 'and': 3, 'the': 2, 'of': 2, ',': 2, 'whom': 1, 'he': 1, 'intended': 1, 'bequeath': 1, 'it': 1, 'In': 1, 'society': 1, 'his': 1, 'nephew': 1, 'niece': 1, 'their': 1, 'children': 1, 'old': 1, 'Gentleman': 1, \"'\": 1, 's': 1, 'days': 1, 'were': 1, 'comfortably': 1, 'spent': 1, 'His': 1, 'attachment': 1, 'them': 1, 'all': 1, 'increased': 1, 'The': 1, 'constant': 1, 'attention': 1, 'Mr': 1, 'Mrs': 1, 'Henry': 1, 'Dashwood': 1})\n"
          ]
        }
      ],
      "source": [
        "print(counts1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({'his': 5, ',': 4, 'and': 2, 'in': 2, '.': 2, 'her': 2, 'the': 2, 'housekeeper': 1, 'sister': 1, 'But': 1, 'death': 1, 'which': 1, 'happened': 1, 'ten': 1, 'years': 1, 'before': 1, 'own': 1, 'produced': 1, 'a': 1, 'great': 1, 'alteration': 1, 'home': 1, ';': 1, 'for': 1, 'to': 1, 'supply': 1, 'loss': 1, 'he': 1, 'invited': 1, 'received': 1, 'into': 1, 'house': 1, 'family': 1, 'of': 1, 'nephew': 1, 'Mr': 1, 'Henry': 1, 'Dashwood': 1})\n"
          ]
        }
      ],
      "source": [
        "counts2 = Counter(pp_excerpt)\n",
        "print(counts2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({'.': 2, 'the': 2, 'and': 2, ',': 2, 'to': 1, 'he': 1, 'of': 1, 'his': 1, 'nephew': 1, 'Mr': 1, 'Henry': 1, 'Dashwood': 1})\n"
          ]
        }
      ],
      "source": [
        "common_tokens_counts = counts1 & counts2\n",
        "print(common_tokens_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "kAG5MUTdGtdm"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'his': 5,\n",
              "         ',': 4,\n",
              "         'and': 2,\n",
              "         'in': 2,\n",
              "         '.': 2,\n",
              "         'her': 2,\n",
              "         'the': 2,\n",
              "         'housekeeper': 1,\n",
              "         'sister': 1,\n",
              "         'But': 1,\n",
              "         'death': 1,\n",
              "         'which': 1,\n",
              "         'happened': 1,\n",
              "         'ten': 1,\n",
              "         'years': 1,\n",
              "         'before': 1,\n",
              "         'own': 1,\n",
              "         'produced': 1,\n",
              "         'a': 1,\n",
              "         'great': 1,\n",
              "         'alteration': 1,\n",
              "         'home': 1,\n",
              "         ';': 1,\n",
              "         'for': 1,\n",
              "         'to': 1,\n",
              "         'supply': 1,\n",
              "         'loss': 1,\n",
              "         'he': 1,\n",
              "         'invited': 1,\n",
              "         'received': 1,\n",
              "         'into': 1,\n",
              "         'house': 1,\n",
              "         'family': 1,\n",
              "         'of': 1,\n",
              "         'nephew': 1,\n",
              "         'Mr': 1,\n",
              "         'Henry': 1,\n",
              "         'Dashwood': 1})"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 10:\n",
        "# Run Counter() on 50 tokens from text2.\n",
        "# What's the most common token as revealed by Counter()?\n",
        "\n",
        "# Solution\n",
        "pp_excerpt = text2[100:150] # Just analyze 50 of the words\n",
        "\n",
        "Counter(pp_excerpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alg5dggPHJnq"
      },
      "source": [
        "## Section 1.8 ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LHpv65l3ZFn"
      },
      "outputs": [],
      "source": [
        "# Question 11\n",
        "\n",
        "# Write and test a function that receives two token lists.\n",
        "# The function should use Counter() to enumerate the unique tokens in each token list (between the specified start and end).\n",
        "# Then the function should print the token counts for each text, but ONLY for tokens that exist in both input texts.\n",
        "\n",
        "# Solution\n",
        "\n",
        "def word_count_analysis(list1, list2):\n",
        "\n",
        "        excerpt_un = Counter(list1[100:150]) \n",
        "        excerpt_deux = Counter(list2[100:150]) \n",
        "\n",
        "        common_tokens_counts = excerpt_un & excerpt_deux\n",
        "\n",
        "        print('The tokens that exist in both input texts are: ',\n",
        "                common_tokens_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tokens that exist in both input texts are:  Counter({',': 4, 'the': 2, 'and': 1, 'to': 1, 'a': 1, 'in': 1, 'which': 1, 'of': 1})\n"
          ]
        }
      ],
      "source": [
        "word_count_analysis(text1, text2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9RszQWEEK9s"
      },
      "source": [
        "Download a copy of this notebook with all of your exercises completed and the outputs of each code block retained. Then upload that file (which will have the .ipynb extension) to the appropriate drop box on Blackboard.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

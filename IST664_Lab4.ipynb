{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34nwFy4M5w-0"
      },
      "source": [
        "### **IST664 Lab 4: Extracting Semantics**\n",
        "\n",
        "Credit: Dr. Preeti Jagadev and Dr. Jeff Stanton\n",
        "\n",
        "## Section 4.1\n",
        "\n",
        "If the end goal of a project is natural language understanding, at some point we must figure out how to associate words, phrases, sentences, and larger structures with the meaning that they convey. Figuring out the meaning of utterances is the goal of semantics. In fact the leading definition of the word semantics is, \"the study of the meaning of words\" In this lab, we are going to conisder three different approaches to extracting semantics from text.\n",
        "\n",
        "One of the earliest and most comprehensive efforts to explore semantics on a large scale arose from the work of George Miller at Princeton in the mid-1980s. The database arising from Miller's work, known as WordNet, was an award-winning effort to create a network of interconnected meanings of words. The WordNet project is alive and well in the present day, in fact there is an international organization  known as the Global WordNet Association that continues research and development of WordNet. Check it out here:\n",
        "\n",
        "http://globalwordnet.org\n",
        "\n",
        "GWA has an annual conference and offers some databases and documentation to the world community for free. These databases, now covering more than 200 languages, represent a massive amount of collective human effort, which is both amazing and illustrative of the core challenge with such resources: The maintenance of manually developed language resources requires lots of manual labor.\n",
        "\n",
        "Possibly, some of the value of what WordNet provides has been or will eventually be superceded by approaches based on deep learning. We see inklings of this with GloVe word embedding and more sophisticated embedding approaches such as BERT that are initially trained (in an unsupervised mode) on masses of unlabeled natural language text. Even so, WordNet works (and works fast!) without needing to provide any training data, so there are many applications where it is still an appropriate choice. In this first part of the week 4 lab, we explore some of the WordNet capabilities afforded by NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fUKzxUVv31MY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to C:\\Users\\Black\n",
            "[nltk_data]     Knight\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Black\n",
            "[nltk_data]     Knight\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "method"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet') # Colab does not have it installed by default\n",
        "nltk.download('omw-1.4') # Colab does not have it installed by default\n",
        "from nltk.corpus import wordnet as wn\n",
        "type(wn.synsets) # A key function call (method) that we will use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cQXUKYTC4G1R"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(list, 10)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's start by getting data on the word cat.\n",
        "# A \"synset\" is a very basic data structure supported by NLTK that can be used to look up synonyms\n",
        "# and related information for any word that the WordNet folks have included in the giant database.\n",
        "syn = wn.synsets('cat')\n",
        "type(syn), len(syn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z36vNxJP4hXT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[nltk.corpus.reader.wordnet.Synset,\n",
              " nltk.corpus.reader.wordnet.Synset,\n",
              " nltk.corpus.reader.wordnet.Synset,\n",
              " nltk.corpus.reader.wordnet.Synset,\n",
              " nltk.corpus.reader.wordnet.Synset,\n",
              " nltk.corpus.reader.wordnet.Synset,\n",
              " nltk.corpus.reader.wordnet.Synset,\n",
              " nltk.corpus.reader.wordnet.Synset,\n",
              " nltk.corpus.reader.wordnet.Synset,\n",
              " nltk.corpus.reader.wordnet.Synset]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The output above shows that the return data structure has 10 elements in a list.\n",
        "# What are these different list elements?\n",
        "[type(s) for s in syn]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PeJqwuhC3xGB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synset name :   cat.n.01\n",
            "\n",
            "Synset meaning :  feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats\n",
            "\n",
            "Synset example :  []\n"
          ]
        }
      ],
      "source": [
        "# Each element in the list is a synset object.\n",
        "cat0 = syn[0] # Let's look at some of the details for the first synset\n",
        "print (\"Synset name :  \", cat0.name())\n",
        "\n",
        "# Defining the word\n",
        "print (\"\\nSynset meaning : \", cat0.definition())\n",
        "\n",
        "# list of phrases that use the word in context; not all words have these\n",
        "print (\"\\nSynset example : \", cat0.examples())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "l7s6PD_K86Q9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synset name :   cat.n.01\n",
            "\n",
            "Synset root hypernym:   [Synset('entity.n.01')]\n",
            "\n",
            "Synset hypernyms:   [Synset('feline.n.01')]\n",
            "\n",
            "Synset hyponyms:   [Synset('domestic_cat.n.01'), Synset('wildcat.n.03')]\n"
          ]
        }
      ],
      "source": [
        "print (\"Synset name :  \", cat0.name()) # Let's show the name again\n",
        "\n",
        "# Here is the \"root\" word, the highest level hypernym\n",
        "print (\"\\nSynset root hypernym:  \", cat0.root_hypernyms())\n",
        "\n",
        "# These are the more general terms\n",
        "print (\"\\nSynset hypernyms:  \", cat0.hypernyms())\n",
        "\n",
        "# These are the more specific terms\n",
        "print (\"\\nSynset hyponyms:  \", cat0.hyponyms())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyWNgAoDmgDw"
      },
      "source": [
        "In the first line of code below, we extract the second element of the synset list. Use it to show the name, definition, example, root hypernym, hypernyms, and hyponyms for this first synonym of cat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "48e3WIGn97lq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synset name :   guy.n.01\n",
            "\n",
            "Synset Part of Speech  :  n\n",
            "\n",
            "Synset meaning :  an informal term for a youth or man\n",
            "\n",
            "Synset example :  ['a nice guy', \"the guy's only doing it for some doll\"]\n",
            "\n",
            "Synset root hypernym:   [Synset('entity.n.01')]\n",
            "\n",
            "Synset hypernyms:   [Synset('man.n.01')]\n",
            "\n",
            "Synset hyponyms:   [Synset('sod.n.04')]\n"
          ]
        }
      ],
      "source": [
        "# Exercises: Explore the second synset for \"cat.\"\n",
        "# Create a new block of code for each of the following exercises.\n",
        "\n",
        "cat1 = syn[1] # Let's look at some of the details for the second element\n",
        "\n",
        "# Question 4.1: Print the name of cat1: What part of speech is it?\n",
        "print (\"Synset name :  \", cat1.name())\n",
        "print (\"\\nSynset Part of Speech  : \", cat1.pos())\n",
        "\n",
        "# Question 4.2: Print the definition of cat1\n",
        "print (\"\\nSynset meaning : \", cat1.definition())\n",
        "\n",
        "# Question 4.3: Print the examples of use of cat1 in context\n",
        "print (\"\\nSynset example : \", cat1.examples())\n",
        "\n",
        "# Question 4.4: Print the root hypernym of cat1\n",
        "print (\"\\nSynset root hypernym:  \", cat1.root_hypernyms())\n",
        "\n",
        "# Question 4.5: Print a list of hypernyms of cat1\n",
        "print (\"\\nSynset hypernyms:  \", cat1.hypernyms())\n",
        "\n",
        "# Question 4.6: Print a list of hyponyms of cat1\n",
        "print (\"\\nSynset hyponyms:  \", cat1.hyponyms())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9QhTvTbeK14"
      },
      "source": [
        "## Section 4.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NwNfii4hAD-Q"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats',\n",
              " 'an informal term for a youth or man',\n",
              " 'a spiteful woman gossip',\n",
              " 'the leaves of the shrub Catha edulis which are chewed like tobacco or used to make tea; has the effect of a euphoric stimulant',\n",
              " 'a whip with nine knotted cords',\n",
              " 'a large tracked vehicle that is propelled by two endless metal belts; frequently used for moving earth in construction and farm work',\n",
              " 'any of several large cats typically able to roar and living in the wild',\n",
              " 'a method of examining body organs by scanning them with X rays and using a computer to construct a series of cross-sectional scans along a single axis',\n",
              " \"beat with a cat-o'-nine-tails\",\n",
              " 'eject the contents of the stomach through the mouth']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cat is such a common word in English that it has been reused to refer to many different kinds of things.\n",
        "# Let's go back to the complete list to show all of the definitions:\n",
        "[s.definition() for s in syn]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Jlp3992_BA4g"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('cat.n.01',\n",
              "  'feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats'),\n",
              " ('guy.n.01', 'an informal term for a youth or man'),\n",
              " ('cat.n.03', 'a spiteful woman gossip'),\n",
              " ('kat.n.01',\n",
              "  'the leaves of the shrub Catha edulis which are chewed like tobacco or used to make tea; has the effect of a euphoric stimulant'),\n",
              " (\"cat-o'-nine-tails.n.01\", 'a whip with nine knotted cords'),\n",
              " ('caterpillar.n.02',\n",
              "  'a large tracked vehicle that is propelled by two endless metal belts; frequently used for moving earth in construction and farm work'),\n",
              " ('big_cat.n.01',\n",
              "  'any of several large cats typically able to roar and living in the wild'),\n",
              " ('computerized_tomography.n.01',\n",
              "  'a method of examining body organs by scanning them with X rays and using a computer to construct a series of cross-sectional scans along a single axis'),\n",
              " ('cat.v.01', \"beat with a cat-o'-nine-tails\"),\n",
              " ('vomit.v.01', 'eject the contents of the stomach through the mouth')]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# That's an amazing variety.\n",
        "# Let's also glue the corresponding synset name to the definition so that we can see the parts of speech and numbering.\n",
        "[ (s.name(), s.definition())  for s in syn]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MNFyf-zrC3WN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('cat.n.01', ['cat', 'true_cat']),\n",
              " ('guy.n.01', ['guy', 'cat', 'hombre', 'bozo']),\n",
              " ('cat.n.03', ['cat']),\n",
              " ('kat.n.01',\n",
              "  ['kat', 'khat', 'qat', 'quat', 'cat', 'Arabian_tea', 'African_tea']),\n",
              " (\"cat-o'-nine-tails.n.01\", [\"cat-o'-nine-tails\", 'cat']),\n",
              " ('caterpillar.n.02', ['Caterpillar', 'cat']),\n",
              " ('big_cat.n.01', ['big_cat', 'cat']),\n",
              " ('computerized_tomography.n.01',\n",
              "  ['computerized_tomography',\n",
              "   'computed_tomography',\n",
              "   'CT',\n",
              "   'computerized_axial_tomography',\n",
              "   'computed_axial_tomography',\n",
              "   'CAT']),\n",
              " ('cat.v.01', ['cat']),\n",
              " ('vomit.v.01',\n",
              "  ['vomit',\n",
              "   'vomit_up',\n",
              "   'purge',\n",
              "   'cast',\n",
              "   'sick',\n",
              "   'cat',\n",
              "   'be_sick',\n",
              "   'disgorge',\n",
              "   'regorge',\n",
              "   'retch',\n",
              "   'puke',\n",
              "   'barf',\n",
              "   'spew',\n",
              "   'spue',\n",
              "   'chuck',\n",
              "   'upchuck',\n",
              "   'honk',\n",
              "   'regurgitate',\n",
              "   'throw_up'])]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can also get lemmas for each synonym entry in our list of 10:\n",
        "[ (s.name(), s.lemma_names())  for s in syn]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OWkVa6JbDWgh"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cat',\n",
              " 'guy',\n",
              " 'cat',\n",
              " 'kat',\n",
              " \"cat-o'-nine-tails\",\n",
              " 'Caterpillar',\n",
              " 'big_cat',\n",
              " 'computerized_tomography',\n",
              " 'cat',\n",
              " 'vomit']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[s.lemma_names()[0] for s in syn]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "q4K8rcg7FMFK"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['the flesh of a chicken used for food',\n",
              " 'a domestic fowl bred for flesh or eggs; believed to have been developed from the red jungle fowl',\n",
              " 'a person who lacks confidence, is irresolute and wishy-washy',\n",
              " 'a foolhardy competition; a dangerous activity that is continued until one competitor becomes afraid and stops',\n",
              " 'easily frightened']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 4.7: Now repeat the process by finding the synset for an adjectival word of your choice.\n",
        "syn = wn.synsets('chicken')\n",
        "[s.definition() for s in syn]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synset name :   wimp.n.01\n",
            "\n",
            "Synset Part of Speech  :  n\n",
            "\n",
            "Synset meaning :  a person who lacks confidence, is irresolute and wishy-washy\n",
            "\n",
            "Synset example :  []\n",
            "\n",
            "Synset root hypernym:   [Synset('entity.n.01')]\n",
            "\n",
            "Synset hypernyms:   [Synset('weakling.n.01')]\n",
            "\n",
            "Synset hyponyms:   []\n"
          ]
        }
      ],
      "source": [
        "# Solution\n",
        "\n",
        "chck = syn[2] # Let's look at some of the details for the second element\n",
        "\n",
        "print (\"Synset name :  \", chck.name())\n",
        "print (\"\\nSynset Part of Speech  : \", chck.pos())\n",
        "\n",
        "print (\"\\nSynset meaning : \", chck.definition())\n",
        "\n",
        "print (\"\\nSynset example : \", chck.examples())\n",
        "\n",
        "print (\"\\nSynset root hypernym:  \", chck.root_hypernyms())\n",
        "\n",
        "print (\"\\nSynset hypernyms:  \", chck.hypernyms())\n",
        "\n",
        "print (\"\\nSynset hyponyms:  \", chck.hyponyms())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('chicken.n.01', ['chicken', 'poulet', 'volaille']),\n",
              " ('chicken.n.02', ['chicken', 'Gallus_gallus']),\n",
              " ('wimp.n.01', ['wimp', 'chicken', 'crybaby']),\n",
              " ('chicken.n.04', ['chicken']),\n",
              " ('chicken.s.01',\n",
              "  ['chicken',\n",
              "   'chickenhearted',\n",
              "   'lily-livered',\n",
              "   'white-livered',\n",
              "   'yellow',\n",
              "   'yellow-bellied'])]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show the list of lemma_names for that word.\n",
        "[ (s.name(), s.lemma_names())  for s in syn]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'lily-livered', 'white-livered', 'crybaby', 'Gallus_gallus', 'chickenhearted', 'wimp', 'yellow', 'chicken', 'volaille', 'yellow-bellied', 'poulet'}\n"
          ]
        }
      ],
      "source": [
        "# As a related task, reduce that list of lemma names to its unique set in order to eliminate duplicates.\n",
        "\n",
        "lemma_name = [name for s in syn for name in s.lemma_names()]\n",
        "chicken_lemmas = set(lemma_name)\n",
        "print(chicken_lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "M2GB7qH1KN9D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word         POS    Definition\n",
            "-----------  -----  ----------------------------------------------------------------------------------------------\n",
            "return       n      the income or profit arising from such transactions as the sale of land or other property\n",
            "take         n      the act of photographing a scene or part of a scene without interruption\n",
            "take         v      carry out\n",
            "take         v      require (time or space)\n",
            "lead         v      take somebody somewhere\n",
            "take         v      get into one's hands, take physically\n",
            "assume       v      take on a certain form, attribute, or aspect\n",
            "take         v      interpret something in a certain way; convey a particular meaning or impression\n",
            "bring        v      take something or somebody with oneself somewhere\n",
            "take         v      take into one's possession\n",
            "take         v      travel or go by means of a certain kind of transportation, or a certain route\n",
            "choose       v      pick out, select, or choose from a number of alternatives\n",
            "accept       v      receive willingly something given or offered\n",
            "fill         v      assume, as of positions or roles\n",
            "consider     v      take into consideration for exemplifying purposes\n",
            "necessitate  v      require as useful, just, or proper\n",
            "take         v      experience or feel or submit to\n",
            "film         v      make a film or photograph of something\n",
            "remove       v      remove something concrete, as by lifting, pushing, or taking off, or remove something abstract\n",
            "consume      v      serve oneself to, or consume regularly\n",
            "take         v      accept or undergo, often unwillingly\n",
            "take         v      make use of or accept for some purpose\n",
            "take         v      take by force\n",
            "assume       v      occupy or take on\n",
            "accept       v      admit into a group or community\n",
            "take         v      ascertain or determine by measuring, computing or take a reading from a dial\n",
            "learn        v      be a student of a certain subject\n",
            "claim        v      take as an undesirable consequence of some event or state of affairs\n",
            "take         v      head into a specified direction\n",
            "aim          v      point or cause to go (blows, weapons, or objects such as photographic equipment) towards\n",
            "take         v      be seized or affected in a specified way\n",
            "carry        v      have with oneself; have on one's person\n",
            "lease        v      engage for service under a term of contract\n",
            "subscribe    v      receive or obtain regularly\n",
            "take         v      buy, select\n",
            "take         v      to get into a position of having, e.g., safety, comfort\n",
            "take         v      have sex with; archaic use\n",
            "claim        v      lay claim to; as of an idea\n",
            "accept       v      be designed to hold or take\n",
            "contain      v      be capable of holding or containing\n",
            "take         v      develop a habit\n",
            "drive        v      proceed along in a vehicle\n",
            "take         v      obtain by winning\n",
            "contract     v      be stricken by an illness, fall victim to an illness\n"
          ]
        }
      ],
      "source": [
        "# There are a couple more useful things we can do with a synset.\n",
        "from tabulate import tabulate # To make a neat table\n",
        "takesyn = wn.synsets('take') # The word \"take\" has many senses - noun and verb\n",
        "poslist = [(s.lemma_names()[0], s.pos(), s.definition()) for s in takesyn]\n",
        "print(tabulate(poslist,  headers=[\"Word\", \"POS\", \"Definition\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "K2M0wbIHrUjq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.1111111111111111"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "birdsyn = wn.synset('bird.n.01')\n",
        "goatsyn = wn.synset('goat.n.01')\n",
        "sheepsyn = wn.synset('sheep.n.01')\n",
        "birdsyn.path_similarity(goatsyn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pdlWtyc9tFAx"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.1111111111111111"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "birdsyn.path_similarity(sheepsyn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rpSb4pNOtoYB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.3333333333333333"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "goatsyn.path_similarity(sheepsyn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nTddgazJwb-P"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet_ic to C:\\Users\\Black\n",
            "[nltk_data]     Knight\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.1111111111111111, 1.4403615823901665, 5.2175784741185165)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Leacock-Chodorow similarity also uses the path lengths, but also considers how deep the least common ancestor is in the hierarchy.\n",
        "# Resnik similarity also considers the relative frequency of a word in a  corpus you provide.\n",
        "# We show the earlier value of path similarity here justfor the sake of comparison.\n",
        "nltk.download('wordnet_ic')\n",
        "from nltk.corpus import wordnet_ic\n",
        "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
        "birdsyn.path_similarity(goatsyn), birdsyn.lch_similarity(goatsyn), birdsyn.res_similarity(goatsyn, brown_ic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fnEjDFJnwVQ1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.3333333333333333, 2.538973871058276, 8.005695458684853)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 4.8: Add code to produce the Leacock-Chodorow and the Resnick similarity for sheepsyn to goatsyn\n",
        "# Solution\n",
        "\n",
        "sheepsyn.path_similarity(goatsyn), sheepsyn.lch_similarity(goatsyn), sheepsyn.res_similarity(goatsyn, brown_ic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "EIsWfofPyiui"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('good.n.01', 'benefit'),\n",
              " ('good.n.02', 'moral excellence or admirableness'),\n",
              " ('good.n.03', 'that which is pleasing or valuable or useful'),\n",
              " ('commodity.n.01', 'articles of commerce'),\n",
              " ('good.a.01',\n",
              "  'having desirable or positive qualities especially those suitable for a thing specified'),\n",
              " ('full.s.04', 'having the normally expected amount'),\n",
              " ('good.a.03', 'morally admirable'),\n",
              " ('estimable.s.01', 'deserving of esteem and respect'),\n",
              " ('beneficial.s.01', 'promoting or enhancing well-being'),\n",
              " ('good.s.04', 'agreeable or pleasing'),\n",
              " ('good.s.05', 'of moral excellence'),\n",
              " ('adept.s.01', 'having or showing knowledge and skill and aptitude'),\n",
              " ('good.s.07', 'thorough'),\n",
              " ('dear.s.02', 'with or in a close or intimate relationship'),\n",
              " ('dependable.s.03', 'financially sound'),\n",
              " ('good.s.10', 'most suitable or right for a particular purpose'),\n",
              " ('good.s.11', 'resulting favorably'),\n",
              " ('effective.s.03', 'exerting force or influence'),\n",
              " ('good.s.13', 'capable of pleasing'),\n",
              " ('good.s.14', 'appealing to the mind'),\n",
              " ('good.s.15', 'in excellent physical condition'),\n",
              " ('good.s.16', 'tending to promote physical well-being; beneficial to health'),\n",
              " ('good.s.17', 'not forged'),\n",
              " ('good.s.18', 'not left to spoil'),\n",
              " ('good.s.19', 'generally admired'),\n",
              " ('well.r.01',\n",
              "  \"(often used as a combining form) in a good or proper or satisfactory manner or to a high standard (`good' is a nonstandard dialectal variant for `well')\"),\n",
              " ('thoroughly.r.02',\n",
              "  \"completely and absolutely (`good' is sometimes used informally for `thoroughly')\")]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Antonyms: If we want to find a word with the opposite meaning, WordNet can provide us with choices:\n",
        "syn = wn.synsets('good') # Grab all of the synonyms for good\n",
        "[(s.name(), s.definition()) for s in syn] # Display them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "24ghhlidz7cH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[Lemma('bad.a.01.bad')]]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "goodsyn = wn.synset('good.a.01')\n",
        "[l.antonyms() for l in goodsyn.lemmas()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "U88lc1L024RT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[Lemma('good.a.01.good')]]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 4.9: Now look up the antonym(s) for the adjectival sense of bad.\n",
        "\n",
        "# Solution\n",
        "syn = wn.synsets('bad') # Grab all of the synonyms for good\n",
        "\n",
        "badsyn = wn.synset('bad.a.01')\n",
        "[l.antonyms() for l in badsyn.lemmas()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DGWWImT3QdW"
      },
      "source": [
        "## Section 4.3:\n",
        "\n",
        "Lets look at a way that we could use the structure of WordNet to get at different forms of sentiment. Let's start simple and then work our way to some more complex options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "z8wADjvN5uN7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Unnamed: 0', 'Business', 'Comments', 'Rating', 'Sentiment Label', 'Sentiment Score', 'URL']\n"
          ]
        }
      ],
      "source": [
        "# First, we need a dataset to work on.\n",
        "# Anything with some comments and a sentiment indicator will do.\n",
        "# This URL refers to a project on Github that Dennis Pan posted.\n",
        "# If the URL is unavailable, find another CSV file with text comments and a sentiment score.\n",
        "import pandas as pd\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/dennisypan/Quick-N-Dirty-Sentiment-Analysis/master/sentiment_on_business.csv\")\n",
        "print(list(data)) # Show column names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dppMX25BBG85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(70, 7)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We will be examining the Comments for hints about sentiment along with Sentiment Label and Sentiment Score,\n",
        "# which were set up by the creator of the dataset to help with machine learning tasks.\n",
        "# Show the number of rows and columns in this data set\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qHuGS_YN-XZE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    Boyfriend and I ordered togo sushi and picked ...\n",
              "1    I am pleased to recommend this restaurant in S...\n",
              "2    great food been going to japan town since i wa...\n",
              "3    Small sushi boat restaurant located in SJ Japa...\n",
              "4    Sushi Maru is one of the many gems of SJ's Jap...\n",
              "Name: Comments, dtype: object"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# View a few comments. Note that the name of this data field ('Comments') is peculiar to the Dennis Pan Yelp/Sushi data.\n",
        "# If you change the dataset, you may need to change the name of the field where the comments are found.\n",
        "data['Comments'][0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "JLV5ZKQD-p4N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am pleased to recommend this restaurant in San Jose's \"Japantown\"right next door to Union Bank. The restaurant is quite popular and fills up at lunchtime.  The service is fast and friendly.   There is a sushi-boat bar for those who are so inclined as it is definitely well-liked.  The booths are spacious and comfortable.  The  luncheon menu  is somewhat limited. Their tempura appetizer is really remarkable, both in taste and price, ample and affordable.  I was slightly disappointed in their salad dressing.  It's usually so good in Japanese restaurants. The restaurant is clean as is the ladies' room. Parking is not a problem. There is both street (metered) and lot parking.\n",
            "Sushi Maru is one of the many gems of SJ's Japan town. They have a rotating sushi belt, but also a solid menu of entrees as well as individual nigiri and hand roll options. My friend and I both ordered the Chef's sushi special and some hot sake- the toro and salmon belly with skin were to DIE for. It was a great way to get a taste of everything. I definitely want to come back and try some of the other entrees or for some more yummy nigiri, but ultimately...this is a very, very sushi place but not the best I've tried. I'd give 3.75 stars if I could  However, I'd still come back if I'm looking for a good spot in Japan town.\n",
            "Sushi Maru is a conveyor belt sushi restaurant and a popular lunch spot during the week. Unlike the Sushi Maru in Milpitas, this location has GOOD food. I've never really tried the sushi here and usually end up ordering from their lunch menu/specials. For about $10.99-12.99, you can get an entree (or two depending on if you opt for the combo) plus salad and soup. The options are decent and the food is filling. If you come here often, you probably already know about their stamp card. I think you need to spend $200 in six months in order to save $15 on your next bill.  This place opens at 11:30am during the week so come early because this place fills up quickly!\n",
            "Food: Sushi, bento, udon, donburi.....all consistent and good flavors. If you are seated at the sushi bar, you can order as needed as well as take from the conveyor belt. The rolls are good size and there is board of what fresh fish are available. The wasabi and ginger are in community containers, so you just be cautious, and you could ask for a fresh batch from behind the counter if you need to. Overall the dishes are consistent with what Japanese cuisine is and you will always have decent meal. Service: Sit at the booth closest to the sushi bar if you don't want to wait for you rolls as you can grab from the belt and order from the menu. Seats at the bar allow to order from the menu but the space is limited for bigger plates. When it gets busy you may wait a little longer for your orders. Ambiance: Clean, simple and good for smaller groups and easy to have conversations.\n",
            "I struggle with this joint.  To me it's a \"tweener\" meaning it's sometimes \"just-ok\" while others it's \"Not-ok.\"  Let's go with a true 2.5 Stars here. I've had good service and some not-so-good service here which is annoying.   The food isn't bad but the quality of the sushi is borderline and it's debatable on how long they've been riding the \"sushi-go-round.\"  Oh, and the price for sushi is in the realm of \"that wasn't worth it\" and \"stick to non-sushi dishes.\"\n",
            "Delicious food and great service, but a little expensive...  I'm a big fan of their pumpkin croquette and the nanook (seared salmon nigiri). Their dinner boxes and other sushi are pretty good too. Total cost for dinner for two: around $75. Not sure if this quite makes up for the price, but they do have a frequent diner's stamp card. However, there is a time frame that you have to earn the stamps within.  Sitting at the conveyor belt sushi bar was cho natsukashii (nostalgic).\n",
            "I've come here once before and the first time was good. This time... no. The food on the rotating bar was NOT fresh. The bento box was ok but as I was sitting there eating, the sushi chef started picking his nose and then picked up his phone and checked his POF account. Yeah, never coming here again.\n",
            "This place is amazing! The staff was so friendly and we were seated quickly (it was also 30 min from closing, so this could be different for earlier in the day). My partner and I decided to try a little bit of everything, so he got a plate that came with two entrées, salad, rice, and 3 CA rolls. I got a bowl of chicken udon noodles. We both shared the Nanook rolls and Jackson rolls. They were all so delicious! This place also has that cool conveyor belt for their sushi and you have the option of just eating those. Pricing for sushi is based on the kind of plate you get, lowest being $3.99 and highest being $6.49. Very good food, will be visiting again.\n",
            "This is my favorite sushi place in San Jose! Super good, super high quality too. Great people that work there. My family and I have been going to this restaurant for so long and it gets better and better! Really recommend going.\n",
            "Stopped by for a quick lunch on the way back from Santa Cruz. Although it was memorials day, I was surprised that the restaurant was only half full. We had a big group of 15, but were seated pretty quickly. It's a standard Japanese restaurant with bento boxes, sushi, and bowls.  They also have daily specials, which change everyday. I got the sushi lunch ($15.99) which comes with nigiri, baked fish, california role, oranges, and some small side dishes. It also comes with a salad and miso soup, which came out first. The sushi was pretty standard, I like how they didn't put a lot of rice with the sushi. The baked fish was pretty fatty, but it wasn't too salty or under seasoned. The seaweed salad on the side was he perfect pallete cleanser.  I also liked how the dish came with slices of Orange, which was refreshing after the meal. You get what you pay for and it was the perfect amount for me. Overall, a good Japanese restaurant with friendly staff and clean atmosphere.   Highly recommend stopping by if you are in San Jose Japan town.\n",
            "The plusses: Food is very good, service is friendly, reasonable price, and we didn't have to wait long.  Also, the sushi at the bar is all covered with plastic covers to keep it sanitary -- most restaurants just let it sit out exposed to the elements. The minuses: crowded, feels a bit cramped (they don't seem to make the best use of space the way other restaurants do in Japantown), and we had to sit at the sushi bar because they're aren't enough tables. Overall this was a very good restaurant, but if I ever go back I'll choose a time when it isn't rush hour.\n",
            "You need to chose the guy in the bar to make Nanook but not the women. She made it in heavy soil sauce flavor which cover all the salmon taste.  The bad thing is after I talked to the waiter, she asked me rudely 'so what do you want me to do?' Update 5/25/17 after a year visit again. Taste still good. Eggplant and kalbi recommend. Just don't argue the food with them you will be fine.\n",
            "So yummy here, we where here on Christmas Eve. And we wanted sushi, thank good it was open that day. Service was great, food was excellent and the employees where up to beat. I can't wait to eat here again. The bento  box was enough food for us to share.. we also got some sushi and oysters at the bar to keep us full. Hot tea is always a bonus..\n",
            "This is my favorite sushi place. The staff and chefs are friendly and always working hard. The location is located conveniently in downtown. Just a couple blocks away from Coleman and Taylor. This is my go to place. Whenever I have a craving for sushi, I always end up choosing their teriyaki salmon over the sushi on the conveyor belt. It's not that the I don't like their sushi, it's just that their salmon is REALLY good. It's never dry and it's never too saucy. Also, they have THE best baked mussels out there. Its always made fresh to order, so it always comes out hot. Its very flavorful: Its sweet, chewy, and a little bit spicy. Other places can't compare.  Some drawback is that it is quite small inside. It can get pretty crowded and you while you're seated, you may have to wait a little while until more sushi plates are placed on the belt. They also don't have those special sushi rolls like the \"Dragon roll\" or \"Spider-Man roll.\" But it's okay, you're in Japantown. This ain't no Asian fusion place. With all that, I don't mind the wait. It's amazing to watch and see how the chefs move so quickly. I always end up having a good time and I leave with my belly full! Can't complain about that\n",
            "Best bento place ever! We eat here like every month. We get the Chicken Teriyaki and Katsu Combo & Lion King Roll. The best, they never disappoint. Plus, very affordable and great customer service. I'm speechless. They're that good!\n",
            "I came here for lunch today with my friend and his Mom, and was surprised when I saw that it was a sushi boat place. I had never come to a sushi boat place before because my Dad is a sushi snob. I was surprised by how much space the conveyer belt took up.  The sushi conveyer belt was an efficient way to add things to your order, but over time, as more people took sushi from the conveyer belt, there weren't many things to grab.  Therefore we needed to ask the sushi chef to make the sushi for us, which defeated the purpose of the conveyer belt. My friend recommended getting the nanook, which was seared salmon with a sweet sauce. When I first bit into the salmon, a bunch of fat poured out from the searing. The searing also made the salmon much softer, and brought out a lot of the natural salmon flavor. Most of the fish was decent quality, except for the tuna, which didn't have much flavor and was a little hard. Overall, this is a pretty good meal, and if you take advantage of the conveyer belt, you can really fill yourself up here.\n",
            "I had the Chicken teriyaki and Salmon along with the Eel and Octopus Sushi and it was very good. The service here was very fast and friendly. Prices for what you get are reasonable. I would definitely come back again.\n",
            "Our families favorite sushi spot here in Japantown. Love the options and the chefs really make sure you are full and happy. Sushi boat has good options and more you can choose from on the far wall. More of a traditional style sushi, not many fancy specialty rolls. Bento Boxes seem to be filling as well. Overall, have great experience here and we continue to go back when we are walking down Japantown for sushi.\n",
            "The menu is slightly limited, not many rolls (no special rolls, rolls come in 2-6 pieces at 2.99-5.99 per order).  Lunch bento box is $11. The quality of food is good (the raw fish tasted fresh) and everything just tastes light, as it should.  Miso soup is top notch.  Rolls are tasty. They have a conveyer belt, and if you have a group of 3 or more you can sit at an actual table next to the belt.\n",
            "Food was pretty good with decent price. The place has both sushi belt and tables so you can order with the server or may pick up & order to the sushiman. The server doesn't come often to take your order so if you're not ordering hot food, better off order to the sushi bar directly if you're eating at the booth or the bar.\n",
            "It was my first time coming here for lunch with my dad! We came around 1pm and decided to sit by the bar.  For their food, I just stuck with nigiri. I ordered ebi and a tempura nigiri. Their ebi tasted really fresh. One of the freshest sushi I've tasted I feel. Their sushi rice is also good with its light sweetness and perfect texture. The tempura nigiri was also delicious because they dipped the rice in some kind of sauce, but it tasted delicious and it went well with the tempura. My dad ordered the fried oysters for appetizers and he enjoyed them as well.  Since we came close to closing time, they would put two different kinds of sushi on one plate so you get more on one plate! So awesome. It'd be a shame wasting all that sushi anyways. I would definitely come back for lunch!\n",
            "Sat at the bar and the service was terrible.  Fish was presliced and the portion of fish for nigiri was very thin.  Variety was Not very good, most of the standard rolls included crab salad, which i dont like.  Not a very good value.  Will not go back.\n",
            "First time here and the people were very nice and the food was good. Enjoyed the beef tariyaki and some of the sushi's my friend was having me tryout.\n",
            "My go to sushi bar in San Jose.  I have come here for many years and when I read yelp reviews for sushi restaurants I quickly appreciate that people don't know what good sushi tastes like.  High quality, authentic Japanese sushi, and the prices haven't gone up in years plus they give you a stamp card to come back.\n",
            "My sister in law and brother in law took me here for the first tine.  I must say, I LOVE it!  My favorite ones are the nanook and tai....so ooooo good!  Will definitely come back!\n",
            "Not much of a wait during 12:30 lunch time. It is somewhat busy. There's a sushi boat conveyor belt at the bar or you can sit at a regular table. Service was fast too. Sashimi was very fresh, good quality, and delicious! Not to mention, its not too expensive.   I love white fish so I ordered mostly white fish sashimi. Hamachi, Albacore, Toro, Halibut, Saba and two other ones I forgot the name. Love the california rolls too! Real crabmeat and huge. The sushi rice was good too.\n",
            "Showed up and there was no table my kid could get a high chair for without obstructing the walk ways. The hostess, who also busses tables and serves, kindly asked if a table of 3 dudes drinking wouldn't mind moving to another table. They agreed and we were seated. Super accommodating. The food was pretty dang good. I walked out feeling %100 satisfied and thankful I live walking distance. Only thing I have to say is TURN YOUR HEATER OFF!!! It was so freakin hot in there. 5 star none the less.\n",
            "Walking around sj japantown in the carmeet and me and my sibs decide to eat some Japanese food. This place is pretty busy but we got seated right away. However, we waited about 20 mins to have our order taken. The food was decent. It wasnt the best japanese food but it was okay. I love how the waiters speak japanese though. Overall experience was good and the food was decent.\n",
            "Sushi Maru in Jtown is top notch value. I would say arguably the best value sushi in San Jose. Now, don't get me wrong when I say value, the fish here is actually quite fresh and good. Their daily lunch specials are always a really good deal, usually coming with some kind of dashi based soup, a mini chirashi bowl or tuna-don, and some other meat entree. Their sushi and sashimi specials are also really quite good for how cheap they are (and beautiful presentation too!). They don't take reservations here, but if you're a regular, you can get a discount card where they give you a $15 gift card every few visits. It's a really solid place to have as your \"go-to\" sushi place in SJ.\n"
          ]
        }
      ],
      "source": [
        "# We know that the word \"good\" has many senses, but they are generally all positive as we saw in the WordNet work above.\n",
        "# What if we just find all the comments where a good-related word appears in the text?\n",
        "# Would that tell us anything about sentiment?\n",
        "# First, let's see if we can just match one word\n",
        "for g in data['Comments']:\n",
        "  if 'good' in g.lower():\n",
        "    print(g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "kGT9sL2y_8Td"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44\n"
          ]
        }
      ],
      "source": [
        "# Now look for good AND all of its synonyms. Notice the use of set().\n",
        "lemmas = set([s.lemma_names()[0] for s in wn.synsets('good')])\n",
        "\n",
        "# This is going to be a longer list, so let's just count the matches for now rather than printing them all.\n",
        "matches = 0\n",
        "\n",
        "for g in data['Comments']:\n",
        "  for l in lemmas:\n",
        "    if l in g.lower():\n",
        "      matches += 1\n",
        "\n",
        "print(matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "iBnFhZesBBgA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "70"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# That's useful: We got a lot of mentions of all good-related words in the 70 comments stored in this dataset.\n",
        "# This time we will count how many matches we get per comment.\n",
        "\n",
        "lemmas = set([s.lemma_names()[0] for s in wn.synsets('good')])\n",
        "\n",
        "# Here we will assign matches into a list containing 70 elements\n",
        "matches = [0] * data.shape[0] # Fill a list with 70 zeroes\n",
        "\n",
        "# This loop uses an enumerator, which is a cool Python solution to a common programming problem.\n",
        "# The \"i\" that the enumerator produces is the index of the corresponding g.\n",
        "# In this case the \"g\"s are the comments extracted from each successive row.\n",
        "for i, g in enumerate(data['Comments']): # Loop over all comments\n",
        "  for l in lemmas:  # Loop over all lemmas of good\n",
        "    if l in g.lower(): # If we get a match, increment the count\n",
        "      matches[i] += 1\n",
        "\n",
        "len(matches) # We should have 70 entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tSgqQwI1DYPu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([35.,  0.,  0., 27.,  0.,  0.,  7.,  0.,  0.,  1.]),\n",
              " array([0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3. ]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# What's the distribution of counts\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(matches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9M6Ovk48bq_"
      },
      "source": [
        "#Section 4.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "cZOQ0b7CEoaz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.12068240259212179)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's use pandas to calculate the simple Pearson's correlation between the sentiment score in the data set and the count of good-related words.\n",
        "x = pd.Series(data['Sentiment Score'])\n",
        "y = pd.Series(matches)\n",
        "x.corr(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "# What if we used the wordbad instead of the word good. Would that make any difference?\n",
        "\n",
        "# Question 4.10: Copy the loop from four blocks above and change from good to bad\n",
        "\n",
        "lemmas = set([s.lemma_names()[0] for s in wn.synsets('bad')])\n",
        "\n",
        "# This is going to be a longer list, so let's just count the matches for now rather than printing them all.\n",
        "matches = 0\n",
        "\n",
        "for g in data['Comments']:\n",
        "  for l in lemmas:\n",
        "    if l in g.lower():\n",
        "      matches += 1\n",
        "\n",
        "print(matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
              " array([2.5, 2.6, 2.7, 2.8, 2.9, 3. , 3.1, 3.2, 3.3, 3.4, 3.5]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGn1JREFUeJzt3XtsVvX9wPFvAbkYpeoQUCSrOq9TwIEwRGd0KJnEyx/LCBohzMvwFqUxE7yAeIM5Uf4QJaJuZgkTNWqMMJxj4uJgYYJuahDHkNE4ucUJDGZReH75nl/aUWmRutIPbV+v5EzO4Zz2PCfP2jffc3nKSqVSKQEABGkX9Y0BADIxAgCEEiMAQCgxAgCEEiMAQCgxAgCEEiMAQCgxAgCE6pBagJ07d6Z//vOf6eCDD05lZWXRuwMA7IX8XNUtW7akI488MrVr165lx0gOkd69e0fvBgDwNVRVVaWjjjqqZcdIHhGpeTFdu3aN3h0AYC9s3ry5GEyo+T3eomOk5tRMDhExAgAty1ddYuECVgAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAFpWjPzhD39IF154YfEJfPnxri+++OJXbrNw4cL0ne98J3Xq1Cl961vfSr/85S+/7v4CAG09RrZu3Zr69u2bZsyYsVfrf/jhh2n48OHpnHPOSW+//Xa66aab0pVXXpleeeWVr7O/AEAr0+gPyvvBD35QTHtr5syZ6eijj07Tpk0r5k866aT0xhtvpIceeigNGzassd8eAGhl9vk1I4sXL05Dhw6tsyxHSF7ekOrq6uJjh3edAIDWqdEjI421du3a1KNHjzrL8nwOjP/85z+pS5cuu20zZcqUNHny5H29a8B+omL83NTSrJ46PHoXoNXYL++mmTBhQtq0aVPtVFVVFb1LAEBLHRnp2bNnWrduXZ1leb5r1671jopk+a6bPAEArd8+HxkZPHhwWrBgQZ1lr776arEcAKDRMfLvf/+7uEU3TzW37uY/r1mzpvYUy6hRo2rXHzt2bFq1alX66U9/mt5///30yCOPpGeeeSaNGzeuKV8HANBWYuTNN99Mp512WjFllZWVxZ8nTpxYzH/88ce1YZLl23rnzp1bjIbk55PkW3wff/xxt/UCAIWyUqlUSvu5fOdNeXl5cTFrvtYEaF3cTQOt097+/t4v76YBANoOMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAtLwYmTFjRqqoqEidO3dOgwYNSkuWLNnj+tOnT08nnHBC6tKlS+rdu3caN25c+uyzz77uPgMAbTlG5syZkyorK9OkSZPSsmXLUt++fdOwYcPS+vXr611/9uzZafz48cX6y5cvT0888UTxNW699dam2H8AoK3FyIMPPpiuuuqqNGbMmHTyySenmTNnpgMPPDA9+eST9a6/aNGiNGTIkHTppZcWoynnn39+Gjly5FeOpgAAbUOjYmT79u1p6dKlaejQof/9Au3aFfOLFy+ud5szzjij2KYmPlatWpXmzZuXLrjggga/T3V1ddq8eXOdCQBonTo0ZuWNGzemHTt2pB49etRZnufff//9erfJIyJ5uzPPPDOVSqX0xRdfpLFjx+7xNM2UKVPS5MmTG7NrAEALtc/vplm4cGG677770iOPPFJcY/L888+nuXPnprvvvrvBbSZMmJA2bdpUO1VVVe3r3QQAWsLISLdu3VL79u3TunXr6izP8z179qx3mzvuuCNdfvnl6corryzmTz311LR169Z09dVXp9tuu604zfNlnTp1KiYAoPVr1MhIx44dU//+/dOCBQtql+3cubOYHzx4cL3bbNu2bbfgyEGT5dM2AEDb1qiRkSzf1jt69Og0YMCANHDgwOIZInmkI99dk40aNSr16tWruO4ju/DCC4s7cE477bTimSQrV64sRkvy8pooAQDarkbHyIgRI9KGDRvSxIkT09q1a1O/fv3S/Pnzay9qXbNmTZ2RkNtvvz2VlZUV//3oo4/S4YcfXoTIvffe27SvBABokcpKLeBcSb61t7y8vLiYtWvXrtG7AzSxivFzU0uzeurw6F2A1Fp+f/tsGgAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQAglBgBAEKJEQCg5cXIjBkzUkVFRercuXMaNGhQWrJkyR7X//TTT9N1112XjjjiiNSpU6d0/PHHp3nz5n3dfQYAWpEOjd1gzpw5qbKyMs2cObMIkenTp6dhw4alFStWpO7du++2/vbt29N5551X/N1zzz2XevXqlf7xj3+kQw45pKleAwDQlmLkwQcfTFdddVUaM2ZMMZ+jZO7cuenJJ59M48eP3239vPyTTz5JixYtSgcccECxLI+qAAA0+jRNHuVYunRpGjp0aO2ydu3aFfOLFy+ud5uXXnopDR48uDhN06NHj3TKKaek++67L+3YsaPB71NdXZ02b95cZwIAWqdGxcjGjRuLiMhRsas8v3bt2nq3WbVqVXF6Jm+XrxO544470rRp09I999zT4PeZMmVKKi8vr5169+7dmN0EAFqQfX43zc6dO4vrRR577LHUv3//NGLEiHTbbbcVp3caMmHChLRp06baqaqqal/vJgDQEq4Z6datW2rfvn1at25dneV5vmfPnvVuk++gydeK5O1qnHTSScVISj7t07Fjx922yXfc5AkAaP0aNTKSwyGPbixYsKDOyEeez9eF1GfIkCFp5cqVxXo1PvjggyJS6gsRAKBtafRpmnxb76xZs9JTTz2Vli9fnq655pq0devW2rtrRo0aVZxmqZH/Pt9Nc+ONNxYRku+8yRew5gtaAQAafWtvvuZjw4YNaeLEicWpln79+qX58+fXXtS6Zs2a4g6bGvni01deeSWNGzcu9enTp3jOSA6TW265pWlfCQDQIpWVSqVS2s/lW3vzXTX5YtauXbtG7w7QxCrGz00tzeqpw6N3AVJr+f3ts2kAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgFBiBAAIJUYAgJYXIzNmzEgVFRWpc+fOadCgQWnJkiV7td3TTz+dysrK0iWXXPJ1vi0A0Ao1OkbmzJmTKisr06RJk9KyZctS375907Bhw9L69ev3uN3q1avTzTffnM4666z/ZX8BgLYeIw8++GC66qqr0pgxY9LJJ5+cZs6cmQ488MD05JNPNrjNjh070mWXXZYmT56cjjnmmP91nwGAthoj27dvT0uXLk1Dhw797xdo166YX7x4cYPb3XXXXal79+7piiuu2KvvU11dnTZv3lxnAgBap0bFyMaNG4tRjh49etRZnufXrl1b7zZvvPFGeuKJJ9KsWbP2+vtMmTIllZeX1069e/duzG4CAC3IPr2bZsuWLenyyy8vQqRbt257vd2ECRPSpk2baqeqqqp9uZsAQKAOjVk5B0X79u3TunXr6izP8z179txt/b///e/FhasXXnhh7bKdO3f+/zfu0CGtWLEiHXvssbtt16lTp2ICAFq/Ro2MdOzYMfXv3z8tWLCgTlzk+cGDB++2/oknnpjeeeed9Pbbb9dOF110UTrnnHOKPzv9AgA0amQky7f1jh49Og0YMCANHDgwTZ8+PW3durW4uyYbNWpU6tWrV3HdR34OySmnnFJn+0MOOaT475eXAwBtU6NjZMSIEWnDhg1p4sSJxUWr/fr1S/Pnz6+9qHXNmjXFHTYAAHujrFQqldJ+Lt/am++qyRezdu3aNXp3gCZWMX5uamlWTx0evQuQWsvvb0MYAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAhBIjAEAoMQIAtLwYmTFjRqqoqEidO3dOgwYNSkuWLGlw3VmzZqWzzjorHXroocU0dOjQPa4PALQtjY6ROXPmpMrKyjRp0qS0bNmy1Ldv3zRs2LC0fv36etdfuHBhGjlyZHrttdfS4sWLU+/evdP555+fPvroo6bYfwCghSsrlUqlxmyQR0JOP/309PDDDxfzO3fuLALjhhtuSOPHj//K7Xfs2FGMkOTtR40atVffc/Pmzam8vDxt2rQpde3atTG7C7QAFePnppZm9dTh0bsA+729/f3dqJGR7du3p6VLlxanWmq/QLt2xXwe9dgb27ZtS59//nk67LDDGlynurq6eAG7TgBA69SoGNm4cWMxstGjR486y/P82rVr9+pr3HLLLenII4+sEzRfNmXKlKKkaqY88gIAtE7NejfN1KlT09NPP51eeOGF4uLXhkyYMKEY0qmZqqqqmnM3AYBm1KExK3fr1i21b98+rVu3rs7yPN+zZ889bvvAAw8UMfK73/0u9enTZ4/rdurUqZgAgNavUSMjHTt2TP37908LFiyoXZYvYM3zgwcPbnC7+++/P919991p/vz5acCAAf/bHgMAbXdkJMu39Y4ePbqIioEDB6bp06enrVu3pjFjxhR/n++Q6dWrV3HdR/azn/0sTZw4Mc2ePbt4NknNtSUHHXRQMQEAbVujY2TEiBFpw4YNRWDksOjXr18x4lFzUeuaNWuKO2xqPProo8VdOD/84Q/rfJ38nJI777yzKV4DANCWnjMSwXNGoHXznBFonfbJc0YAAJqaGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRACCUGAEAQokRAKDlxciMGTNSRUVF6ty5cxo0aFBasmTJHtd/9tln04knnlisf+qpp6Z58+Z93f0FANp6jMyZMydVVlamSZMmpWXLlqW+ffumYcOGpfXr19e7/qJFi9LIkSPTFVdckd566610ySWXFNO7777bFPsPALRwZaVSqdSYDfJIyOmnn54efvjhYn7nzp2pd+/e6YYbbkjjx4/fbf0RI0akrVu3ppdffrl22Xe/+93Ur1+/NHPmzL36nps3b07l5eVp06ZNqWvXro3ZXaAFqBg/N7U0q6cOj94F2O/t7e/vDo35otu3b09Lly5NEyZMqF3Wrl27NHTo0LR48eJ6t8nL80jKrvJIyosvvtjg96muri6mGvlF1LwooPXZWb0ttTR+HsHe///kq8Y9GhUjGzduTDt27Eg9evSoszzPv//++/Vus3bt2nrXz8sbMmXKlDR58uTdlucRGID9Qfn06D2AlmPLli3FCEmTxEhzySMvu46m5FNBn3zySfrGN76RysrKUluvzBxlVVVVTlntY45183Ccm4fj3Dwc57ryiEgOkSOPPDLtSaNipFu3bql9+/Zp3bp1dZbn+Z49e9a7TV7emPWzTp06FdOuDjnkkMbsaquX3+Te6M3DsW4ejnPzcJybh+P8X3saEflad9N07Ngx9e/fPy1YsKDOqEWeHzx4cL3b5OW7rp+9+uqrDa4PALQtjT5Nk0+fjB49Og0YMCANHDgwTZ8+vbhbZsyYMcXfjxo1KvXq1au47iO78cYb09lnn52mTZuWhg8fnp5++un05ptvpscee6zpXw0A0PpjJN+qu2HDhjRx4sTiItR8i+78+fNrL1Jds2ZNcYdNjTPOOCPNnj073X777enWW29Nxx13XHEnzSmnnNK0r6SNyKev8jNevnwai6bnWDcPx7l5OM7Nw3FupueMAAA0JZ9NAwCEEiMAQCgxAgCEEiMAQCgxsh/Jt0PnDyE8+OCDU/fu3YtPN16xYsVXbvfpp5+m6667Lh1xxBHFFdzHH398mjdvXrPsc1s71vlW9hNOOCF16dKleMriuHHj0meffdYs+9wSPfroo6lPnz61D4DKzxf6zW9+s8dtnn322XTiiSemzp07p1NPPdV7eR8c51mzZqWzzjorHXroocWUP19syZIlzbrPbeX9XCM/1iI/QTz/rGF3YmQ/8vrrrxdR8ac//al4MNznn3+ezj///OI5Lnv68MLzzjsvrV69Oj333HPFL9T8gyY/64WmPdb5FvX8ydT5tr3ly5enJ554Is2ZM6e4ZZ36HXXUUWnq1KnFB2zm5wude+656eKLL07vvfdevesvWrQojRw5Ml1xxRXprbfeKn5w5+ndd99t9n1vzcd54cKFxXF+7bXXig8zzWGd3/8fffRRs+97az7ONfLP55tvvrkIQBqQb+1l/7R+/fp823Xp9ddfb3CdRx99tHTMMceUtm/f3qz71haP9XXXXVc699xz6yyrrKwsDRkypBn2sPU49NBDS48//ni9f/ejH/2oNHz48DrLBg0aVPrJT37STHvXNo7zl33xxRelgw8+uPTUU0/t8/1qa8c5H9szzjijWGf06NGliy++uFn3r6UwMrIf27RpU/Hfww47rMF1XnrppWKoMP8rPz94Lj9M7r777is+XZmmPdb5AX75X0Q1w9mrVq0qTiFccMEFzbafLVl+T+ah6jz61NDHQeR/pedTBrsaNmxYsZymO85ftm3btmJ0cE/vf77ecb7rrruKU8F5tI/Usj61l///zJ+bbropDRkyZI9Pq82/EH//+9+nyy67rPjFuHLlynTttdcWP1jy6QSa7lhfeumlaePGjenMM88sPonyiy++SGPHjnWa5iu88847xQ/rfG3NQQcdlF544YV08skn17tufqpzzdOca+T5vJymO85fdssttxSfqvrlEOR/O85vvPFGcTr37bffbvb9bHGih2ao39ixY0vf/OY3S1VVVXtc77jjjiv17t27GAqsMW3atFLPnj2bYS/b1rF+7bXXSj169CjNmjWr9Ne//rX0/PPPF8f+rrvuarZ9bYmqq6tLf/vb30pvvvlmafz48aVu3bqV3nvvvXrXPeCAA0qzZ8+us2zGjBml7t27N9Peto3jvKspU6YUpxr+8pe/NMt+tpXjvHnz5lJFRUVp3rx5tcucpmmYGNkP5WsTjjrqqNKqVau+ct3vfe97pe9///t1luU3f+7M/H8amu5Yn3nmmaWbb765zrJf/epXpS5dupR27NixD/eydcnv16uvvrrev8tx99BDD9VZNnHixFKfPn2aae/axnGu8fOf/7xUXl5e+vOf/9xs+9VWjvNbb71V/Bxu37597VRWVlZM+c8rV64M2d/9lWtG9iM5Dq+//vpi2C+fejn66KO/cpt8aiGfmsmnGmp88MEHxW2+HTt23Md73LaOdT6vvuuHQGbt27ev/Xrsnfxera6urvfv8vD3ggUL6izLdzvt7bUP7N1xzu6///509913Fx90mj+FnaY9zvn29HxKJ5+iqZkuuuiidM455xR/zncwsYvoGuK/rrnmmuJfKQsXLix9/PHHtdO2bdtq17n88suLocEaa9asKa6Cv/7660srVqwovfzyy8WQ9j333BP0KlrvsZ40aVJxrH/9618XIym//e1vS8cee2xxBwj1y8cv36H04YcfFqe28nz+l2E+dvUd4z/+8Y+lDh06lB544IHS8uXLi2OeT9288847ga+i9R3nqVOnljp27Fh67rnn6rz/t2zZEvgqWt9x/jKnaRomRvYjuQ3rm37xi1/UrnP22WcXb+hdLVq0qLj9sVOnTsVtvvfee2+da0hommP9+eefl+68884iQDp37lycUrj22mtL//rXv4Jexf7vxz/+cXE9Tv7Fd/jhhxdD2jU/uBt6Pz/zzDOl448/vtjm29/+dmnu3LkBe966j3Net773f44/mvb9vCsx0rCy/D+7jpQAADQn14wAAKHECAAQSowAAKHECAAQSowAAKHECAAQSowAAKHECAAQSowAAKHECAAQSowAAKHECACQIv0fRB2j4gh3lGYAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Question 4.11: Create a histogram of matches\n",
        "# Question 4.12: Write a comment interpreting the histogram\n",
        "# most of the lemmas are for the 3rd meaning of the word bad\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3037: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  c = cov(x, y, rowvar, dtype=dtype)\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2894: RuntimeWarning: divide by zero encountered in divide\n",
            "  c *= np.true_divide(1, fact)\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2894: RuntimeWarning: invalid value encountered in multiply\n",
            "  c *= np.true_divide(1, fact)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "np.float64(nan)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 4.13: Calculate the correlation between Sentiment Score and matches\n",
        " \n",
        "x = pd.Series(data['Sentiment Score'])\n",
        "y = pd.Series(matches)\n",
        "x.corr(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOunXc1JO-0o"
      },
      "source": [
        "## Linear Discriminant Analysis\n",
        "\n",
        "We are going to divide the dataset into two groups: A positive group and a negative group, and then find out to what degree each word functions differently in those two groups.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "1bfT4lvbj3a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sklearn.feature_extraction.text.TfidfVectorizer"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# To perform the LDA calculation, we need a TF-IDF matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize.casual import casual_tokenize\n",
        "\n",
        "# We're keeping it simple for now, no special preprocessing.\n",
        "tfidfingest = TfidfVectorizer(tokenizer=casual_tokenize)\n",
        "type(tfidfingest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "2RopuuGmldTw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "((70, 1254), numpy.ndarray)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now we have the tool we need to create the TF-IDF matrix:\n",
        "tfidfmatrix = tfidfingest.fit_transform(raw_documents=data['Comments']).toarray()\n",
        "tfidfmatrix.shape, type(tfidfmatrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "DLlKcuznmfu1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.11255423, 0.        , 0.053985  , 0.        , 0.        ],\n",
              "       [0.        , 0.15665963, 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.22535615, 0.        , 0.0540444 , 0.        , 0.07541902],\n",
              "       [0.0478254 , 0.        , 0.20644883, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.15721287, 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remember that an array in numpy is a matrix-like object containing values all of the same type.\n",
        "# What's in our array?\n",
        "tfidfmatrix[:10,:5] # Just the first ten rows and five columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZkLSbvDDos0I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[True, True, True, False, True, True, True, False]\n",
            "[False, False, False, True, False, False, False, True]\n"
          ]
        }
      ],
      "source": [
        "# Typical sparse array: Mostly zeroes.\n",
        "# The non-zero values are fractional, showing that this is TF-IDF and not an array of simple counts.\n",
        "# Now let's subdivide our original dataset based on whether each row was classed as a positive or negative comment.\n",
        "# Here we are creating what Python programmers sometimes call a \"mask.\"\n",
        "# The first mask will let us select the positive cases. The second mask will let us select the negative cases.\n",
        "classlist = [(c==\"Positive\" or c==\"Neutral\") for c in data['Sentiment Label']] # Positive mask\n",
        "notclasslist = [not c for c in classlist] # Negative mask\n",
        "print(classlist[:8]) # Compare the first 8 results from the two masks\n",
        "print(notclasslist[:8]) # Entries in these two lists should be boolean inverses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "BFlH9wztplQm"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.05894595, -0.11399296,  0.01746025, ...,  0.00252529,\n",
              "        0.00154828,  0.01642816], shape=(1254,))"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can use the masks to select the cases we want and the mean method to summarize our tfidf for each subset of the data\n",
        "# Centroid means \"a point in space that is in the middle of a bunch of other points\"\n",
        "poscentroid = tfidfmatrix[classlist].mean(axis=0)\n",
        "negcentroid = tfidfmatrix[notclasslist].mean(axis=0)\n",
        "separator = poscentroid - negcentroid # The difference in means for each word\n",
        "separator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "SHcE2rk6wBTI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1254, 2), ['Token', 'Sentiment'])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now let's take our separator and put it together in a little dataset with the list of tokens, so we can see what's going on.\n",
        "wordsent = pd.DataFrame({\n",
        "    \"Token\":list(tfidfingest.get_feature_names_out()),\n",
        "    \"Sentiment\":list(separator)\n",
        "})\n",
        "# Here we sort the cases from lowest to highest, based on the value that we took from separator.\n",
        "wordsent.sort_values(by=['Sentiment'], inplace=True)\n",
        "wordsent.shape, list(wordsent) # Summarize our data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "-oolTGPU08xo"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Token</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"</td>\n",
              "      <td>-0.113993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1076</th>\n",
              "      <td>the</td>\n",
              "      <td>-0.050822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1214</th>\n",
              "      <td>which</td>\n",
              "      <td>-0.048907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>but</td>\n",
              "      <td>-0.047988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>948</th>\n",
              "      <td>she</td>\n",
              "      <td>-0.044887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>fish</td>\n",
              "      <td>-0.044311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>alright</td>\n",
              "      <td>-0.043009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>agedashi</td>\n",
              "      <td>-0.043009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1104</th>\n",
              "      <td>tofu</td>\n",
              "      <td>-0.043009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>few</td>\n",
              "      <td>-0.042864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>762</th>\n",
              "      <td>ordered</td>\n",
              "      <td>-0.042253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1168</th>\n",
              "      <td>visit</td>\n",
              "      <td>-0.038870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>684</th>\n",
              "      <td>most</td>\n",
              "      <td>-0.038832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1206</th>\n",
              "      <td>were</td>\n",
              "      <td>-0.036540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1219</th>\n",
              "      <td>will</td>\n",
              "      <td>-0.036013</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Token  Sentiment\n",
              "1            \"  -0.113993\n",
              "1076       the  -0.050822\n",
              "1214     which  -0.048907\n",
              "200        but  -0.047988\n",
              "948        she  -0.044887\n",
              "436       fish  -0.044311\n",
              "87     alright  -0.043009\n",
              "78    agedashi  -0.043009\n",
              "1104      tofu  -0.043009\n",
              "425        few  -0.042864\n",
              "762    ordered  -0.042253\n",
              "1168     visit  -0.038870\n",
              "684       most  -0.038832\n",
              "1206      were  -0.036540\n",
              "1219      will  -0.036013"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# After sorting, the \"most negative\" words are at the beginning of the data frame and the \"most positive\" words are at the end.\n",
        "wordsent.head(15) # Give us 15 words that are most negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "MM-7CNaP1VmQ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Token</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>best</td>\n",
              "      <td>0.021562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1074</th>\n",
              "      <td>that</td>\n",
              "      <td>0.021747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>delicious</td>\n",
              "      <td>0.021767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>806</th>\n",
              "      <td>place</td>\n",
              "      <td>0.022676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>come</td>\n",
              "      <td>0.022697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>can</td>\n",
              "      <td>0.024593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1101</th>\n",
              "      <td>to</td>\n",
              "      <td>0.024680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>761</th>\n",
              "      <td>order</td>\n",
              "      <td>0.025505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1033</th>\n",
              "      <td>sushi</td>\n",
              "      <td>0.027182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491</th>\n",
              "      <td>great</td>\n",
              "      <td>0.027710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>455</th>\n",
              "      <td>fresh</td>\n",
              "      <td>0.028815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>have</td>\n",
              "      <td>0.030804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>are</td>\n",
              "      <td>0.036204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>and</td>\n",
              "      <td>0.039881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>!</td>\n",
              "      <td>0.058946</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Token  Sentiment\n",
              "161        best   0.021562\n",
              "1074       that   0.021747\n",
              "312   delicious   0.021767\n",
              "806       place   0.022676\n",
              "252        come   0.022697\n",
              "209         can   0.024593\n",
              "1101         to   0.024680\n",
              "761       order   0.025505\n",
              "1033      sushi   0.027182\n",
              "491       great   0.027710\n",
              "455       fresh   0.028815\n",
              "510        have   0.030804\n",
              "111         are   0.036204\n",
              "100         and   0.039881\n",
              "0             !   0.058946"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wordsent.tail(15) # Give us 15 words that are most positive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8brSoputGKTP"
      },
      "source": [
        "Well, remember that this is a really small dataset and we did not make the effort to filter out stop words or punctuation. Still, there's some interesting stuff in there, and we've shown how with a few simple steps, we can create a kind of \"empirical\" sentiment value for each word.\n",
        "\n",
        "The next step also uses a cool matrix math trick. By creating *dot products* of the separator vector with the TF-IDF vector for each of our 70 comments, we can summarize the similarity/distance of a given comment from the dividing structure. Maybe the resulting document scores will be useful for something."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "6y78fidHG5Bk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.03190957,  0.00489625,  0.02053524, -0.1131945 ,  0.0068361 ,\n",
              "        0.03714363,  0.05276984, -0.1372704 ,  0.01527414,  0.04051255,\n",
              "       -0.13587406,  0.03995574,  0.00276345, -0.00473419,  0.03760517,\n",
              "        0.02651981,  0.06201844,  0.00034177, -0.01263898,  0.03052205,\n",
              "        0.02293302,  0.0197362 , -0.13294862,  0.02436511,  0.03068126,\n",
              "        0.01019862,  0.00424415,  0.01116213,  0.04171756,  0.03642798,\n",
              "        0.03830759,  0.0108454 ,  0.04254235,  0.00893868,  0.03582491,\n",
              "        0.03351333,  0.03417034,  0.03088434, -0.00089552,  0.04566911,\n",
              "        0.04567206,  0.05179615,  0.03851501,  0.04892801,  0.03642214,\n",
              "        0.02451584,  0.03657144,  0.03245421, -0.12116866,  0.01874153,\n",
              "        0.02458296, -0.00526663,  0.04728941,  0.0562507 ,  0.03587462,\n",
              "        0.04009349, -0.12562397,  0.0534958 ,  0.02060276,  0.02848552,\n",
              "        0.03458464,  0.04533432,  0.04616221,  0.01689287,  0.02849751,\n",
              "        0.0065034 ,  0.02493422, -0.01567791,  0.05086638,  0.02925724])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docusent = tfidfmatrix.dot(separator)\n",
        "docusent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "TwANGGz1HTG7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.618)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = pd.Series(data['Sentiment Score']) # Here's the sentiment score from the data\n",
        "y = pd.Series(docusent) # Here's our calculated separation of each document from the divider\n",
        "x.corr(y).round(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxICzoqWxglk"
      },
      "source": [
        "## Latent Dirichlet Allocation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "JGJAKB1X0ipv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "((70, 408), numpy.ndarray)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "counter = CountVectorizer(min_df=2, max_df=0.9,\n",
        "                          tokenizer=casual_tokenize, stop_words='english')\n",
        "countmatrix = counter.fit_transform(raw_documents=data['Comments']).toarray()\n",
        "countmatrix.shape, type(countmatrix) # Rows? Columns? Type?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Cs_WLe8K1OAG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((70, 408), pandas.core.frame.DataFrame)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# When we want to do calculations or transformations on a sparse matrix, we usually need to convert it to a data frame first.\n",
        "\n",
        "import pandas as pd\n",
        "countdf = pd.DataFrame(countmatrix, columns=counter.get_feature_names_out())\n",
        "countdf.shape, type(countdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "pGteB8nT58oa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>!</th>\n",
              "      <th>\"</th>\n",
              "      <th>$</th>\n",
              "      <th>%</th>\n",
              "      <th>&amp;</th>\n",
              "      <th>'</th>\n",
              "      <th>(</th>\n",
              "      <th>)</th>\n",
              "      <th>,</th>\n",
              "      <th>-</th>\n",
              "      <th>...</th>\n",
              "      <th>white</th>\n",
              "      <th>won't</th>\n",
              "      <th>work</th>\n",
              "      <th>worst</th>\n",
              "      <th>worth</th>\n",
              "      <th>wouldn't</th>\n",
              "      <th>wrong</th>\n",
              "      <th>years</th>\n",
              "      <th>you're</th>\n",
              "      <th>yummy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 408 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   !  \"  $  %  &  '  (  )  ,  -  ...  white  won't  work  worst  worth  \\\n",
              "0  3  0  1  0  0  0  4  4  4  4  ...      0      0     0      0      0   \n",
              "1  0  2  0  0  0  1  1  1  2  0  ...      0      0     0      0      0   \n",
              "2  0  0  0  0  0  0  0  0  0  0  ...      0      0     0      0      0   \n",
              "3  0  0  0  0  0  0  0  0  2  0  ...      0      0     0      0      0   \n",
              "4  0  0  0  0  0  0  0  0  4  1  ...      0      0     0      0      0   \n",
              "\n",
              "   wouldn't  wrong  years  you're  yummy  \n",
              "0         0      1      0       0      0  \n",
              "1         0      0      0       0      0  \n",
              "2         0      0      0       0      0  \n",
              "3         0      0      0       0      0  \n",
              "4         0      0      0       0      1  \n",
              "\n",
              "[5 rows x 408 columns]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 4.14: Take a look at the first few rows of the data frame using the head() method.\n",
        "# Solution\n",
        "countdf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "9NMv4rqb6qPV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9, 408)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sci-Kit learn has a Latent Dirichlet Allocation fitting algorithm that we can use.\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDIA\n",
        "from numpy import random as rnd\n",
        "rnd.seed(123)\n",
        "\n",
        "numtopics = 9 # Later on you will change this as an exercise.\n",
        "\n",
        "# Create an instance of the LDIA analyzer\n",
        "ldiamodel = LDIA(n_components=numtopics, learning_method='batch')\n",
        "\n",
        "# Provide the pandas data frame of word counts for it to work on\n",
        "ldiamodel = ldiamodel.fit(countdf)\n",
        "\n",
        "ldiamodel.components_.shape # What is the resulting data frame like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "E7SdZyuQtAfQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 1.11107766,  4.11114844,  1.11109594,  0.11111111,  0.11111111],\n",
              "       [ 0.11125124,  0.11111111,  0.97980161,  0.11111111,  1.11112435],\n",
              "       [ 0.11111899,  2.11111287,  2.11111606,  0.11111111,  0.11111111],\n",
              "       [ 9.89858297,  0.11111111,  1.60645729,  0.11111111,  0.87929276],\n",
              "       [ 0.11111112,  0.11111112,  0.11111112,  0.11111112,  0.11111112],\n",
              "       [ 7.64300236, 20.11106664,  7.44997875,  1.11111118,  0.11111111],\n",
              "       [42.79141612,  0.11111648,  2.40814966,  1.11111101,  1.34291621],\n",
              "       [ 3.11128055,  0.11111112,  0.11111112,  0.11111112,  0.11111112],\n",
              "       [ 2.11115898,  0.11111111,  2.11117846,  0.11111112,  0.11111111]])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Each row repesents one topic, and each entry in a row pertains to one of the terms in our count matrix.\n",
        "# Let's examine just the first five entries in each row.\n",
        "# Remember that these are pseudocounts and not probability values.\n",
        "ldiamodel.components_[:numtopics,:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "x989zdlOqjxN"
      },
      "outputs": [],
      "source": [
        "# Here's a function for showing the top ten words for each topic.\n",
        "# This was adapted from an example in the SciKit Learn documentation.\n",
        "# The model is the fitted model, the vectorizer is the initalized instance of the count vectorizer, and\n",
        "# top_words sets how many of the most influential words you want to see.\n",
        "def selected_topics(model, vectorizer, top_words=10):\n",
        "    for idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic %d:\" % (idx)) # One of these headers for each topic\n",
        "\n",
        "        # This uses a list comprehension to iterate over the words\n",
        "        # in each topic, picking out the highest coefficient values.\n",
        "        print([(vectorizer.get_feature_names_out()[i], topic[i])\n",
        "                        for i in topic.argsort()[:-top_words - 1:-1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "iBYH9_9Kq1-v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic 0:\n",
            "[(',', np.float64(15.111341076976649)), ('sushi', np.float64(12.111324065910685)), (\"it's\", np.float64(6.111247105085878)), ('food', np.float64(5.111233201946385)), ('good', np.float64(5.111230365526831)), ('place', np.float64(4.111215047432107)), ('japanese', np.float64(4.111196331523579)), (\"don't\", np.float64(4.111171989763613)), ('\"', np.float64(4.111148437552365)), ('little', np.float64(3.111220809173756))]\n",
            "Topic 1:\n",
            "[('sushi', np.float64(7.725661982034618)), ('order', np.float64(6.08071357385364)), (',', np.float64(4.972983349706629)), ('-', np.float64(4.111152733351147)), ('roll', np.float64(3.98315351449739)), (')', np.float64(3.8119383014187473)), ('(', np.float64(3.805807435608722)), ('dinner', np.float64(3.089288756356988)), ('belt', np.float64(3.022295193858417)), ('bar', np.float64(2.9186287763305208))]\n",
            "Topic 2:\n",
            "[('sushi', np.float64(9.76940533760243)), (',', np.float64(6.328024454704488)), ('fresh', np.float64(5.111216601182591)), ('food', np.float64(4.291097283459323)), ('jose', np.float64(4.11122810775634)), ('san', np.float64(4.1112203220390215)), ('just', np.float64(4.111085522000185)), ('time', np.float64(4.1110186353743625)), ('delicious', np.float64(4.110982472504919)), ('lunch', np.float64(3.489092790836181))]\n",
            "Topic 3:\n",
            "[(',', np.float64(23.01172710847149)), ('sushi', np.float64(14.669325698676124)), ('!', np.float64(9.898582968732633)), ('food', np.float64(5.710130850278592)), ('place', np.float64(4.693161253433213)), ('maru', np.float64(4.465909752339851)), ('lunch', np.float64(4.370379479387548)), ('ordered', np.float64(3.9846298709770553)), ('service', np.float64(3.834928738248808)), ('decent', np.float64(3.6656393869082864))]\n",
            "Topic 4:\n",
            "[('beef', np.float64(0.11111114754318115)), ('restaurants', np.float64(0.11111113765995871)), ('husband', np.float64(0.11111113334323532)), ('teriyaki', np.float64(0.11111113187644751)), ('area', np.float64(0.11111113146072446)), ('reasonable', np.float64(0.11111113118890495)), ('tai', np.float64(0.11111113091150182)), ('little', np.float64(0.11111113083222687)), ('w', np.float64(0.11111113053758505)), ('eat', np.float64(0.11111113038297334))]\n",
            "Topic 5:\n",
            "[(',', np.float64(116.21548303789446)), ('sushi', np.float64(39.51141284908605)), ('\"', np.float64(20.11106664091215)), (')', np.float64(19.641678636470726)), ('good', np.float64(18.79483717970957)), ('belt', np.float64(17.892976804301416)), ('(', np.float64(17.518610518019322)), ('salmon', np.float64(14.111098953973107)), (\"it's\", np.float64(11.97131301965214)), ('fish', np.float64(10.574241384421823))]\n",
            "Topic 6:\n",
            "[('!', np.float64(42.7914161246888)), (',', np.float64(42.02695376178438)), ('sushi', np.float64(25.879497015987596)), ('bar', np.float64(14.314617991566992)), ('great', np.float64(11.31813672788745)), (\"it's\", np.float64(10.250838417437086)), ('delicious', np.float64(10.110872576408935)), ('time', np.float64(9.467316535866514)), ('nice', np.float64(9.111108853950084)), ('love', np.float64(9.110914571799034))]\n",
            "Topic 7:\n",
            "[('!', np.float64(3.111280548726866)), ('tai', np.float64(1.1112210558920412)), ('love', np.float64(1.1111970410938607)), ('definitely', np.float64(1.1111719493282501)), ('come', np.float64(1.111165848278718)), ('nanook', np.float64(1.1111613632900368)), ('say', np.float64(1.1111597109970865)), ('good', np.float64(1.1111579633696658)), ('...', np.float64(1.1111508781183606)), ('took', np.float64(1.111149972332459))]\n",
            "Topic 8:\n",
            "[(',', np.float64(7.111408635367725)), ('rolls', np.float64(3.1112080974970975)), ('sushi', np.float64(3.111150807178332)), ('got', np.float64(2.111256788793555)), ('place', np.float64(2.111223267320671)), ('friendly', np.float64(2.11119953249811)), ('chicken', np.float64(2.11118343025074)), ('good', np.float64(2.111182912857618)), ('$', np.float64(2.1111784560980777)), ('!', np.float64(2.111158976874018))]\n"
          ]
        }
      ],
      "source": [
        "# Here we call our function\n",
        "selected_topics(ldiamodel, counter, top_words=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wATkk-QyHSNs"
      },
      "source": [
        "When we look at the output above, we see that if two words in a topic both have high values, it means they often appear together in the same documents. If we ignore punctuation and just look at the words, we can notice natural connections. For example, even if someone doesn’t know that sushi is a type of food, they could guess it, because sushi and food show up together in several topics.\n",
        "\n",
        "Earlier, when we used **linear discriminant analysis with TF-IDF**, we only got a very broad sense of sentiment (positive vs. negative) by checking which words were important in different groups of text.\n",
        "\n",
        "But now, this method gives us a richer picture: it shows which words usually appear together, and from that, it builds groups of related words (topics).\n",
        "\n",
        "This is based on the **distributional hypothesis:** words that are related in meaning tend to appear near each other in text.\n",
        "\n",
        "However, the connections between words and topics are just one part of the story. We also need to know the connections between topics and documents. In other words, we want to see which topics show up in which comments.\n",
        "\n",
        "Scikit-learn can give us this information using the **.transform()** method on our **document-term matrix**.\n",
        "\n",
        "**The result will be:**\n",
        "The same number of rows as comments (because each row = one document, and the same number of columns as topics (because each column = one topic).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "mbOnIE72zpVs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LatentDirichletAllocation was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "((70, 9), numpy.ndarray)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use transform() to get the topic vectors.\n",
        "# We should have one vector for each document in our sushi data.\n",
        "topicvectors = ldiamodel.transform(X=countmatrix)\n",
        "topicvectors.shape, type(topicvectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "XqxXxL6g1WYy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0015 0.8206 0.0015 0.0015 0.0015 0.0015 0.1691 0.0015 0.0015]\n",
            " [0.0022 0.0022 0.0022 0.0022 0.0022 0.6663 0.3185 0.0022 0.0022]\n",
            " [0.0093 0.0093 0.0093 0.0093 0.0093 0.0093 0.9259 0.0093 0.0093]\n",
            " [0.0026 0.0026 0.0026 0.9793 0.0026 0.0026 0.0026 0.0026 0.0026]\n",
            " [0.0021 0.0021 0.0021 0.0021 0.0021 0.9832 0.0021 0.0021 0.0021]\n",
            " [0.0016 0.0016 0.0016 0.0016 0.0016 0.0016 0.9871 0.0016 0.0016]\n",
            " [0.0018 0.0018 0.0018 0.2545 0.0018 0.7327 0.0018 0.0018 0.0018]\n",
            " [0.0058 0.0058 0.0059 0.0059 0.0058 0.9532 0.0059 0.0058 0.0059]\n",
            " [0.0018 0.0018 0.0018 0.0018 0.0018 0.9859 0.0018 0.0018 0.0018]\n",
            " [0.0015 0.0015 0.0015 0.0015 0.0015 0.9878 0.0015 0.0015 0.0015]]\n"
          ]
        }
      ],
      "source": [
        "# Let's see the coefficients for the first 10 rows in our sushi data\n",
        "print(topicvectors[0:10,:].round(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qok3oExtwDT-"
      },
      "source": [
        "Make sure to scan each row in the output above. Generally, each document will load most heavily on just one of the modeled topics. So in each row there should be a lot of very small numbers and one larger number in the range of 0.80 up to 0.99. If you see a row that has two or more \"mid-sized\" numbers, it means that the document in question could not be easily \"assigned\" to just one topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "lp3H_LdrJ781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0)]\n"
          ]
        }
      ],
      "source": [
        "# Each of those rows is a probability vector and therefore each row should sum to 1 (within rounding error).\n",
        "# As probabilities they represent the strength of \"attachment\" for each of our topics to each of our documents.\n",
        "# Just as a diagnostic, let's check this assumption.\n",
        "print( [ round(tv.sum(),1) for tv in topicvectors] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DONSzzZ4wyJM"
      },
      "source": [
        "We have topic vectors = Numbers that show how much each document belongs to each topic.\n",
        "\n",
        "Some topics are probably linked to sentiment (positive or negative feeling).\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Topic with words like good, sushi, friendly, service → likely positive.\n",
        "\n",
        "Topic with words like bad, slow, rude → likely negative.\n",
        "\n",
        "Now, to test this idea:\n",
        "\n",
        "We use a simple method called **ordinary least squares (OLS) regression.**\n",
        "\n",
        "This method tries to see:\n",
        "“If I know the topic values for a document, can I predict its sentiment score?”\n",
        "\n",
        "In very simple words:\n",
        "We are checking whether certain topics (word groups) are good at predicting whether a review is positive or negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "iOmhHyzy2Rhc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.5683062551922707)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We will use OLS regression from sklearn\n",
        "from sklearn import linear_model\n",
        "reg = linear_model.LinearRegression() # Create an instance of the class\n",
        "reg.fit(topicvectors[:,0:numtopics], data['Sentiment Score'])\n",
        "sentpred = reg.predict(topicvectors[:,0:numtopics])\n",
        "x = pd.Series(data['Sentiment Score'])\n",
        "y = pd.Series(sentpred)\n",
        "x.corr(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "GAqxzVDF33rg"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.08680103, -0.0502043 , -0.03314594,  0.00835262,  0.43663825,\n",
              "       -0.22528869,  0.0213544 ,  0.01771591, -0.08862122])"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reg.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "DRIGWHKJNNHz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document Index: 3, Probability: 0.98\n",
            "Comment: Small sushi boat restaurant located in SJ Japantown. Sushi Maru looks a bit outdated but their food is pretty decent. I ordered a bowl of agedashi tofu and my mom ordered their chicken terriyaki lunch. The agedashi tofu was decent, definitely could be slightly crispier but the flavors were alright. We also grabbed a few plates off the sushi belt and they were decent but they were a tad pricey for okay quality.  Overall this place is alright. It's not high on my list to revisit but if i'm in the area, i'll probably try it out again.\n",
            "Document Index: 23, Probability: 0.96\n",
            "Comment: So yummy here, we where here on Christmas Eve. And we wanted sushi, thank good it was open that day. Service was great, food was excellent and the employees where up to beat. I can't wait to eat here again. The bento  box was enough food for us to share.. we also got some sushi and oysters at the bar to keep us full. Hot tea is always a bonus..\n",
            "Document Index: 43, Probability: 0.98\n",
            "Comment: I seriously think about this place A LOT! My go to things to order here: halibut fin, red snapper, and nanook....amazing.  The service can be slow at times, but they're busy, but if you're okay with my loud little nephew, you're amazing to me.  Plus the food, I can't get enough of...remember to get their stamp card. You now know where to spot me!!\n",
            "Document Index: 46, Probability: 0.95\n",
            "Comment: My kids love this place. They can each eat their own bowl of udon, couple plates of sushi and edamame. And they'll sit through the meal without asking for my phone so I can actually enjoy my kastsu donburi.\n",
            "Document Index: 58, Probability: 0.97\n",
            "Comment: I was on a shopping trip to SJ and fell into Sushi Maru. We were headed to Gombei Japanese Restaurant, when we arrived at Gombei we found out that they only took cash. We walked to the bank to get cash and  the atm was broken. Next to the bank with the broken atm was Sushi Maru. Bam! Divine intervention. We had a wonderful dinner. The service was very fast and friendly. We both had the dinner combo, the bento box, and we both enjoyed it. Dinner for two was under $40. A happy accident that I would be happy to repeat.\n",
            "Document Index: 62, Probability: 0.94\n",
            "Comment: We have been to many Sushi restaurants and this one truly is among  the best. There pricing is lower than most and the qualiy at the top. Friendly, clean, fast, and tasty!\n",
            "Document Index: 67, Probability: 0.90\n",
            "Comment: got the beef teriyaki lunch. not bad. the sushi boat needs some work\n"
          ]
        }
      ],
      "source": [
        "# Question 4.15:\n",
        "# Use an enumerator to find the documents where Topic 4 represented a probability value larger than 0.90.\n",
        "# Print out the comment corresponding to that document\n",
        "\n",
        "# Solution\n",
        "for idx, tv in enumerate(topicvectors): #start of code, now continue from here.\n",
        "    if tv[3] > 0.90:\n",
        "        print(f\"Document Index: {idx}, Probability: {tv[3]:.2f}\")\n",
        "        print(f\"Comment: {data['Comments'][idx]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "JhKD-1z9VOX5"
      },
      "outputs": [],
      "source": [
        "# Question 4.16:\n",
        "# Include an if statement in this for loop to test the probability value for Topic 4 in this particular row\n",
        "\n",
        "# Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "9rXzHh1Z5b_1"
      },
      "outputs": [],
      "source": [
        "# Question 4.17: Inside the if statement, use idx as the row slicer to get the 'Comment' field from the original data set. Print that comment.\n",
        "\n",
        "# Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "U8ri4_w4O6Dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10, 408)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 4.18:\n",
        "# Go back to where numtopics is set and change to the next highest integer.\n",
        "# Rerun the code from there down to here.\n",
        "# Interpret the results.\n",
        "# Is this new number of topics a better choice? Why or why not?\n",
        "\n",
        "# Solution\n",
        "\n",
        "\n",
        "numtopics = 10 # Later on you will change this as an exercise.\n",
        "\n",
        "# Create an instance of the LDIA analyzer\n",
        "ldiamodel = LDIA(n_components=numtopics, learning_method='batch')\n",
        "\n",
        "# Provide the pandas data frame of word counts for it to work on\n",
        "ldiamodel = ldiamodel.fit(countdf)\n",
        "\n",
        "ldiamodel.components_.shape # What is the resulting data frame like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic 0:\n",
            "[('booth', np.float64(6.069795908289698)), ('sushi', np.float64(5.585392628658739)), ('bar', np.float64(5.198959810083399)), ('order', np.float64(5.100044108618672)), ('?', np.float64(5.079942470561786)), ('place', np.float64(4.786174667567146)), ('food', np.float64(4.100053657341722)), ('!', np.float64(3.1514869272308874)), ('fresh', np.float64(2.8245233462386476)), ('conveyor', np.float64(2.7649630264575733))]\n",
            "Topic 1:\n",
            "[('food', np.float64(10.623078433424167)), (',', np.float64(8.888997209058118)), ('!', np.float64(8.100012597324007)), ('sushi', np.float64(7.100032051512819)), ('lunch', np.float64(5.146829915436471)), ('time', np.float64(4.796485284954928)), ('good', np.float64(4.1000205688136795)), ('japanese', np.float64(4.100018301814213)), ('come', np.float64(4.100017104106963)), ('tasted', np.float64(4.055866774902506))]\n",
            "Topic 2:\n",
            "[(',', np.float64(10.100052163793213)), ('sushi', np.float64(8.100059498388168)), ('!', np.float64(6.100015537521612)), ('roll', np.float64(5.1000269999260075)), ('(', np.float64(4.10002700847618)), (')', np.float64(4.10002678006835)), ('decent', np.float64(4.100024661579459)), ('-', np.float64(4.100019938437083)), ('great', np.float64(3.1000319186317555)), ('super', np.float64(3.1000235643990885))]\n",
            "Topic 3:\n",
            "[('truly', np.float64(4.099998251555714)), ('restaurant', np.float64(3.520420393345963)), ('$', np.float64(3.203284782121842)), ('clean', np.float64(2.9020315010737123)), (',', np.float64(2.308238575961172)), ('parking', np.float64(2.1000365367239344)), ('san', np.float64(2.1000316905716514)), ('\"', np.float64(2.1000018552088755)), ('fills', np.float64(2.099993151414548)), ('week', np.float64(2.099992494321093))]\n",
            "Topic 4:\n",
            "[(',', np.float64(187.9585802968509)), ('sushi', np.float64(83.69764956510683)), ('!', np.float64(38.14061651322379)), ('belt', np.float64(29.703641338686744)), (')', np.float64(29.037314513618004)), ('(', np.float64(27.032498121168672)), ('good', np.float64(26.029776367657472)), ('salmon', np.float64(21.0999752558359)), (\"it's\", np.float64(20.0999497350734)), ('place', np.float64(19.194025248038365))]\n",
            "Topic 5:\n",
            "[('!', np.float64(7.007823981613447)), ('took', np.float64(3.0958035452225308)), ('busy', np.float64(3.0958030826761576)), ('wait', np.float64(3.089916260137306)), ('bar', np.float64(2.9791538935380713)), ('seat', np.float64(2.0992076289105923)), ('mom', np.float64(2.0991919884541135)), ('night', np.float64(2.097962145094367)), ('day', np.float64(2.0954666763285124)), ('came', np.float64(2.087894561066636))]\n",
            "Topic 6:\n",
            "[('portion', np.float64(0.10000000711087367)), ('tai', np.float64(0.10000000539797722)), ('soba', np.float64(0.10000000508489655)), ('yummy', np.float64(0.10000000487701076)), ('quick', np.float64(0.10000000441297283)), ('phone', np.float64(0.10000000433486411)), ('reasonable', np.float64(0.10000000360585967)), ('shopping', np.float64(0.10000000354927385)), ('directly', np.float64(0.10000000349326688)), ('tasty', np.float64(0.10000000327719888))]\n",
            "Topic 7:\n",
            "[('portion', np.float64(0.10000000711087367)), ('tai', np.float64(0.10000000539797722)), ('soba', np.float64(0.10000000508489655)), ('yummy', np.float64(0.10000000487701076)), ('quick', np.float64(0.10000000441297283)), ('phone', np.float64(0.10000000433486411)), ('reasonable', np.float64(0.10000000360585967)), ('shopping', np.float64(0.10000000354927385)), ('directly', np.float64(0.10000000349326688)), ('tasty', np.float64(0.10000000327719888))]\n",
            "Topic 8:\n",
            "[('\"', np.float64(12.100123623666223)), (\"it's\", np.float64(4.1000672759499945)), ('sushi', np.float64(4.1000594012538585)), (',', np.float64(4.100044631800347)), ('service', np.float64(2.100042443322679)), ('got', np.float64(2.1000295419514052)), ('bad', np.float64(2.1000032940456976)), ('soba', np.float64(1.1000434546304187)), ('beef', np.float64(1.1000177385908922)), ('price', np.float64(1.1000154842409615))]\n",
            "Topic 9:\n",
            "[('service', np.float64(4.100092318670543)), ('!', np.float64(4.100038605695312)), ('place', np.float64(3.1000551624751504)), ('delicious', np.float64(3.100049333314144)), ('restaurant', np.float64(2.1000282113306468)), ('quality', np.float64(2.1000196516625027)), ('cheap', np.float64(2.100014479582267)), ('friday', np.float64(2.0999999995413443)), ('new', np.float64(1.1000196166532312)), ('moved', np.float64(1.100019583876777))]\n",
            "[[0.0013 0.0013 0.9882 0.0013 0.0013 0.0013 0.0013 0.0013 0.0013 0.0013]\n",
            " [0.002  0.002  0.002  0.9824 0.002  0.002  0.002  0.002  0.002  0.002 ]\n",
            " [0.0083 0.925  0.0083 0.0083 0.0083 0.0083 0.0083 0.0083 0.0083 0.0083]\n",
            " [0.0023 0.0023 0.9791 0.0023 0.0023 0.0023 0.0023 0.0023 0.0023 0.0023]\n",
            " [0.0019 0.0019 0.0019 0.0019 0.983  0.0019 0.0019 0.0019 0.0019 0.0019]\n",
            " [0.0014 0.0014 0.0014 0.0014 0.987  0.0014 0.0014 0.0014 0.0014 0.0014]\n",
            " [0.0016 0.0016 0.0016 0.1616 0.8253 0.0016 0.0016 0.0016 0.0016 0.0016]\n",
            " [0.0053 0.0053 0.0053 0.0053 0.0053 0.0053 0.0053 0.0053 0.9526 0.0053]\n",
            " [0.0016 0.0016 0.0016 0.0016 0.9857 0.0016 0.0016 0.0016 0.0016 0.0016]\n",
            " [0.0014 0.0014 0.0014 0.0014 0.9877 0.0014 0.0014 0.0014 0.0014 0.0014]]\n",
            "[np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0)]\n",
            "Document Index: 1, Probability: 0.98\n",
            "Comment: I am pleased to recommend this restaurant in San Jose's \"Japantown\"right next door to Union Bank. The restaurant is quite popular and fills up at lunchtime.  The service is fast and friendly.   There is a sushi-boat bar for those who are so inclined as it is definitely well-liked.  The booths are spacious and comfortable.  The  luncheon menu  is somewhat limited. Their tempura appetizer is really remarkable, both in taste and price, ample and affordable.  I was slightly disappointed in their salad dressing.  It's usually so good in Japanese restaurants. The restaurant is clean as is the ladies' room. Parking is not a problem. There is both street (metered) and lot parking.\n",
            "Document Index: 30, Probability: 0.97\n",
            "Comment: Sushi Maru San Jose is truly a Hidden Gem the place is always clean and neat all of the staff make you feel welcome and the food is truly consistent every time we haven't eaten there. The dinners are about $15 each with drinks and hors d'oeuvres about $22 per person for dinner and you truly walk away full and very satisfied.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LatentDirichletAllocation was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "ldiamodel.components_[:numtopics,:5]\n",
        "selected_topics(ldiamodel, counter, top_words=10)\n",
        "topicvectors = ldiamodel.transform(X=countmatrix)\n",
        "topicvectors.shape, type(topicvectors)\n",
        "print(topicvectors[0:10,:].round(4))\n",
        "print( [ round(tv.sum(),1) for tv in topicvectors] )\n",
        "# We will use OLS regression from sklearn\n",
        "from sklearn import linear_model\n",
        "reg = linear_model.LinearRegression() # Create an instance of the class\n",
        "reg.fit(topicvectors[:,0:numtopics], data['Sentiment Score'])\n",
        "sentpred = reg.predict(topicvectors[:,0:numtopics])\n",
        "x = pd.Series(data['Sentiment Score'])\n",
        "y = pd.Series(sentpred)\n",
        "x.corr(y)\n",
        "reg.coef_\n",
        "for idx, tv in enumerate(topicvectors): #start of code, now continue from here.\n",
        "    if tv[3] > 0.90:\n",
        "        print(f\"Document Index: {idx}, Probability: {tv[3]:.2f}\")\n",
        "        print(f\"Comment: {data['Comments'][idx]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "EPRTm7okylHN"
      },
      "outputs": [],
      "source": [
        "# Question 4.19:\n",
        "# Experimentation with the number of topics, take two.\n",
        "# Go back to where numtopics is set and change it to a small value such as three or five.\n",
        "# Rerun the code from there down to here.\n",
        "# Interpret the results.\n",
        "# Is this new number of topics a better choice? Why or why not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5, 408)"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "numtopics = 5 # Later on you will change this as an exercise.\n",
        "\n",
        "# Create an instance of the LDIA analyzer\n",
        "ldiamodel = LDIA(n_components=numtopics, learning_method='batch')\n",
        "\n",
        "# Provide the pandas data frame of word counts for it to work on\n",
        "ldiamodel = ldiamodel.fit(countdf)\n",
        "\n",
        "ldiamodel.components_.shape # What is the resulting data frame like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic 0:\n",
            "[(',', np.float64(154.97416456881646)), ('sushi', np.float64(78.37967938851985)), ('!', np.float64(48.25875600409821)), ('bar', np.float64(27.879241928803555)), ('belt', np.float64(24.84412187791781)), ('\"', np.float64(24.19860129876642)), ('(', np.float64(23.836502592191746)), (')', np.float64(23.80827889429326)), ('good', np.float64(23.10711837208723)), ('food', np.float64(21.157574656157873))]\n",
            "Topic 1:\n",
            "[(',', np.float64(5.34207558762554)), ('good', np.float64(4.415514824103289)), ('best', np.float64(3.374778236018628)), ('service', np.float64(3.3288777786246126)), ('!', np.float64(3.203888978276958)), ('reasonable', np.float64(3.2018003518984433)), ('sushi', np.float64(2.2231607485961886)), ('chicken', np.float64(2.204920381863299)), ('teriyaki', np.float64(2.2040666104785402)), ('like', np.float64(2.2034835779370967))]\n",
            "Topic 2:\n",
            "[(',', np.float64(14.614650594374936)), ('sushi', np.float64(11.849932100927173)), ('place', np.float64(7.375191984702232)), ('come', np.float64(7.12227792165359)), ('order', np.float64(6.218778860650829)), (\"you're\", np.float64(5.7613377708332)), ('belt', np.float64(5.285438360355171)), ('food', np.float64(5.213587105889274)), ('...', np.float64(4.2051550594896)), ('end', np.float64(4.195008083360673))]\n",
            "Topic 3:\n",
            "[(',', np.float64(19.280218566188523)), ('sushi', np.float64(16.734010523621645)), ('lunch', np.float64(10.740983489901547)), ('!', np.float64(9.725311732079735)), (\"it's\", np.float64(6.8354910476805815)), ('food', np.float64(6.198094166460651)), ('good', np.float64(5.200744730409578)), ('really', np.float64(4.517939840444256)), ('maru', np.float64(4.460148635763233)), ('definitely', np.float64(4.1957061821365))]\n",
            "Topic 4:\n",
            "[(',', np.float64(21.788890682993845)), (')', np.float64(8.311344684600812)), ('(', np.float64(6.282901831284407)), ('place', np.float64(5.203464661595841)), ('rolls', np.float64(4.255540136600816)), ('food', np.float64(4.230562126468557)), (\"it's\", np.float64(4.201866898463793)), ('lunch', np.float64(4.201803256402918)), ('truly', np.float64(4.199373678237539)), ('sushi', np.float64(3.8132172383344627))]\n",
            "[[0.9893 0.0027 0.0027 0.0027 0.0027]\n",
            " [0.984  0.004  0.004  0.004  0.004 ]\n",
            " [0.0171 0.0168 0.0169 0.017  0.9322]\n",
            " [0.0048 0.0047 0.0047 0.9811 0.0047]\n",
            " [0.0039 0.0039 0.7298 0.2586 0.0038]\n",
            " [0.8427 0.0029 0.003  0.1484 0.003 ]\n",
            " [0.0033 0.0033 0.4467 0.5433 0.0033]\n",
            " [0.9568 0.0106 0.011  0.0106 0.011 ]\n",
            " [0.9871 0.0032 0.0033 0.0032 0.0032]\n",
            " [0.9889 0.0028 0.0028 0.0028 0.0028]]\n",
            "[np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0)]\n",
            "Document Index: 3, Probability: 0.98\n",
            "Comment: Small sushi boat restaurant located in SJ Japantown. Sushi Maru looks a bit outdated but their food is pretty decent. I ordered a bowl of agedashi tofu and my mom ordered their chicken terriyaki lunch. The agedashi tofu was decent, definitely could be slightly crispier but the flavors were alright. We also grabbed a few plates off the sushi belt and they were decent but they were a tad pricey for okay quality.  Overall this place is alright. It's not high on my list to revisit but if i'm in the area, i'll probably try it out again.\n",
            "Document Index: 26, Probability: 0.96\n",
            "Comment: Came here for lunch on a Friday and the line was out the door. It moved quickly though. We both had the salmon teriyaki bento box and it was DELICIOUS! We also had some rolls from the sushi boats that were equally as delicious! Loved it and will most definitely be back.\n",
            "Document Index: 32, Probability: 0.96\n",
            "Comment: I go because it's close to my home. I know what to order right in Japanese. Price is a lot higher than sushi in Japan. But among all other sushi restaurants in San Jose Japantown area, this is the best one.\n",
            "Document Index: 40, Probability: 0.96\n",
            "Comment: my husband and I come here at least once a week! food their food and reasonable price. I always order their lunch combo it's a great deal! sometimes it's hard to find parking here though.\n",
            "Document Index: 49, Probability: 0.96\n",
            "Comment: Fresh sushi and sashimi. Check out the daily lunch special, sometimes it's pretty unique (not regular items on the menu). Atmosphere is friendly, there could be a short line during lunch hours. Finding a parking space nearby occasionally can be a challenge.\n",
            "Document Index: 56, Probability: 0.96\n",
            "Comment: This location is definitely on the decline. We stopped by yesterday for lunch. My husband and I ordered the bento box, and we both picked salmon and tuna sashimi. The fish tasted old, but the worst part was we got food poisoning from it. Will never visit again.\n",
            "Document Index: 60, Probability: 0.97\n",
            "Comment: Showed up and there was no table my kid could get a high chair for without obstructing the walk ways. The hostess, who also busses tables and serves, kindly asked if a table of 3 dudes drinking wouldn't mind moving to another table. They agreed and we were seated. Super accommodating. The food was pretty dang good. I walked out feeling %100 satisfied and thankful I live walking distance. Only thing I have to say is TURN YOUR HEATER OFF!!! It was so freakin hot in there. 5 star none the less.\n",
            "Document Index: 67, Probability: 0.91\n",
            "Comment: got the beef teriyaki lunch. not bad. the sushi boat needs some work\n",
            "Document Index: 69, Probability: 0.99\n",
            "Comment: Sushi Maru in Jtown is top notch value. I would say arguably the best value sushi in San Jose. Now, don't get me wrong when I say value, the fish here is actually quite fresh and good. Their daily lunch specials are always a really good deal, usually coming with some kind of dashi based soup, a mini chirashi bowl or tuna-don, and some other meat entree. Their sushi and sashimi specials are also really quite good for how cheap they are (and beautiful presentation too!). They don't take reservations here, but if you're a regular, you can get a discount card where they give you a $15 gift card every few visits. It's a really solid place to have as your \"go-to\" sushi place in SJ.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LatentDirichletAllocation was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "ldiamodel.components_[:numtopics,:5]\n",
        "selected_topics(ldiamodel, counter, top_words=10)\n",
        "topicvectors = ldiamodel.transform(X=countmatrix)\n",
        "topicvectors.shape, type(topicvectors)\n",
        "print(topicvectors[0:10,:].round(4))\n",
        "print( [ round(tv.sum(),1) for tv in topicvectors] )\n",
        "# We will use OLS regression from sklearn\n",
        "from sklearn import linear_model\n",
        "reg = linear_model.LinearRegression() # Create an instance of the class\n",
        "reg.fit(topicvectors[:,0:numtopics], data['Sentiment Score'])\n",
        "sentpred = reg.predict(topicvectors[:,0:numtopics])\n",
        "x = pd.Series(data['Sentiment Score'])\n",
        "y = pd.Series(sentpred)\n",
        "x.corr(y)\n",
        "reg.coef_\n",
        "for idx, tv in enumerate(topicvectors): #start of code, now continue from here.\n",
        "    if tv[3] > 0.90:\n",
        "        print(f\"Document Index: {idx}, Probability: {tv[3]:.2f}\")\n",
        "        print(f\"Comment: {data['Comments'][idx]}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

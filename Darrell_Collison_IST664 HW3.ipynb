{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBy_JaSAGpux"
      },
      "source": [
        "# IST664 - Homework 3\n",
        "\n",
        "Originality assertion: All of the text and comments in this file are my original work (except for template items written by the instructor). All of the code in this file is my work, except where I give credit to another source. By adding my name below, I affirm this originality assertion.\n",
        "\n",
        "*** My name: _Darrell_Collison_ ***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBdgCgdcRNrq"
      },
      "source": [
        "**Task 1: Read Sentence Pairs from Github**\n",
        "\n",
        "It is a weird acronym, but the \"Sentences Involving Compositional Knowledge\" (SICK) dataset includes a large number of sentence pairs with various levels of similarity. Here's a link to a web page with more information about the data:\n",
        "https://marcobaroni.org/composes/sick.html\n",
        "\n",
        "When you review the data, you will notice a field where the options are ENTAILMENT, CONTRADICTION, or NEUTRAL. Textual entailment is when one sentence logically follows based on the meaning of another sentence. You can read more about textual entailment here: https://en.wikipedia.org/wiki/Textual_entailment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wtW_IYsqRMGQ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pair_ID</th>\n",
              "      <th>sentence_A</th>\n",
              "      <th>sentence_B</th>\n",
              "      <th>relatedness_score</th>\n",
              "      <th>entailment_judgment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>A group of kids is playing in a yard and an ol...</td>\n",
              "      <td>A group of boys in a yard is playing and a man...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>NEUTRAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>A group of children is playing in the house an...</td>\n",
              "      <td>A group of kids is playing in a yard and an ol...</td>\n",
              "      <td>3.2</td>\n",
              "      <td>NEUTRAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>The young boys are playing outdoors and the ma...</td>\n",
              "      <td>The kids are playing outdoors near a man with ...</td>\n",
              "      <td>4.7</td>\n",
              "      <td>ENTAILMENT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>The kids are playing outdoors near a man with ...</td>\n",
              "      <td>A group of kids is playing in a yard and an ol...</td>\n",
              "      <td>3.4</td>\n",
              "      <td>NEUTRAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>The young boys are playing outdoors and the ma...</td>\n",
              "      <td>A group of kids is playing in a yard and an ol...</td>\n",
              "      <td>3.7</td>\n",
              "      <td>NEUTRAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pair_ID                                         sentence_A  \\\n",
              "0        1  A group of kids is playing in a yard and an ol...   \n",
              "1        2  A group of children is playing in the house an...   \n",
              "2        3  The young boys are playing outdoors and the ma...   \n",
              "3        5  The kids are playing outdoors near a man with ...   \n",
              "4        9  The young boys are playing outdoors and the ma...   \n",
              "\n",
              "                                          sentence_B  relatedness_score  \\\n",
              "0  A group of boys in a yard is playing and a man...                4.5   \n",
              "1  A group of kids is playing in a yard and an ol...                3.2   \n",
              "2  The kids are playing outdoors near a man with ...                4.7   \n",
              "3  A group of kids is playing in a yard and an ol...                3.4   \n",
              "4  A group of kids is playing in a yard and an ol...                3.7   \n",
              "\n",
              "  entailment_judgment  \n",
              "0             NEUTRAL  \n",
              "1             NEUTRAL  \n",
              "2          ENTAILMENT  \n",
              "3             NEUTRAL  \n",
              "4             NEUTRAL  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "sick_data = pd.read_csv(\"SICK_train.txt\" , sep='\\t', on_bad_lines='skip', index_col=None)\n",
        "\n",
        "# HW3T1A\n",
        "# Add code to display the first few rows of data\n",
        "\n",
        "sick_data.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3YsUGm0R8Gr"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4500, 5)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# HW3T1B\n",
        "# Add code that shows the number of variables and the number of rows in the dataset.\n",
        "# Add some comments describing what each of the columns contains.\n",
        "# The columns are the pair id, 2 sentences A and B, how related each sentence is, and its entailment (where the sentences depend on each other)\n",
        "\n",
        "sick_data.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMBf3r-qTE_7"
      },
      "source": [
        "**Task 2: Add a Similarity Score to Each Row**\n",
        "\n",
        "Use a pre-trained sentence embedding model to generate a similarity score for each sentence pair. To get started, here's some code from Lab 7:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Xg9FEECDTZqV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (5.1.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (2.9.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.7.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (12.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: setuptools in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (72.1.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\black knight\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.7.4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.3\n",
            "[notice] To update, run: C:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# HW3T2A\n",
        "# The first thing we will need is the library for loading sentence transformers\n",
        "# This generates a lot of output, but should run pretty fast.\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "e-pEfIZHTrhh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Black Knight\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Black Knight\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        }
      ],
      "source": [
        "# HW3T2B\n",
        "# Now load a pre-trained sentence transformer. There are hundreds to choose from.\n",
        "# This downloads a lot of data to your virtual machine and takes half a minute or so.\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Here's a sentence transformer model that encodes a d=384 vector. See:\n",
        "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5CGy2Ou4UcfA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The young boys are playing outdoors and the man is smiling nearby\n",
            "The kids are playing outdoors near a man with a smile\n",
            "ENTAILMENT\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8005356192588806"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# HW3T2C\n",
        "# Here's an example of how we can put the sentence transformer to use\n",
        "# by encoding the two sentences from the first row and computing a\n",
        "# cosine similarity between them.\n",
        "from sentence_transformers.util import cos_sim\n",
        "\n",
        "show_row = 2\n",
        "\n",
        "print(sick_data[\"sentence_A\"][show_row])\n",
        "print(sick_data[\"sentence_B\"][show_row])\n",
        "print(sick_data[\"entailment_judgment\"][show_row])\n",
        "a = model.encode([sick_data[\"sentence_A\"][show_row]])\n",
        "b = model.encode([sick_data[\"sentence_B\"][show_row]])\n",
        "\n",
        "cos_sim(a, b).tolist()[0][0] # The notation on the end extracts a scalar from the tensor object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dczow-X7Vrtq"
      },
      "outputs": [],
      "source": [
        "# HW3T2D\n",
        "# Add code to produce a similarity score for each sentence pair and insert the\n",
        "# list into the Pandas dataframe as a new column. You can use the \"insert\" method\n",
        "# on a Pandas df to add a column. Choose a sensible label for your new variable as\n",
        "# you will need it later for the regression analysis.\n",
        "#\n",
        "# Note that if you process the whole dataset, it will take a couple of minutes\n",
        "# to encode all 4500 of the sentences. You can subset the data down to 1000\n",
        "# or 1500 rows if you prefer to shorten the run time.\n",
        "\n",
        "import torch\n",
        "\n",
        "a = model.encode(sick_data[\"sentence_A\"].tolist())\n",
        "b = model.encode(sick_data[\"sentence_B\"].tolist())\n",
        "\n",
        "similarities_tensor = cos_sim(torch.from_numpy(a), torch.from_numpy(b))\n",
        "similarity_scores = [similarities_tensor[i][i].item() for i in range(len(sick_data))]\n",
        "\n",
        "sick_data.insert(loc=5, column='Cosine_Similarity_Score', value=similarity_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PxFfFJyn1iv5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pair_ID</th>\n",
              "      <th>sentence_A</th>\n",
              "      <th>sentence_B</th>\n",
              "      <th>relatedness_score</th>\n",
              "      <th>entailment_judgment</th>\n",
              "      <th>Cosine_Similarity_Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>A group of kids is playing in a yard and an ol...</td>\n",
              "      <td>A group of boys in a yard is playing and a man...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.851040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>A group of children is playing in the house an...</td>\n",
              "      <td>A group of kids is playing in a yard and an ol...</td>\n",
              "      <td>3.2</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.540574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>The young boys are playing outdoors and the ma...</td>\n",
              "      <td>The kids are playing outdoors near a man with ...</td>\n",
              "      <td>4.7</td>\n",
              "      <td>ENTAILMENT</td>\n",
              "      <td>0.800536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>The kids are playing outdoors near a man with ...</td>\n",
              "      <td>A group of kids is playing in a yard and an ol...</td>\n",
              "      <td>3.4</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.608034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>The young boys are playing outdoors and the ma...</td>\n",
              "      <td>A group of kids is playing in a yard and an ol...</td>\n",
              "      <td>3.7</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.489728</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pair_ID                                         sentence_A  \\\n",
              "0        1  A group of kids is playing in a yard and an ol...   \n",
              "1        2  A group of children is playing in the house an...   \n",
              "2        3  The young boys are playing outdoors and the ma...   \n",
              "3        5  The kids are playing outdoors near a man with ...   \n",
              "4        9  The young boys are playing outdoors and the ma...   \n",
              "\n",
              "                                          sentence_B  relatedness_score  \\\n",
              "0  A group of boys in a yard is playing and a man...                4.5   \n",
              "1  A group of kids is playing in a yard and an ol...                3.2   \n",
              "2  The kids are playing outdoors near a man with ...                4.7   \n",
              "3  A group of kids is playing in a yard and an ol...                3.4   \n",
              "4  A group of kids is playing in a yard and an ol...                3.7   \n",
              "\n",
              "  entailment_judgment  Cosine_Similarity_Score  \n",
              "0             NEUTRAL                 0.851040  \n",
              "1             NEUTRAL                 0.540574  \n",
              "2          ENTAILMENT                 0.800536  \n",
              "3             NEUTRAL                 0.608034  \n",
              "4             NEUTRAL                 0.489728  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# HW3T2E\n",
        "# Review a few rows of the modified Pandas df with the new column\n",
        "#\n",
        "sick_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vS_NDhiyXNXQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.8415403591981562)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# HW3T2F\n",
        "# Run and interpret a simple correlation between your new similarity score\n",
        "# and the original relatedness_score column. The corr() method from Pandas\n",
        "# (which is a method for columns) can do this.\n",
        "\n",
        "sick_data['relatedness_score'].corr(sick_data['Cosine_Similarity_Score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QESAX9U0kSli"
      },
      "source": [
        "Add your interpretation of the correlation by replacing this text. Would you consider this value to be small? Large? Is it what you expected? Why?\n",
        "\n",
        "The correlation R2 is 0.84. This is a very strong correlation but was expected. I assumed that related score for the 2 sentences was just another form of similarity. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IH5vlX_a-pD"
      },
      "source": [
        "**Task 3: Create Dummy Codes for Entailment**\n",
        "\n",
        "In this task after this one, you will produce a regression analysis that predicts the original relatedness_score from your newly added similarity measure along with two dummy codes created from the entailment_judgment.\n",
        "\n",
        "With linear regression we can use as predictors any combination of metric variables and binary codes. With a label like entailment_judgment, with three or more categories, we need to create dummy codes to represent the categories. To avoid collinearity, there should always be one less dummy code than the number of categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "c4CaB6u7arRD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'CONTRADICTION', 'ENTAILMENT', 'NEUTRAL'}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# HW3T3A\n",
        "# Show the three types of entailment_judgment using set().\n",
        "set(sick_data['entailment_judgment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dxxHH8szmUd7"
      },
      "outputs": [],
      "source": [
        "# HW3T3B\n",
        "contra = [ int(ej == \"CONTRADICTION\") for ej in sick_data[\"entailment_judgment\"]]\n",
        "entail = [ int(ej == \"ENTAILMENT\") for ej in sick_data[\"entailment_judgment\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-Dk7JbvL2nM2"
      },
      "outputs": [],
      "source": [
        "# HW3T3C\n",
        "# Insert the two new dummy codes into your pandas df\n",
        "\n",
        "sick_data['is_contradiction'] = contra\n",
        "sick_data['is_entailment'] = entail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MGbBKyuX2s5O"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pair_ID</th>\n",
              "      <th>sentence_A</th>\n",
              "      <th>sentence_B</th>\n",
              "      <th>relatedness_score</th>\n",
              "      <th>entailment_judgment</th>\n",
              "      <th>Cosine_Similarity_Score</th>\n",
              "      <th>is_contradiction</th>\n",
              "      <th>is_entailment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>A group of kids is playing in a yard and an ol...</td>\n",
              "      <td>A group of boys in a yard is playing and a man...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.851040</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>A group of children is playing in the house an...</td>\n",
              "      <td>A group of kids is playing in a yard and an ol...</td>\n",
              "      <td>3.2</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.540574</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>The young boys are playing outdoors and the ma...</td>\n",
              "      <td>The kids are playing outdoors near a man with ...</td>\n",
              "      <td>4.7</td>\n",
              "      <td>ENTAILMENT</td>\n",
              "      <td>0.800536</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>The kids are playing outdoors near a man with ...</td>\n",
              "      <td>A group of kids is playing in a yard and an ol...</td>\n",
              "      <td>3.4</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.608034</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>The young boys are playing outdoors and the ma...</td>\n",
              "      <td>A group of kids is playing in a yard and an ol...</td>\n",
              "      <td>3.7</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.489728</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>12</td>\n",
              "      <td>Two dogs are fighting</td>\n",
              "      <td>Two dogs are wrestling and hugging</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.670884</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>14</td>\n",
              "      <td>A brown dog is attacking another animal in fro...</td>\n",
              "      <td>Two dogs are fighting</td>\n",
              "      <td>3.5</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.564386</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>18</td>\n",
              "      <td>A brown dog is attacking another animal in fro...</td>\n",
              "      <td>Two dogs are wrestling and hugging</td>\n",
              "      <td>3.2</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.411986</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>25</td>\n",
              "      <td>Nobody is riding the bicycle on one wheel</td>\n",
              "      <td>A person in a black jacket is doing tricks on ...</td>\n",
              "      <td>2.8</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.201614</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>26</td>\n",
              "      <td>A person is riding the bicycle on one wheel</td>\n",
              "      <td>A man in a black jacket is doing tricks on a m...</td>\n",
              "      <td>3.7</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.306472</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>28</td>\n",
              "      <td>A person on a black motorbike is doing tricks ...</td>\n",
              "      <td>A person is riding the bicycle on one wheel</td>\n",
              "      <td>3.4</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.377968</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>30</td>\n",
              "      <td>A man with a jersey is dunking the ball at a b...</td>\n",
              "      <td>The ball is being dunked by a man with a jerse...</td>\n",
              "      <td>4.9</td>\n",
              "      <td>ENTAILMENT</td>\n",
              "      <td>0.946253</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>35</td>\n",
              "      <td>A man with a jersey is dunking the ball at a b...</td>\n",
              "      <td>A man who is playing dunks the basketball into...</td>\n",
              "      <td>3.6</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.770519</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>40</td>\n",
              "      <td>The player is dunking the basketball into the ...</td>\n",
              "      <td>A man with a jersey is dunking the ball at a b...</td>\n",
              "      <td>3.8</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.722840</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>42</td>\n",
              "      <td>Two people are kickboxing and spectators are n...</td>\n",
              "      <td>Two people are kickboxing and spectators are w...</td>\n",
              "      <td>3.4</td>\n",
              "      <td>CONTRADICTION</td>\n",
              "      <td>0.854378</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    pair_ID                                         sentence_A  \\\n",
              "0         1  A group of kids is playing in a yard and an ol...   \n",
              "1         2  A group of children is playing in the house an...   \n",
              "2         3  The young boys are playing outdoors and the ma...   \n",
              "3         5  The kids are playing outdoors near a man with ...   \n",
              "4         9  The young boys are playing outdoors and the ma...   \n",
              "5        12                              Two dogs are fighting   \n",
              "6        14  A brown dog is attacking another animal in fro...   \n",
              "7        18  A brown dog is attacking another animal in fro...   \n",
              "8        25          Nobody is riding the bicycle on one wheel   \n",
              "9        26        A person is riding the bicycle on one wheel   \n",
              "10       28  A person on a black motorbike is doing tricks ...   \n",
              "11       30  A man with a jersey is dunking the ball at a b...   \n",
              "12       35  A man with a jersey is dunking the ball at a b...   \n",
              "13       40  The player is dunking the basketball into the ...   \n",
              "14       42  Two people are kickboxing and spectators are n...   \n",
              "\n",
              "                                           sentence_B  relatedness_score  \\\n",
              "0   A group of boys in a yard is playing and a man...                4.5   \n",
              "1   A group of kids is playing in a yard and an ol...                3.2   \n",
              "2   The kids are playing outdoors near a man with ...                4.7   \n",
              "3   A group of kids is playing in a yard and an ol...                3.4   \n",
              "4   A group of kids is playing in a yard and an ol...                3.7   \n",
              "5                  Two dogs are wrestling and hugging                4.0   \n",
              "6                               Two dogs are fighting                3.5   \n",
              "7                  Two dogs are wrestling and hugging                3.2   \n",
              "8   A person in a black jacket is doing tricks on ...                2.8   \n",
              "9   A man in a black jacket is doing tricks on a m...                3.7   \n",
              "10        A person is riding the bicycle on one wheel                3.4   \n",
              "11  The ball is being dunked by a man with a jerse...                4.9   \n",
              "12  A man who is playing dunks the basketball into...                3.6   \n",
              "13  A man with a jersey is dunking the ball at a b...                3.8   \n",
              "14  Two people are kickboxing and spectators are w...                3.4   \n",
              "\n",
              "   entailment_judgment  Cosine_Similarity_Score  is_contradiction  \\\n",
              "0              NEUTRAL                 0.851040                 0   \n",
              "1              NEUTRAL                 0.540574                 0   \n",
              "2           ENTAILMENT                 0.800536                 0   \n",
              "3              NEUTRAL                 0.608034                 0   \n",
              "4              NEUTRAL                 0.489728                 0   \n",
              "5              NEUTRAL                 0.670884                 0   \n",
              "6              NEUTRAL                 0.564386                 0   \n",
              "7              NEUTRAL                 0.411986                 0   \n",
              "8              NEUTRAL                 0.201614                 0   \n",
              "9              NEUTRAL                 0.306472                 0   \n",
              "10             NEUTRAL                 0.377968                 0   \n",
              "11          ENTAILMENT                 0.946253                 0   \n",
              "12             NEUTRAL                 0.770519                 0   \n",
              "13             NEUTRAL                 0.722840                 0   \n",
              "14       CONTRADICTION                 0.854378                 1   \n",
              "\n",
              "    is_entailment  \n",
              "0               0  \n",
              "1               0  \n",
              "2               1  \n",
              "3               0  \n",
              "4               0  \n",
              "5               0  \n",
              "6               0  \n",
              "7               0  \n",
              "8               0  \n",
              "9               0  \n",
              "10              0  \n",
              "11              1  \n",
              "12              0  \n",
              "13              0  \n",
              "14              0  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# HW3T3D\n",
        "# Review the newly modified df\n",
        "\n",
        "sick_data.head(15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S48eJkJxpkEb"
      },
      "source": [
        "**Task 4: See if the root verbs match**\n",
        "\n",
        "In HW2 you conducted some information extraction from sentences including locating the root verb. If the root verbs match on two sentences, that may indicate something about how related the sentences are. For this task you will create a list of dummy codes that indicate whether the two sentences in each pair use the same root verb."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FWsjMchar74m"
      },
      "outputs": [],
      "source": [
        "# HW3T4A\n",
        "import spacy # SpaCy will help us find roots\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3AV5MVb9qCB3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Two dogs are fighting\n",
            "Two dogs are wrestling and hugging\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(fighting, wrestling)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# HW3T4B\n",
        "# Here's some demo code that shows how to fetch the roots\n",
        "show_row = 5\n",
        "\n",
        "print(sick_data[\"sentence_A\"][show_row])\n",
        "print(sick_data[\"sentence_B\"][show_row])\n",
        "\n",
        "doc1 = nlp(sick_data[\"sentence_A\"][show_row])\n",
        "doc2 = nlp(sick_data[\"sentence_B\"][show_row])\n",
        "\n",
        "list(doc1.sents)[0].root, list(doc2.sents)[0].root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "72lExZHWr6BG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# HW3T4C\n",
        "# Here's a test that compares the two strings\n",
        "list(doc1.sents)[0].root.text == list(doc2.sents)[0].root.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "62VFUsvOs2nz"
      },
      "outputs": [],
      "source": [
        "# HW3T4D\n",
        "# Write a loop that will perform the test on each pair of sentences and save the result in a list\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "my_list=[] \n",
        "\n",
        "for index, row in sick_data.iterrows():\n",
        "    doc1 = nlp(row['sentence_A'])\n",
        "    doc2 = nlp(row['sentence_B'])\n",
        "\n",
        "    results = list(doc1.sents)[0].root.text == list(doc2.sents)[0].root.text\n",
        "\n",
        "    my_list.append(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[True, True, True, True, True, False, False, False, False, False, False, False, False, True, True, True, True, False, False, True, False, False, False, False, True, False, True, False, False, False, False, True, False, False, False, True, False, True, False, True, False, True, True, True, True, True, False, False, False, False, True, True, True, False, True, False, False, False, True, False, False, True, False, False, True, True, False, False, False, True, False, False, True, True, True, False, True, True, True, False, True, False, False, False, False, True, True, False, False, True, True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, False, False, False, False, False, True, False, False, True, False, False, True, True, False, False, False, True, True, False, True, True, True, False, True, True, True, False, False, False, True, True, True, True, False, False, True, True, False, False, False, True, True, False, False, False, False, False, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, False, True, False, False, False, True, True, True, False, False, False, False, True, False, False, False, False, False, True, False, True, False, True, False, False, False, True, False, True, False, False, False, True, True, False, False, True, True, False, False, False, False, False, False, False, False, True, True, True, False, True, True, True, True, False, False, False, False, True, False, False, False, True, False, True, False, True, True, False, False, False, False, False, False, False, True, True, False, False, False, False, True, False, False, True, True, False, True, False, False, False, True, False, True, True, False, False, False, False, False, False, False, False, True, False, False, True, True, False, False, True, False, True, False, False, True, True, True, False, False, False, True, True, True, False, False, False, True, False, False, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, True, True, False, False, False, True, True, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, True, False, True, False, True, True, False, True, True, False, False, False, False, False, False, True, False, True, False, False, False, False, True, True, False, False, True, False, False, False, False, True, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, True, True, False, False, True, True, False, False, True, False, False, False, True, False, False, False, False, True, True, False, False, False, True, False, True, False, False, False, False, False, False, False, True, False, True, True, False, True, False, False, False, False, False, True, True, False, True, False, True, False, False, False, False, False, True, True, False, True, False, True, False, False, True, False, False, False, False, False, False, False, False, False, True, True, False, False, False, True, True, False, False, False, True, False, False, False, True, True, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, True, True, True, True, True, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, True, False, False, True, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, True, True, False, False, True, True, True, True, False, True, False, True, True, True, True, True, False, False, False, False, True, False, False, False, False, True, False, False, True, False, False, False, True, True, True, False, True, False, False, False, False, False, False, False, False, False, True, True, False, False, False, True, True, True, False, True, False, False, False, True, False, False, True, True, True, True, False, True, True, True, False, False, False, False, True, False, True, False, False, False, False, True, False, False, True, False, False, False, False, True, False, False, False, False, True, False, False, False, True, False, True, True, False, False, False, False, False, True, False, True, True, False, False, False, False, True, True, False, True, False, True, True, False, True, False, False, False, False, False, False, True, True, True, True, False, False, True, True, True, True, True, False, True, False, False, False, False, True, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, True, True, False, False, False, False, False, True, False, True, False, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, True, True, False, False, True, True, False, False, False, True, False, False, False, False, False, False, True, True, True, False, False, False, False, False, True, False, False, False, True, True, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, True, False, True, False, False, False, True, True, True, True, True, False, False, True, False, False, True, True, True, True, False, True, False, False, True, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, True, False, True, False, False, False, True, True, False, False, True, False, True, False, True, False, False, True, True, False, False, False, True, True, False, False, True, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, True, False, False, True, False, True, True, False, True, False, False, False, True, False, False, True, False, False, False, False, True, True, True, False, True, True, True, False, True, True, False, True, True, False, False, False, False, False, False, True, False, True, False, False, True, True, True, False, True, False, False, True, True, True, False, True, True, True, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, True, False, False, True, False, True, False, False, False, False, True, False, True, True, False, False, False, True, False, False, True, False, False, False, False, False, False, True, True, False, True, False, False, False, False, False, False, True, True, False, True, True, True, True, False, False, True, False, False, False, True, True, True, False, True, True, False, False, True, False, False, True, True, True, True, False, True, False, False, False, True, True, True, False, True, False, False, True, True, True, False, True, False, True, False, True, True, False, True, False, False, True, True, True, True, False, True, False, True, False, False, False, False, False, False, False, False, True, False, True, False, False, True, True, True, False, False, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, True, False, False, False, True, False, True, True, False, False, False, False, True, True, False, False, False, False, True, True, True, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, True, False, True, False, True, True, True, False, False, True, False, False, False, False, False, False, True, True, True, True, True, True, False, True, False, True, True, False, True, False, True, True, True, True, True, True, True, False, False, False, False, False, True, False, False, False, True, True, True, False, True, False, True, False, False, True, True, False, False, False, False, True, False, True, False, False, False, True, False, True, True, False, False, False, False, False, True, True, True, True, True, True, True, False, False, False, True, True, True, True, False, True, True, False, False, False, False, False, False, True, False, False, False, True, False, False, False, True, False, False, True, False, False, True, False, False, True, False, True, True, False, True, True, True, True, False, True, True, False, True, False, True, False, False, False, False, True, True, False, False, False, True, True, True, True, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, False, True, True, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, False, True, False, True, True, False, False, True, False, False, False, False, False, False, True, False, False, False, True, True, True, False, True, True, True, True, True, True, True, True, False, False, False, False, True, False, True, True, False, True, False, False, False, False, False, False, True, True, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, True, True, True, True, False, True, True, True, True, False, False, False, False, True, False, True, True, False, False, True, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, False, True, True, False, False, False, False, True, False, False, True, True, True, True, True, True, True, False, False, True, False, True, False, False, False, False, False, True, True, True, False, True, True, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, True, True, True, False, False, True, False, True, True, True, False, False, False, True, False, True, False, False, True, False, False, True, True, True, False, False, False, True, True, False, False, False, False, True, True, True, True, True, False, True, False, False, False, False, True, False, True, False, True, True, False, True, True, False, False, False, False, False, False, True, False, True, False, True, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, True, False, False, True, True, True, False, True, False, True, False, False, False, True, True, False, False, True, True, False, True, False, False, False, False, False, False, True, True, False, True, False, True, True, False, True, False, False, False, False, True, False, False, True, True, True, True, True, False, False, False, False, False, False, False, False, True, False, True, True, False, True, False, False, False, False, False, False, False, True, False, True, True, True, True, True, True, True, False, False, True, False, True, False, False, False, False, False, True, False, False, True, True, False, True, True, True, False, True, False, False, True, False, False, False, True, True, False, True, False, False, False, False, True, True, True, True, True, True, True, True, False, False, False, True, False, False, False, False, False, True, False, True, True, False, True, True, False, True, True, True, False, False, False, False, False, True, False, True, False, False, False, True, False, True, True, False, False, False, False, False, True, True, False, True, True, True, True, True, False, False, False, False, False, False, True, False, False, False, True, False, False, False, True, False, True, True, False, False, False, True, False, False, False, True, False, True, False, False, False, False, False, True, True, False, True, True, False, True, False, True, True, False, True, False, False, False, False, False, False, False, False, False, True, True, False, True, False, False, False, True, False, True, False, True, True, True, True, True, True, True, True, True, False, True, False, True, False, True, True, True, True, False, False, False, False, False, True, False, False, True, False, True, True, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, True, True, False, False, False, False, True, True, False, True, False, False, True, True, True, True, False, True, False, True, False, True, False, False, False, False, False, True, False, True, False, False, False, True, False, True, False, False, True, True, False, False, True, False, False, True, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, True, False, False, False, True, False, True, True, False, True, True, True, False, False, True, False, False, False, False, False, False, False, False, True, False, True, False, True, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, False, False, False, True, False, True, True, False, False, True, True, True, True, True, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, True, False, False, False, True, True, False, False, False, False, True, True, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, False, False, False, True, False, False, False, True, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, True, False, False, False, False, False, True, False, False, False, False, False, False, True, True, False, False, False, True, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, True, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, True, True, True, False, False, True, False, True, True, False, True, True, False, False, False, True, False, False, False, False, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, False, False, True, True, True, True, False, False, True, False, False, False, False, True, True, False, True, True, False, True, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, True, True, False, False, True, False, False, False, False, False, False, False, False, False, True, False, True, True, False, True, False, False, False, False, False, False, True, True, False, False, True, False, False, True, False, True, True, True, True, False, True, False, False, False, False, True, False, False, True, False, True, True, True, True, False, False, False, False, True, True, True, False, False, False, True, True, False, False, False, True, False, False, True, True, False, False, False, False, False, False, True, True, False, False, True, True, False, True, True, False, False, True, False, False, True, True, True, False, False, True, True, False, False, True, True, False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, True, True, True, False, False, False, True, True, False, False, False, False, False, True, True, True, False, False, False, False, False, False, False, False, True, False, False, False, False, True, True, True, True, True, True, False, False, True, False, False, True, True, True, False, True, False, False, True, True, True, True, True, False, False, False, False, False, True, True, True, False, True, False, False, False, False, False, False, True, True, False, True, True, False, True, False, False, False, False, True, False, False, False, False, False, True, False, False, True, True, True, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, True, False, False, False, False, True, False, True, False, True, False, True, True, False, False, False, True, True, False, False, False, True, True, False, True, False, False, False, False, True, True, False, False, True, True, True, True, True, False, False, True, False, False, False, False, False, False, False, False, False, True, True, True, False, False, True, False, False, True, False, False, False, False, True, True, True, True, False, True, True, False, False, False, False, False, False, False, True, False, True, True, True, False, True, False, False, True, False, True, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False, True, True, True, False, True, True, False, True, False, False, True, True, False, True, True, True, False, True, False, True, True, False, True, True, True, True, True, False, True, False, False, False, True, False, False, True, True, False, False, False, False, True, True, False, True, False, False, False, False, False, False, False, True, True, True, True, True, True, False, True, True, False, False, False, False, False, False, False, False, False, False, True, True, True, False, True, False, True, False, True, True, True, True, False, False, False, True, True, True, True, True, True, True, False, True, True, False, False, True, True, False, False, False, False, True, True, False, True, False, True, True, False, True, False, False, False, False, False, True, False, True, True, False, True, True, False, True, False, True, True, False, False, True, False, True, False, True, False, True, False, False, False, False, True, False, False, False, False, True, True, False, False, False, True, True, False, False, True, True, False, True, True, True, False, False, True, True, False, False, False, False, True, False, False, False, True, True, False, True, False, False, True, False, False, False, True, False, True, True, True, False, True, True, False, False, False, True, True, False, False, True, True, False, False, False, False, False, False, True, False, True, False, True, True, True, False, False, True, True, False, True, True, True, True, True, False, False, False, True, True, False, False, False, True, False, True, False, False, False, True, False, False, True, False, True, False, False, False, True, True, False, False, False, False, False, False, False, False, False, True, True, True, False, False, True, True, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, True, True, True, True, False, False, False, False, True, False, True, True, True, True, True, True, False, False, False, True, True, False, False, False, False, True, True, True, True, False, True, False, False, False, False, False, False, False, False, False, False, True, True, True, True, False, False, True, False, True, False, False, False, False, False, False, True, False, False, False, True, False, True, True, False, False, True, False, False, True, True, False, False, True, False, False, False, False, False, True, True, False, True, True, False, False, True, True, False, True, True, True, True, True, True, True, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, True, True, True, False, False, False, True, True, True, False, True, True, True, False, False, False, False, False, False, False, True, False, False, False, True, True, False, False, True, False, False, False, False, False, True, True, False, False, False, False, False, False, False, True, True, True, False, True, False, False, False, True, True, True, False, False, False, False, False, False, False, False, False, True, False, False, False, True, True, True, False, True, False, False, False, False, False, True, True, False, False, True, True, False, True, False, True, False, False, True, False, False, True, False, False, False, True, False, False, True, True, False, True, False, False, False, False, True, True, False, True, True, False, False, False, False, False, False, False, True, False, False, False, True, False, True, False, True, True, True, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, True, False, True, True, False, False, False, False, False, False, True, False, False, True, False, True, True, True, False, False, False, False, True, True, True, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, True, False, True, True, False, True, True, True, False, True, False, False, False, True, False, False, False, False, True, False, False, True, True, True, True, True, True, True, False, False, False, False, False, True, False, True, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, False, True, False, True, True, False, True, True, False, False, False, False, True, False, False, False, True, True, False, False, True, True, True, True, False, False, True, False, True, False, False, False, False, True, True, True, True, False, False, False, True, False, False, False, False, False, False, True, False, False, False, False, True, True, False, False, False, False, True, False, False, False, False, True, True, False, False, False, False, True, True, True, True, True, False, False, False, False, False, True, True, True, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, True, True, False, False, False, False, True, True, True, True, False, True, True, True, True, True, True, True, True, False, False, False, False, True, False, False, False, False, False, True, True, False, False, False, False, True, False, False, False, False, True, False, False, True, True, True, False, False, True, False, True, False, True, True, True, True, True, False, True, True, False, True, False, True, True, True, False, False, False, True, True, True, False, False, False, False, True, False, True, False, True, True, True, True, False, False, False, True, False, False, False, False, True, True, False, False, True, True, False, False, False, True, True, False, False, False, True, True, False, False, False, False, True, True, False, False, False, True, False, False, True, False, False, True, True, True, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, True, False, False, True, True, True, True, True, True, True, True, True, False, False, True, True, True, False, True, False, True, False, True, False, False, False, True, True, True, False, False, False, False, False, False, True, True, False, False, False, True, True, False, True, False, True, False, False, False, False, True, False, True, False, False, True, True, True, True, True, True, True, True, False, False, False, False, True, True, True, True, False, False, False, False, False, True, True, False, False, True, False, False, False, True, True, False, False, False, True, True, False, False, False, True, True, False, False, True, True, True, True, True, True, True, False, True, False, True, True, True, False, False, False, False, True, False, True, False, True, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, False, False, True, False, False, True, False, False, True, True, True, True, True, False, False, True, True, True, False, False, True, True, False, False, True, False, False, False, False, False, False, False, False, False, True, True, True, True, False, False, False, True, False, True, False, True, True, False, True, False, False, False, True, True, True, True, False, True, True, True, True, False, True, True, True, False, False, False, False, False, False, False, False, True, False, False, False, True, True, True, True, True, True, True, True, False, False, False, True, True, False, False, True, True, False, False, False, True, True, True, True, True, True, True, False, False, False, False, False, True, False, False, True, True, True, False, True, True, True, True, True, True, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, True, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, False, True, True, True, False, True, True, True, True, False, True, True, False, True, True, False, False, True, True, True, False, False, True, False, True, False, False, False, True, False, True, True, True, True, True, False, True, True, False, False, False, False, True, True, True, True, True, False, False, False, False, False, True, True, True, True, False, False, True, True, False, False, True, True, True, False, False, True, True, False, False, False, False, True, True, True, True, True, True, True, True, False, False, False, True, True, False, False, False, True, True, True, False, False, False, True, True, False, False, True, True, True, True, True, True, False, False, False, True, False, False, True, True, True, True, True, False, True, False, False, False, True, False, False, True, True, False, False, False, False, True, True, True, True, True, True, True, False, False, True, True, True, True, True, False, False, False, False, True, False, False, False, True, True, False, True, True, True, True, False, False, False, False, True, False, False, False, True, False, False, False, False, False, True, True, False, True, False, True, True, False, True, False, True, False, False, False, True, True, False, True, False, False, True, False, True, False, True, True, False, False, False, True, True, True, False, False, False, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, False, False, True, False, True, False, True, True, False, True, True, True, False, False, False, False, False, False, False, True, False, True, True, False, False, False, False, True, True, True, True, True, True, True, True, False, False, True, True, False, False, True, True, True, True, True, True, True, False, True, True, True, False, False, False, True, True, True, False, True, False, False, True, True, False, True, True, True, False, True, True, True, False, True, False, False, True, False, False, True, True, True, True, True, True, True, True, False, False, True, False, False, True, True, False, True, True, False, True, True, True, True, False, False, True, True, False, True, True, False, False, False, False, False, False, True, True, True, False, False, False, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, False, False, False, False, False, False, False, True, True, True, True, False, False, False, False, True, True, False, True, False, True, False, True, True, True, False, True, True, True, True, False, False, False, True, False, False, False, True, False, False, False, False, False, False, True, True, False, True, True, True, True, False, True, True, True, True, False, False, False, False, False, True, False, False, True, True, True, True, True, True, True, True, False, False, True, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4500"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(my_list)\n",
        "len(my_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "nJ7iinsds1Av"
      },
      "outputs": [],
      "source": [
        "# HW3T4E\n",
        "# Insert the new dummy code into your pandas df.\n",
        "# Use the column name, \"root_match\", as shown in the example\n",
        "\n",
        "sick_data.insert(loc=6, column='root_match', value= my_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Si7-RlWfs-nA"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pair_ID</th>\n",
              "      <th>sentence_A</th>\n",
              "      <th>sentence_B</th>\n",
              "      <th>relatedness_score</th>\n",
              "      <th>entailment_judgment</th>\n",
              "      <th>Cosine_Similarity_Score</th>\n",
              "      <th>root_match</th>\n",
              "      <th>is_contradiction</th>\n",
              "      <th>is_entailment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>A group of kids is playing in a yard and an ol...</td>\n",
              "      <td>A group of boys in a yard is playing and a man...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.851040</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>A group of children is playing in the house an...</td>\n",
              "      <td>A group of kids is playing in a yard and an ol...</td>\n",
              "      <td>3.2</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.540574</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>The young boys are playing outdoors and the ma...</td>\n",
              "      <td>The kids are playing outdoors near a man with ...</td>\n",
              "      <td>4.7</td>\n",
              "      <td>ENTAILMENT</td>\n",
              "      <td>0.800536</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>The kids are playing outdoors near a man with ...</td>\n",
              "      <td>A group of kids is playing in a yard and an ol...</td>\n",
              "      <td>3.4</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.608034</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>The young boys are playing outdoors and the ma...</td>\n",
              "      <td>A group of kids is playing in a yard and an ol...</td>\n",
              "      <td>3.7</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.489728</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>12</td>\n",
              "      <td>Two dogs are fighting</td>\n",
              "      <td>Two dogs are wrestling and hugging</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.670884</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>14</td>\n",
              "      <td>A brown dog is attacking another animal in fro...</td>\n",
              "      <td>Two dogs are fighting</td>\n",
              "      <td>3.5</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.564386</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>18</td>\n",
              "      <td>A brown dog is attacking another animal in fro...</td>\n",
              "      <td>Two dogs are wrestling and hugging</td>\n",
              "      <td>3.2</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.411986</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>25</td>\n",
              "      <td>Nobody is riding the bicycle on one wheel</td>\n",
              "      <td>A person in a black jacket is doing tricks on ...</td>\n",
              "      <td>2.8</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.201614</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>26</td>\n",
              "      <td>A person is riding the bicycle on one wheel</td>\n",
              "      <td>A man in a black jacket is doing tricks on a m...</td>\n",
              "      <td>3.7</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>0.306472</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pair_ID                                         sentence_A  \\\n",
              "0        1  A group of kids is playing in a yard and an ol...   \n",
              "1        2  A group of children is playing in the house an...   \n",
              "2        3  The young boys are playing outdoors and the ma...   \n",
              "3        5  The kids are playing outdoors near a man with ...   \n",
              "4        9  The young boys are playing outdoors and the ma...   \n",
              "5       12                              Two dogs are fighting   \n",
              "6       14  A brown dog is attacking another animal in fro...   \n",
              "7       18  A brown dog is attacking another animal in fro...   \n",
              "8       25          Nobody is riding the bicycle on one wheel   \n",
              "9       26        A person is riding the bicycle on one wheel   \n",
              "\n",
              "                                          sentence_B  relatedness_score  \\\n",
              "0  A group of boys in a yard is playing and a man...                4.5   \n",
              "1  A group of kids is playing in a yard and an ol...                3.2   \n",
              "2  The kids are playing outdoors near a man with ...                4.7   \n",
              "3  A group of kids is playing in a yard and an ol...                3.4   \n",
              "4  A group of kids is playing in a yard and an ol...                3.7   \n",
              "5                 Two dogs are wrestling and hugging                4.0   \n",
              "6                              Two dogs are fighting                3.5   \n",
              "7                 Two dogs are wrestling and hugging                3.2   \n",
              "8  A person in a black jacket is doing tricks on ...                2.8   \n",
              "9  A man in a black jacket is doing tricks on a m...                3.7   \n",
              "\n",
              "  entailment_judgment  Cosine_Similarity_Score  root_match  is_contradiction  \\\n",
              "0             NEUTRAL                 0.851040        True                 0   \n",
              "1             NEUTRAL                 0.540574        True                 0   \n",
              "2          ENTAILMENT                 0.800536        True                 0   \n",
              "3             NEUTRAL                 0.608034        True                 0   \n",
              "4             NEUTRAL                 0.489728        True                 0   \n",
              "5             NEUTRAL                 0.670884       False                 0   \n",
              "6             NEUTRAL                 0.564386       False                 0   \n",
              "7             NEUTRAL                 0.411986       False                 0   \n",
              "8             NEUTRAL                 0.201614       False                 0   \n",
              "9             NEUTRAL                 0.306472       False                 0   \n",
              "\n",
              "   is_entailment  \n",
              "0              0  \n",
              "1              0  \n",
              "2              1  \n",
              "3              0  \n",
              "4              0  \n",
              "5              0  \n",
              "6              0  \n",
              "7              0  \n",
              "8              0  \n",
              "9              0  "
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# HW3T4F\n",
        "# Display the pandas df\n",
        "\n",
        "sick_data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMr4W8zJ24Xz"
      },
      "source": [
        "**Task 5: Run Regression Analysis to Predict relatedness_score**\n",
        "\n",
        "Next, your job is to conduct a regression analysis where you predict relatedness_score from your newly added similarity measure along with two dummy codes created from the entailment_judgment. The sklearn library has a linear regression model, but statsmodels.api produces nicer output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "-JZaSUIKlEck"
      },
      "outputs": [],
      "source": [
        "# HW3T5A\n",
        "# The sm.OLS() method does regression and once you have fitted the model\n",
        "# the summary() method produces nice output with all the values needed to\n",
        "# interpret regression results.\n",
        "#\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# You can specify your regression model using a formula language, like this:\n",
        "results = smf.ols('relatedness_score ~ Cosine_Similarity_Score + entailment_judgment + is_contradiction + root_match', data=sick_data).fit()\n",
        "# Of course, you may need to adjust the X-variable names depending upon the\n",
        "# labels you used when you inserted them into the database.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "LFuSoytKpKm8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>    <td>relatedness_score</td> <th>  R-squared:         </th> <td>   0.775</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>        <th>  Adj. R-squared:    </th> <td>   0.774</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>   <th>  F-statistic:       </th> <td>   3090.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Mon, 01 Dec 2025</td>  <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>22:18:27</td>      <th>  Log-Likelihood:    </th> <td> -3144.1</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td>  4500</td>       <th>  AIC:               </th> <td>   6300.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td>  4494</td>       <th>  BIC:               </th> <td>   6339.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>     5</td>       <th>                     </th>     <td> </td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>     <th>                     </th>     <td> </td>   \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "                  <td></td>                     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th>                         <td> 9.374e+11</td> <td> 1.41e+12</td> <td>    0.666</td> <td> 0.506</td> <td>-1.82e+12</td> <td>  3.7e+12</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>entailment_judgment[T.ENTAILMENT]</th> <td>-9.374e+11</td> <td> 1.41e+12</td> <td>   -0.666</td> <td> 0.506</td> <td> -3.7e+12</td> <td> 1.82e+12</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>entailment_judgment[T.NEUTRAL]</th>    <td>-9.374e+11</td> <td> 1.41e+12</td> <td>   -0.666</td> <td> 0.506</td> <td> -3.7e+12</td> <td> 1.82e+12</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>root_match[T.True]</th>                <td>   -0.0855</td> <td>    0.017</td> <td>   -5.161</td> <td> 0.000</td> <td>   -0.118</td> <td>   -0.053</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Cosine_Similarity_Score</th>           <td>    2.8237</td> <td>    0.038</td> <td>   75.296</td> <td> 0.000</td> <td>    2.750</td> <td>    2.897</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>is_contradiction</th>                  <td>-9.374e+11</td> <td> 1.41e+12</td> <td>   -0.666</td> <td> 0.506</td> <td> -3.7e+12</td> <td> 1.82e+12</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>10.834</td> <th>  Durbin-Watson:     </th> <td>   1.602</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th> <td> 0.004</td> <th>  Jarque-Bera (JB):  </th> <td>  12.759</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>          <td>-0.043</td> <th>  Prob(JB):          </th> <td> 0.00170</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>      <td> 3.246</td> <th>  Cond. No.          </th> <td>5.55e+14</td>\n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 2.99e-26. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
            ],
            "text/latex": [
              "\\begin{center}\n",
              "\\begin{tabular}{lclc}\n",
              "\\toprule\n",
              "\\textbf{Dep. Variable:}                     & relatedness\\_score & \\textbf{  R-squared:         } &     0.775   \\\\\n",
              "\\textbf{Model:}                             &        OLS         & \\textbf{  Adj. R-squared:    } &     0.774   \\\\\n",
              "\\textbf{Method:}                            &   Least Squares    & \\textbf{  F-statistic:       } &     3090.   \\\\\n",
              "\\textbf{Date:}                              &  Mon, 01 Dec 2025  & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
              "\\textbf{Time:}                              &      22:18:27      & \\textbf{  Log-Likelihood:    } &   -3144.1   \\\\\n",
              "\\textbf{No. Observations:}                  &         4500       & \\textbf{  AIC:               } &     6300.   \\\\\n",
              "\\textbf{Df Residuals:}                      &         4494       & \\textbf{  BIC:               } &     6339.   \\\\\n",
              "\\textbf{Df Model:}                          &            5       & \\textbf{                     } &             \\\\\n",
              "\\textbf{Covariance Type:}                   &     nonrobust      & \\textbf{                     } &             \\\\\n",
              "\\bottomrule\n",
              "\\end{tabular}\n",
              "\\begin{tabular}{lcccccc}\n",
              "                                            & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
              "\\midrule\n",
              "\\textbf{Intercept}                          &    9.374e+11  &     1.41e+12     &     0.666  &         0.506        &    -1.82e+12    &      3.7e+12     \\\\\n",
              "\\textbf{entailment\\_judgment[T.ENTAILMENT]} &   -9.374e+11  &     1.41e+12     &    -0.666  &         0.506        &     -3.7e+12    &     1.82e+12     \\\\\n",
              "\\textbf{entailment\\_judgment[T.NEUTRAL]}    &   -9.374e+11  &     1.41e+12     &    -0.666  &         0.506        &     -3.7e+12    &     1.82e+12     \\\\\n",
              "\\textbf{root\\_match[T.True]}                &      -0.0855  &        0.017     &    -5.161  &         0.000        &       -0.118    &       -0.053     \\\\\n",
              "\\textbf{Cosine\\_Similarity\\_Score}          &       2.8237  &        0.038     &    75.296  &         0.000        &        2.750    &        2.897     \\\\\n",
              "\\textbf{is\\_contradiction}                  &   -9.374e+11  &     1.41e+12     &    -0.666  &         0.506        &     -3.7e+12    &     1.82e+12     \\\\\n",
              "\\bottomrule\n",
              "\\end{tabular}\n",
              "\\begin{tabular}{lclc}\n",
              "\\textbf{Omnibus:}       & 10.834 & \\textbf{  Durbin-Watson:     } &    1.602  \\\\\n",
              "\\textbf{Prob(Omnibus):} &  0.004 & \\textbf{  Jarque-Bera (JB):  } &   12.759  \\\\\n",
              "\\textbf{Skew:}          & -0.043 & \\textbf{  Prob(JB):          } &  0.00170  \\\\\n",
              "\\textbf{Kurtosis:}      &  3.246 & \\textbf{  Cond. No.          } & 5.55e+14  \\\\\n",
              "\\bottomrule\n",
              "\\end{tabular}\n",
              "%\\caption{OLS Regression Results}\n",
              "\\end{center}\n",
              "\n",
              "Notes: \\newline\n",
              " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
              " [2] The smallest eigenvalue is 2.99e-26. This might indicate that there are \\newline\n",
              " strong multicollinearity problems or that the design matrix is singular."
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                            OLS Regression Results                            \n",
              "==============================================================================\n",
              "Dep. Variable:      relatedness_score   R-squared:                       0.775\n",
              "Model:                            OLS   Adj. R-squared:                  0.774\n",
              "Method:                 Least Squares   F-statistic:                     3090.\n",
              "Date:                Mon, 01 Dec 2025   Prob (F-statistic):               0.00\n",
              "Time:                        22:18:27   Log-Likelihood:                -3144.1\n",
              "No. Observations:                4500   AIC:                             6300.\n",
              "Df Residuals:                    4494   BIC:                             6339.\n",
              "Df Model:                           5                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "=====================================================================================================\n",
              "                                        coef    std err          t      P>|t|      [0.025      0.975]\n",
              "-----------------------------------------------------------------------------------------------------\n",
              "Intercept                          9.374e+11   1.41e+12      0.666      0.506   -1.82e+12     3.7e+12\n",
              "entailment_judgment[T.ENTAILMENT] -9.374e+11   1.41e+12     -0.666      0.506    -3.7e+12    1.82e+12\n",
              "entailment_judgment[T.NEUTRAL]    -9.374e+11   1.41e+12     -0.666      0.506    -3.7e+12    1.82e+12\n",
              "root_match[T.True]                   -0.0855      0.017     -5.161      0.000      -0.118      -0.053\n",
              "Cosine_Similarity_Score               2.8237      0.038     75.296      0.000       2.750       2.897\n",
              "is_contradiction                  -9.374e+11   1.41e+12     -0.666      0.506    -3.7e+12    1.82e+12\n",
              "==============================================================================\n",
              "Omnibus:                       10.834   Durbin-Watson:                   1.602\n",
              "Prob(Omnibus):                  0.004   Jarque-Bera (JB):               12.759\n",
              "Skew:                          -0.043   Prob(JB):                      0.00170\n",
              "Kurtosis:                       3.246   Cond. No.                     5.55e+14\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "[2] The smallest eigenvalue is 2.99e-26. This might indicate that there are\n",
              "strong multicollinearity problems or that the design matrix is singular.\n",
              "\"\"\""
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# HW3T5B\n",
        "# Use the summary() method on your results object to show the regression output.\n",
        "results.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UruFHFFbJp23"
      },
      "source": [
        "HW3T5C\n",
        "\n",
        "Replace this text with an explanation of the regression output. Make sure to report on the values of the coefficients for the predictors (and the intercept). Also report and interpret the R-squared value. Is this a good predictive model?\n",
        "\n",
        "This is a relatively good model with an Adj R2 of 0.774. This means that the predictors listed accounts for about 77% of the variation we see in the dependent variable relatedness_score. The intercept is 9.374 x 10 ^ 11, and the coefficients for the 2 significant variables root_match and Cosine_Similarity_Score are -0.085 and 2.824 respectively. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smRnWo7MaVJi"
      },
      "source": [
        "**Task 6: Find the sentence with the largest error of prediction and the smallest error of prediction.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7qrcP1o5KUx"
      },
      "outputs": [],
      "source": [
        "# HW3T6A\n",
        "# Use the predict() method on the results object to produce predicted scores\n",
        "# for each row in your data frame. For the regression equation to work, you must\n",
        "# supply all four of the X variables for each case. Pandas makes it\n",
        "# easy to create a Pandas subset of columns, like this:\n",
        "#\n",
        "# predictors = sick_data[['similarity', 'dummy_entail', 'dummy_contra', 'root_match']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_FuDtwBtj_1"
      },
      "outputs": [],
      "source": [
        "# HW3T6B\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#\n",
        "# Show a histogram of predicted values from the previous block. Comment on\n",
        "# why the range of the predicted values does or does not make sense.\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2N-Jz0vsqxh"
      },
      "outputs": [],
      "source": [
        "# HW3T6C\n",
        "# Compute a squared error value for each row of your data frame. This is easy:\n",
        "# subtract the predicted value from the actual value (i.e., relatedness_score)\n",
        "# and square the result. You can use a for loop or a list comprehension.\n",
        "\n",
        "# Hint 1:\n",
        "# Python's built-in zip() function allows you to bundle two vectors of data\n",
        "# together into a list of tuples.\n",
        "# Hint 2:\n",
        "# When extracting data from a Pandas column, sometimes you must use the\n",
        "# tolist() method to get the data into a structure that can be used for\n",
        "# functions like zip().\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2DmbfpMsjxR"
      },
      "outputs": [],
      "source": [
        "# HW3T6D\n",
        "import numpy as np\n",
        "# Use np.argmin() to find the index of the row in the Pandas dataset with the\n",
        "# smallest squared error of prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWSjeVsorq0h"
      },
      "outputs": [],
      "source": [
        "# HW3T6E\n",
        "# Use np.argmax() to find the index of the row with the largest squared error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MChHQZgLu3kM"
      },
      "outputs": [],
      "source": [
        "# HW3T6F\n",
        "# Display the Pandas data from the row with the worst prediction. Note that\n",
        "# the iloc() method allows you to select a particular row.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWU2L5HSuPxI"
      },
      "outputs": [],
      "source": [
        "# HW3T6G\n",
        "# Display the Pandas data from the row with the best prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o4xtTfTLzxu"
      },
      "source": [
        "HW3T6H\n",
        "\n",
        "Replace this text with an explanation of why the argmin() case was \"easy\" to predict and the argmax() case was \"hard\" to predict. This explanation should consider the semantic content of the sentences in reference to the relatedness_score developed by the experts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T7IM6KotZtC"
      },
      "source": [
        "**Task 7: Write your own two contradictory sentences and predict the relatedness score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9i05sD8ty1d"
      },
      "outputs": [],
      "source": [
        "# HW3T7A\n",
        "# Replace these empty strings with your own sentences\n",
        "test_sentenceA = \"\"\n",
        "test_sentenceB = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PhkNAfYMmqJ"
      },
      "outputs": [],
      "source": [
        "# HW3T7B\n",
        "# Encode the sentences into vectors as was demonstrated for task 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZG5U91nUyXs"
      },
      "outputs": [],
      "source": [
        "# HW3T7C\n",
        "# Compute the cosine similarity between the two vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmjeZaY0Nca1"
      },
      "outputs": [],
      "source": [
        "# HW3T7D\n",
        "# You may have used the formula interface for fitting your regression model\n",
        "# with smf.ols(). If that's true, the easiest way to get a prediction for a\n",
        "# novel case is to build a one-row Pandas dataframe and enter the X variables\n",
        "# you need to make the prediction. The next two lines of commented code\n",
        "# provide an exmaple of this technique:\n",
        "# temp_df = pd.DataFrame(columns=['similarity', 'dummy_entail', 'dummy_contra', 'root_match'])\n",
        "# temp_df.loc[0] = [0.80, 0, 0, 0] # You can include whatever X data you want in place of these three values\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC1e-tJYNGPL"
      },
      "outputs": [],
      "source": [
        "# HW3T7E\n",
        "# Use the predict() method on your regression output object to make one\n",
        "# prediction of relatedness_score using the one-row Pandas dataframe from\n",
        "# the previous step.\n",
        "#\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozWJAQh5P0c6"
      },
      "source": [
        "HW3T7F\n",
        "\n",
        "Replace this text with an interpretation of the output score from the prediction based on your two sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy7gSNmRzsOa"
      },
      "source": [
        "HWCC\n",
        "\n",
        "Don't forget to write a concluding comment describing your findings. Provide an overview of your findings from the tasks you accomplished above. How well were you able to predict the relatedness score from the other variables? Did entailment or contradiction work as predictors and what does each coefficient say about our ability to predict relatedness scores? Would it be helpful to know something about entailment if we are trying to understand paraphrases? Why do we care about entailment in the context of an application such as chat?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
